{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing models using JAX's `stax` module\n",
    "\n",
    "In this notebook, I'll show the code for how to use JAX's `stax` submodule to write arbitrary models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is `stax`?\n",
    "\n",
    "Most deep learning libraries use objects as the data structure for a neural network layer.\n",
    "As such, the tunable parameters of the layer, for example `w` and `b` for a linear (\"dense\") layer\n",
    "are class attributes associated with the forward function.\n",
    "\n",
    "In some sense, because a neural network layer is nothing more than a math function,\n",
    "specifying the layer in terms of a function might also make sense.\n",
    "`stax`, then, is a new take on writing neural network models using pure functions rather than objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does `stax` work?\n",
    "\n",
    "The way that `stax` layers work is as follows.\n",
    "Every neural network layer is nothing more than a math function with a \"forward\" pass.\n",
    "Neural network models typically have their parameters \n",
    "initialized into the right shapes using random number generators.\n",
    "Put these two together, and we have a _pair_ of functions that specify a layer:\n",
    "\n",
    "- An `init_fun` function, that _initializes_ parameters into the correct shapes, and\n",
    "- An `apply_fun` function, that _applies_ the specified math transformations onto incoming data, using parameters of the correct shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Linear layer\n",
    "\n",
    "Let's see an example of this in action, by studying the implementation of the linear (\"dense\") layer in `stax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental import stax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stax.Dense??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the `apply_fun` specifies the linear transformation.\n",
    "It accepts a parameter called `params`,\n",
    "which gets tuple-unpacked into the appropriate `W` and `b`.\n",
    "\n",
    "**Notice how the `params` argument matches up with the second output of `init_fun`!**\n",
    "The `init_fun` always accepts an `rng` parameter, which is returned from JAX's `jax.random.PRNGKey()`.\n",
    "It also accepts an `input_shape` parameter,\n",
    "which specifies what the elementary shape of one sample of data is.\n",
    "So if your entire dataset is of shape `(n_samples, n_columns)`,\n",
    "then you would put in `(n_columns,)` inside there,\n",
    "as you would want to ignore the sample dimension.\n",
    "(More on why this is the case later!)\n",
    "The `init_fun` also returns the `output_shape`,\n",
    "which is used later when we chain layers together.\n",
    "(Trust me, it'll be fun.)\n",
    "\n",
    "Let's see how we can use the Dense layer to specify a linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the initialization and application function pairs\n",
    "\n",
    "Firstly, we create the `init_fun` and `apply_fun` pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_fun, apply_fun = stax.Dense(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the parameters\n",
    "\n",
    "Now, let's initialize parameters using the `init_fun`.\n",
    "\n",
    "Let's assume that we have data that is of 1 column only, to keep things simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.random import PRNGKey, split, normal\n",
    "\n",
    "key = PRNGKey(42)\n",
    "\n",
    "output_shape, params = init_fun(key, input_shape=(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply parameters and data through function\n",
    "\n",
    "We'll create some randomly generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1, k2 = split(key)\n",
    "X = normal(k1, shape=(10, 1))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's some `y_true` values that I've snuck in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.dot(X, 3) + 5\n",
    "y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll pass data through the linear model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_fun??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import vmap\n",
    "from functools import partial\n",
    "\n",
    "y_pred = vmap(partial(apply_fun, params))(data)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voil√†! We have a simple linear model implemented just like that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What was that `vmap`???\n",
    "\n",
    "`vmap` is a special function that JAX provides.\n",
    "If we write all of our functions as behaving like they take in only one i.i.d. sample,\n",
    "then we can use `vmap` to map our function over the inputs that are provided!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "Next question: how do we *optimize* the parameters using JAX?\n",
    "\n",
    "Instead of writing a training loop on our own, we can take advantage of JAX's optimizers, which are also written in a functional paradigm!\n",
    "\n",
    "JAX's optimizers are constructed as a \"triplet\" set of functions:\n",
    "\n",
    "- `init`: Takes `params` and initializes them in as a `state`, which is structured in a fashion that `update` can operate on.\n",
    "- `update`: Takes in `i`, `g`, and `state`, which respectively are:\n",
    "    - `i`: The current loop iteration\n",
    "    - `g`: Gradients calculated from `grad`!\n",
    "    - `state`: The current state of the parameters.\n",
    "- `get_params`: Takes in the `state` at a given point, and returns the parameters structured correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit\n",
    "init, update, get_params = adam(step_size=1e-1)\n",
    "update = jit(update)\n",
    "get_params = jit(get_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're still missing a piece here, that is the loss function.\n",
    "For illustration purposes, let's use the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad\n",
    "import jax.numpy as np\n",
    "\n",
    "def mseloss(params, model, x, y_true):\n",
    "    y_preds = vmap(partial(model, params))(x)\n",
    "    return np.mean(np.power(y_preds - y_true, 2))\n",
    "\n",
    "dmseloss = grad(mseloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Step\" portion of update loop\n",
    "\n",
    "Now, we're going to define the \"step\" portion of the update loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(i, state, dlossfunc, model, x, y_true):\n",
    "    params = get_params(state)\n",
    "    g = dlossfunc(params, model, x, y_true)\n",
    "    state = update(i, g, state)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JIT compilation\n",
    "\n",
    "Because it takes so many parameters (in order to remain pure, and not rely on notebook state),\n",
    "we're going to bind some of them using `functools.partial`.\n",
    "\n",
    "I'm also going to show you what happens when we JIT-compile vs. don't JIT-compile the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_partial = partial(step, dlossfunc=dmseloss, model=apply_fun, x=X, y_true=y_true)\n",
    "step_partial_jit = jit(step_partial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit loops\n",
    "\n",
    "Firstly, let's see what kind of code we'd write if we _did_ write the loop explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "state = init(params)\n",
    "for i in range(1000):\n",
    "    params = get_params(state)\n",
    "    g = dmseloss(params, apply_fun, X, y_true)\n",
    "    state = update(i, g, state)\n",
    "end = time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partialled out loop step\n",
    "\n",
    "Now, let's run the loop with the partialled out function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "state = init(params)\n",
    "for i in range(1000):\n",
    "    state = step_partial(i, state)\n",
    "end = time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JIT-compiled loop!\n",
    "\n",
    "This is much cleaner of a loop, but we did have to do some work up-front.\n",
    "\n",
    "What happens if we now use the JIT-ed function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "state = init(params)\n",
    "for i in range(1000):\n",
    "    state = step_partial_jit(i, state)\n",
    "end = time()\n",
    "print(end - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa, holy smokes, that's fast! At least 10X faster using JIT-compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_params(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aaaaand..... we were able to recover the correct parameters!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-workshop",
   "language": "python",
   "name": "dl-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

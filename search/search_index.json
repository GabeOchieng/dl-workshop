{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-\\.]+"},"docs":[{"location":"","text":"Differential Programming with JAX Thanks for stopping by to read this online book on differential programming! What you will learn From a practical standpoint, this book will teach you the basics of how to use JAX, in particular the idioms and how they map onto what we might alrady know in Python. From a more abstract standpoint, this book will give you practice with a more \"functional\" style of programming (in contrast to an object-oriented style or an imperative style). My goal for you is to finish reading the book having the confidence to write differentiable numeric models of the world. The key operative word here being \"differentiable\" - you can calculate and evaluate the gradient of a model (written as a function) w.r.t. its parameters (which are passed in as inputs). Along the way, you might see the connections between topics that you might be familiar with (Bayesian statistics, deep learning, and more) and differntial computing. If they pop out to you through this book and the examples in there, then I know you'll likely enjoy the thrill of seeing a new connection in your personal knowledge graph. How to use this book For online readers This website, which is freely available to all, can be read in order from start to end. If you're already familiar with differential computing and are curious about how to write JAX programs, head over to the section on JAX programming. If you're curious about how to write neural network models, head over to the stax section. There's also a collection of \"case study\"/\"recipe\"-like chapters, in which we set up a computing problem of relevance and walk through how to write a JAX program there, leveraging what we have learned in the rest of the book. For interactive coding learners If you're the type who likes to execute code and break it in order to learn about what's going on, or if you're in an online interactive learning session with me, then you're in luck! The entire book has been written using Jupyter notebooks and Markdown files, and any section written as a Jupyter notebook has an \"open in Binder\" badge available at the top. Look for the button that looks like the one below: This will bring you to a pre-built Binder sesion that you can use to execute the code, break it, and play around with the ideas in the book. There are exercises interspersed throughout the book that you can stop to read through as well. If you prefer to set up an environment locally, here are instructions for you: conda env create -f environment.yml conda activate dl-workshop # older versions of conda use `source activate` rather than `conda activate` python -m ipykernel install --user --name dl-workshop jupyter labextension install @jupyter-widgets/jupyterlab-manager If you want jax with GPU, you will need to build from source, or follow the installation instructions If you are using Jupyter Lab, you will want to also ensure that ipywidgets is installed: # only if you don't have ipywidgets installed. conda install -c conda-forge ipywidgets # the next line is necessary. jupyter labextension install @jupyter-widgets/jupyterlab-manager Further Reading Demystifying Different Variants of Gradient Descent Optimization Algorithm","title":"Home"},{"location":"#differential-programming-with-jax","text":"Thanks for stopping by to read this online book on differential programming!","title":"Differential Programming with JAX"},{"location":"#what-you-will-learn","text":"From a practical standpoint, this book will teach you the basics of how to use JAX, in particular the idioms and how they map onto what we might alrady know in Python. From a more abstract standpoint, this book will give you practice with a more \"functional\" style of programming (in contrast to an object-oriented style or an imperative style). My goal for you is to finish reading the book having the confidence to write differentiable numeric models of the world. The key operative word here being \"differentiable\" - you can calculate and evaluate the gradient of a model (written as a function) w.r.t. its parameters (which are passed in as inputs). Along the way, you might see the connections between topics that you might be familiar with (Bayesian statistics, deep learning, and more) and differntial computing. If they pop out to you through this book and the examples in there, then I know you'll likely enjoy the thrill of seeing a new connection in your personal knowledge graph.","title":"What you will learn"},{"location":"#how-to-use-this-book","text":"","title":"How to use this book"},{"location":"#for-online-readers","text":"This website, which is freely available to all, can be read in order from start to end. If you're already familiar with differential computing and are curious about how to write JAX programs, head over to the section on JAX programming. If you're curious about how to write neural network models, head over to the stax section. There's also a collection of \"case study\"/\"recipe\"-like chapters, in which we set up a computing problem of relevance and walk through how to write a JAX program there, leveraging what we have learned in the rest of the book.","title":"For online readers"},{"location":"#for-interactive-coding-learners","text":"If you're the type who likes to execute code and break it in order to learn about what's going on, or if you're in an online interactive learning session with me, then you're in luck! The entire book has been written using Jupyter notebooks and Markdown files, and any section written as a Jupyter notebook has an \"open in Binder\" badge available at the top. Look for the button that looks like the one below: This will bring you to a pre-built Binder sesion that you can use to execute the code, break it, and play around with the ideas in the book. There are exercises interspersed throughout the book that you can stop to read through as well. If you prefer to set up an environment locally, here are instructions for you: conda env create -f environment.yml conda activate dl-workshop # older versions of conda use `source activate` rather than `conda activate` python -m ipykernel install --user --name dl-workshop jupyter labextension install @jupyter-widgets/jupyterlab-manager If you want jax with GPU, you will need to build from source, or follow the installation instructions If you are using Jupyter Lab, you will want to also ensure that ipywidgets is installed: # only if you don't have ipywidgets installed. conda install -c conda-forge ipywidgets # the next line is necessary. jupyter labextension install @jupyter-widgets/jupyterlab-manager","title":"For interactive coding learners"},{"location":"#further-reading","text":"Demystifying Different Variants of Gradient Descent Optimization Algorithm","title":"Further Reading"},{"location":"00-preliminaries/01-preface/","text":"Preface The years between 2010 to 2020 was a breakout decade for deep learning. There, we saw an explosion of tooling, model building, flashy demos of performance gains, and more. But beneath the surface hype of deep learning models and methods, I witnessed the maturity of tooling surrounding this idea of \"differential programming\" and composable program transforms. That tooling is the focus of this material you're reading here. On differential programming What do we mean by differential programming ? From one perspective, it is the core of modern-day learning systems, where mathematical derivatives , also known as gradients , are used in optimization and learning tasks. For example, we might use gradient descent to optimize a linear model that maps predictor variables to and output variable of interest. As another example, we might use gradient descent to optimize the parameters of a neural network model that classifies molecules as being biodegradable or not based on their descriptors alone. (As you will see in this book, the \"learning\" in deep learning is nothing more than a optimization of parameters by gradient descent.) With differential computing, the key activity that we engage in is the calculation of derivatives, and tooling that helps us compute derivatives automatically , such that we do not have to calculate them by hand, is central to differential computing. If you took a calculus class, the chain rule will feature prominently here, and JAX provides the tooling that gives us automatic differentiation , i.e. a \"program transformation\" that automatically calculates a derivative function for any other function that calculates a scalar-valued output. On program transformations As mentioned in the last paragraph, when structured in the way that JAX does it, derivative computation falls under this umbrella of \"program transformations\". That is to say, JAX provides functions that transforms a program from one form into another. (We will see by exactly what syntax we'll need to automatically create gradient functions later in the book.) Now, gradients aren't the only program transformation that exist. A function that maps a scalar function over a vector of inputs, thereby producing a vector of outputs instead, is another example of a program transformation. In this book, we will explore through some of the program transformations that are available in JAX, and see how they can be used to write beautifully structured array programs that are more flat than nested, more explicit than implicit, and are tens to hundreds of times more performant than vanilla Python/NumPy programs. The choice of JAX vs. other array frameworks As of the time of writing, there are two dominant deep learning frameworks that also provide automatic differentiation. They are PyTorch and TensorFlow, which have each enjoyed their zenith of fame. JAX generally distinguishes itself from PyTorch and TensorFlow in two ways by being developed against the idiomatic NumPy and SciPy APIs, thereby being extremely compatible with the rest of the PyData ecosystem; extending the Python language with more program transforms than just differential computing transformations, properly documenting the reasons why they depart from the very small subset of idioms that they don't follow. In particular, I would like to highlight the first point. API interoperability between computing packages is crucial for a thriving data science ecosystem. JAX's NumPy and SciPy wrappers ensure that all computations done using existing NumPy and SciPy code can very easily be transformed into differential-compatible computations for which program transforms provided by JAX can be easily applied. As you'll see in the book, you can even plot JAX arrays in matplotlib , the venerable Python plotting library, because of its developer's compatibility efforts.","title":"Preface"},{"location":"00-preliminaries/01-preface/#preface","text":"The years between 2010 to 2020 was a breakout decade for deep learning. There, we saw an explosion of tooling, model building, flashy demos of performance gains, and more. But beneath the surface hype of deep learning models and methods, I witnessed the maturity of tooling surrounding this idea of \"differential programming\" and composable program transforms. That tooling is the focus of this material you're reading here.","title":"Preface"},{"location":"00-preliminaries/01-preface/#on-differential-programming","text":"What do we mean by differential programming ? From one perspective, it is the core of modern-day learning systems, where mathematical derivatives , also known as gradients , are used in optimization and learning tasks. For example, we might use gradient descent to optimize a linear model that maps predictor variables to and output variable of interest. As another example, we might use gradient descent to optimize the parameters of a neural network model that classifies molecules as being biodegradable or not based on their descriptors alone. (As you will see in this book, the \"learning\" in deep learning is nothing more than a optimization of parameters by gradient descent.) With differential computing, the key activity that we engage in is the calculation of derivatives, and tooling that helps us compute derivatives automatically , such that we do not have to calculate them by hand, is central to differential computing. If you took a calculus class, the chain rule will feature prominently here, and JAX provides the tooling that gives us automatic differentiation , i.e. a \"program transformation\" that automatically calculates a derivative function for any other function that calculates a scalar-valued output.","title":"On differential programming"},{"location":"00-preliminaries/01-preface/#on-program-transformations","text":"As mentioned in the last paragraph, when structured in the way that JAX does it, derivative computation falls under this umbrella of \"program transformations\". That is to say, JAX provides functions that transforms a program from one form into another. (We will see by exactly what syntax we'll need to automatically create gradient functions later in the book.) Now, gradients aren't the only program transformation that exist. A function that maps a scalar function over a vector of inputs, thereby producing a vector of outputs instead, is another example of a program transformation. In this book, we will explore through some of the program transformations that are available in JAX, and see how they can be used to write beautifully structured array programs that are more flat than nested, more explicit than implicit, and are tens to hundreds of times more performant than vanilla Python/NumPy programs.","title":"On program transformations"},{"location":"00-preliminaries/01-preface/#the-choice-of-jax-vs-other-array-frameworks","text":"As of the time of writing, there are two dominant deep learning frameworks that also provide automatic differentiation. They are PyTorch and TensorFlow, which have each enjoyed their zenith of fame. JAX generally distinguishes itself from PyTorch and TensorFlow in two ways by being developed against the idiomatic NumPy and SciPy APIs, thereby being extremely compatible with the rest of the PyData ecosystem; extending the Python language with more program transforms than just differential computing transformations, properly documenting the reasons why they depart from the very small subset of idioms that they don't follow. In particular, I would like to highlight the first point. API interoperability between computing packages is crucial for a thriving data science ecosystem. JAX's NumPy and SciPy wrappers ensure that all computations done using existing NumPy and SciPy code can very easily be transformed into differential-compatible computations for which program transforms provided by JAX can be easily applied. As you'll see in the book, you can even plot JAX arrays in matplotlib , the venerable Python plotting library, because of its developer's compatibility efforts.","title":"The choice of JAX vs. other array frameworks"},{"location":"00-preliminaries/02-prerequisites/","text":"Pre-requisite Knowledge Before you go on, here's some topics and Python package APIs that are handy for you to know. My notes: Python: requisite. NumPy API: requisite. SciPy API: requisite. How to take derivatives: good to know, I'll cover a bit of it. How to write functional style programs: good to know, but I'll cover it. Deep learning: not necessary; one chapter covers how to write neural networks using nothing but NumPy, and how to train them using JAX's grad , and another chapter covers a way of writing neural netowrks using stax . Statistical modelling: needed only for the chapters using likelihoods and finding maximum likelihood param values using gradient descent.","title":"Prerequisites"},{"location":"00-preliminaries/02-prerequisites/#pre-requisite-knowledge","text":"Before you go on, here's some topics and Python package APIs that are handy for you to know. My notes: Python: requisite. NumPy API: requisite. SciPy API: requisite. How to take derivatives: good to know, I'll cover a bit of it. How to write functional style programs: good to know, but I'll cover it. Deep learning: not necessary; one chapter covers how to write neural networks using nothing but NumPy, and how to train them using JAX's grad , and another chapter covers a way of writing neural netowrks using stax . Statistical modelling: needed only for the chapters using likelihoods and finding maximum likelihood param values using gradient descent.","title":"Pre-requisite Knowledge"},{"location":"00-preliminaries/03-setup/","text":"Getting Setup If you're interested in executing the code...","title":"Setup"},{"location":"00-preliminaries/03-setup/#getting-setup","text":"If you're interested in executing the code...","title":"Getting Setup"},{"location":"01-differential-programming/01-neural-nets-from-scratch/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Neural Networks from Scratch In this chapter, we are going to explore differential computing in the place where it was most highly leveraged: the training of neural networks. Now, as with all topics, to learn something most clearly, it pays to have an anchoring example that we start with. In this section, we'll lean heavily on linear regression as that anchoring example . We'll also explore what gradient-based optimization is, see an elementary example of that in action, and then connect those ideas back to optimization of a linear model. Once we're done there, then we'll see the exact same ideas in action with a logistic regression model, before finally seeing them in action again with a neural network model. The big takeaway from this chapter is that basically all supervised learning tasks can be broken into: model loss optimizer Hope you enjoy it! If you're ready, let's take a look at linear regression. import jax.numpy as np from jax import jit import numpy.random as npr import matplotlib.pyplot as plt from ipywidgets import interact , FloatSlider from pyprojroot import here Linear Regression Linear regression is foundational to deep learning. It should be a model that everybody has been exposed to before in school. A humorous take I have heard about linear models is that if you zoom in enough into whatever system of the world you're modelling, anything can basically look linear. One of the advantages of linear models is its simplicity. It basically has two parameters, one explaining a \"baseline\" (intercept) and the other explaining strength of relationships (slope). Yet one of the disadvantages of linear models is also its simplicity. A linear model has a strong presumption of linearity. NOTE TO SELF: I need to rewrite this introduction. It is weak. Equation Form Linear regression, as a model, is expressed as follows: y = wx + b y = wx + b Here: The model is the equation, y = wx + b y = wx + b . y y is the output data. x x is our input data. w w is a slope parameter. b b is our intercept parameter. Implicit in the model is the fact that we have transformed y y by another function, the \"identity\" function, f(x) = x f(x) = x . In this model, y y and x x are, in a sense, \"fixed\", because this is the data that we have obtained. On the other hand, w w and b b are the parameters of interest, and we are interested in learning the parameter values for w w and b b that let our model best explain the data . Make Simulated Data To explore this idea in a bit more depth as applied to a linear regression model, let us start by making some simulated data with a bit of injected noise. Exercise: Simulate Data Fill in w_true and b_true with values that you like, or else leave them alone and follow along. from dl_workshop.answers import x , w_true , b_true , noise # exercise: specify ground truth w as w_true. # w_true = ... # exercise: specify ground truth b as b_true # b_true = ... # exercise: write a function to return the linear equation def make_y ( x , w , b ): \"\"\"Your answer here.\"\"\" return None # Comment out my answer below so it doesn't clobber over yours. from dl_workshop.answers import make_y y = make_y ( x , w_true , b_true ) # Plot ground truth data plt . scatter ( x , y ) plt . xlabel ( 'x' ) plt . ylabel ( 'y' ); /home/travis/build/ericmjl/dl-workshop/src/dl_workshop/answers.py:34: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console) from tqdm.autonotebook import tqdm WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) Text(0, 0.5, 'y') Exercise: Take bad guesses Now, let's plot what would be a very bad estimate of w w and b b . Replace the values assigned to w and b with something of your preference, or feel free to leave them alone and go on. # Plot a very bad estimate w = - 5 # exercise: fill in a bad value for w b = 3 # exercise: fill in a bad value for b y_est = w * x + b # exercise: fill in the equation. plt . plot ( x , y_est , color = 'red' , label = 'bad model' ) plt . scatter ( x , y , label = 'data' ) plt . xlabel ( 'x' ) plt . ylabel ( 'y' ) plt . legend (); <matplotlib.legend.Legend at 0x7f1e181d6cd0> Regression Loss Function How bad is our model? We can quantify this by looking at a metric called the \"mean squared error\". The mean squared error is defined as \"the average of the sum of squared errors\". \"Mean squared error\" is but one of many loss functions that are available in deep learning frameworks. It is commonly used for regression tasks. Loss functions are designed to quantify how bad our model is in predicting the data. Exercise: Mean Squared Error Implement the mean squred error function in NumPy code. def mse ( y_true : np . ndarray , y_pred : np . ndarray ) -> float : \"\"\"Implement the function here\"\"\" from dl_workshop.answers import mse # Calculate the mean squared error between print ( mse ( y , y_est )) 713.3526 Activity: Optimize model by hand. Now, we're going to optimize this model by hand. If you're viewing this on the website, I'd encourage you to launch a binder session to play around! import pandas as pd from ipywidgets import interact , FloatSlider import seaborn as sns @interact ( w = FloatSlider ( value = 0 , min =- 10 , max = 10 ), b = FloatSlider ( value = 0 , min =- 10 , max = 30 )) def plot_model ( w , b ): y_est = w * x + b plt . scatter ( x , y ) plt . plot ( x , y_est ) plt . title ( f \"MSE: { mse ( y , y_est ) : .2f } \" ) sns . despine () var element = $('#d18de1a2-5008-41d7-804d-54ff0e3313e0'); {\"version_major\": 2, \"version_minor\": 0, \"model_id\": \"2be5c8ae157143109a914a0ea64562cb\"} Loss Minimization As you were optimizing the model, notice what happens to the mean squared error score: it goes down! Implicit in what you were doing is gradient-based optimization. As a \"human\" doing the optimization, you were aware that you needed to move the sliders for w w and b b in particular directions in order to get a best-fit model. The thing we'd like to learn how to do now is to get a computer to automatically perform this procedure . Let's see how to make that happen. {\"state\": {\"42189a1123a14b0388acb219cc0195ee\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"75292878be2642888c1c07304604424d\": {\"model_name\": \"SliderStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"SliderStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\", \"handle_color\": null}}, \"382e8c734b3848e88929198e3b580500\": {\"model_name\": \"FloatSliderModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"FloatSliderModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"FloatSliderView\", \"continuous_update\": true, \"description\": \"w\", \"description_tooltip\": null, \"disabled\": false, \"layout\": \"IPY_MODEL_42189a1123a14b0388acb219cc0195ee\", \"max\": 10.0, \"min\": -10.0, \"orientation\": \"horizontal\", \"readout\": true, \"readout_format\": \".2f\", \"step\": 0.1, \"style\": \"IPY_MODEL_75292878be2642888c1c07304604424d\", \"value\": 0.0}}, \"ddd314fd716e43bc81544208866c6f7d\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"0ca6277947a946e19d8dcfda4e04f094\": {\"model_name\": \"SliderStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"SliderStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\", \"handle_color\": null}}, \"20089987abc24464853b91d842345a54\": {\"model_name\": \"FloatSliderModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"FloatSliderModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"FloatSliderView\", \"continuous_update\": true, \"description\": \"b\", \"description_tooltip\": null, \"disabled\": false, \"layout\": \"IPY_MODEL_ddd314fd716e43bc81544208866c6f7d\", \"max\": 30.0, \"min\": -10.0, \"orientation\": \"horizontal\", \"readout\": true, \"readout_format\": \".2f\", \"step\": 0.1, \"style\": \"IPY_MODEL_0ca6277947a946e19d8dcfda4e04f094\", \"value\": 0.0}}, \"dd2f4edf9f0040959fcc323f0f97b1bd\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"2be5c8ae157143109a914a0ea64562cb\": {\"model_name\": \"VBoxModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [\"widget-interact\"], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"VBoxModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"VBoxView\", \"box_style\": \"\", \"children\": [\"IPY_MODEL_382e8c734b3848e88929198e3b580500\", \"IPY_MODEL_20089987abc24464853b91d842345a54\", \"IPY_MODEL_5116f88287e1414aa2a2d83693096c49\"], \"layout\": \"IPY_MODEL_dd2f4edf9f0040959fcc323f0f97b1bd\"}}, \"60520a5f2d084fb7a4c946eb10a63e4b\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"5116f88287e1414aa2a2d83693096c49\": {\"model_name\": \"OutputModel\", \"model_module\": \"@jupyter-widgets/output\", \"model_module_version\": \"1.0.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/output\", \"_model_module_version\": \"1.0.0\", \"_model_name\": \"OutputModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/output\", \"_view_module_version\": \"1.0.0\", \"_view_name\": \"OutputView\", \"layout\": \"IPY_MODEL_60520a5f2d084fb7a4c946eb10a63e4b\", \"msg_id\": \"\", \"outputs\": [{\"output_type\": \"display_data\", \"metadata\": {\"image/png\": {\"width\": 368, \"height\": 263}, \"needs_background\": \"light\"}, \"data\": {\"text/plain\": \"<Figure size 432x288 with 1 Axes>\", \"image/png\": \"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\\n\"}}]}}}, \"version_major\": 2, \"version_minor\": 0}","title":"Neural Networks from Scratch"},{"location":"01-differential-programming/01-neural-nets-from-scratch/#neural-networks-from-scratch","text":"In this chapter, we are going to explore differential computing in the place where it was most highly leveraged: the training of neural networks. Now, as with all topics, to learn something most clearly, it pays to have an anchoring example that we start with. In this section, we'll lean heavily on linear regression as that anchoring example . We'll also explore what gradient-based optimization is, see an elementary example of that in action, and then connect those ideas back to optimization of a linear model. Once we're done there, then we'll see the exact same ideas in action with a logistic regression model, before finally seeing them in action again with a neural network model. The big takeaway from this chapter is that basically all supervised learning tasks can be broken into: model loss optimizer Hope you enjoy it! If you're ready, let's take a look at linear regression. import jax.numpy as np from jax import jit import numpy.random as npr import matplotlib.pyplot as plt from ipywidgets import interact , FloatSlider from pyprojroot import here","title":"Neural Networks from Scratch"},{"location":"01-differential-programming/01-neural-nets-from-scratch/#linear-regression","text":"Linear regression is foundational to deep learning. It should be a model that everybody has been exposed to before in school. A humorous take I have heard about linear models is that if you zoom in enough into whatever system of the world you're modelling, anything can basically look linear. One of the advantages of linear models is its simplicity. It basically has two parameters, one explaining a \"baseline\" (intercept) and the other explaining strength of relationships (slope). Yet one of the disadvantages of linear models is also its simplicity. A linear model has a strong presumption of linearity. NOTE TO SELF: I need to rewrite this introduction. It is weak.","title":"Linear Regression"},{"location":"01-differential-programming/01-neural-nets-from-scratch/#equation-form","text":"Linear regression, as a model, is expressed as follows: y = wx + b y = wx + b Here: The model is the equation, y = wx + b y = wx + b . y y is the output data. x x is our input data. w w is a slope parameter. b b is our intercept parameter. Implicit in the model is the fact that we have transformed y y by another function, the \"identity\" function, f(x) = x f(x) = x . In this model, y y and x x are, in a sense, \"fixed\", because this is the data that we have obtained. On the other hand, w w and b b are the parameters of interest, and we are interested in learning the parameter values for w w and b b that let our model best explain the data .","title":"Equation Form"},{"location":"01-differential-programming/01-neural-nets-from-scratch/#make-simulated-data","text":"To explore this idea in a bit more depth as applied to a linear regression model, let us start by making some simulated data with a bit of injected noise.","title":"Make Simulated Data"},{"location":"01-differential-programming/01-neural-nets-from-scratch/#exercise-simulate-data","text":"Fill in w_true and b_true with values that you like, or else leave them alone and follow along. from dl_workshop.answers import x , w_true , b_true , noise # exercise: specify ground truth w as w_true. # w_true = ... # exercise: specify ground truth b as b_true # b_true = ... # exercise: write a function to return the linear equation def make_y ( x , w , b ): \"\"\"Your answer here.\"\"\" return None # Comment out my answer below so it doesn't clobber over yours. from dl_workshop.answers import make_y y = make_y ( x , w_true , b_true ) # Plot ground truth data plt . scatter ( x , y ) plt . xlabel ( 'x' ) plt . ylabel ( 'y' ); /home/travis/build/ericmjl/dl-workshop/src/dl_workshop/answers.py:34: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console) from tqdm.autonotebook import tqdm WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) Text(0, 0.5, 'y')","title":"Exercise: Simulate Data"},{"location":"01-differential-programming/01-neural-nets-from-scratch/#exercise-take-bad-guesses","text":"Now, let's plot what would be a very bad estimate of w w and b b . Replace the values assigned to w and b with something of your preference, or feel free to leave them alone and go on. # Plot a very bad estimate w = - 5 # exercise: fill in a bad value for w b = 3 # exercise: fill in a bad value for b y_est = w * x + b # exercise: fill in the equation. plt . plot ( x , y_est , color = 'red' , label = 'bad model' ) plt . scatter ( x , y , label = 'data' ) plt . xlabel ( 'x' ) plt . ylabel ( 'y' ) plt . legend (); <matplotlib.legend.Legend at 0x7f1e181d6cd0>","title":"Exercise: Take bad guesses"},{"location":"01-differential-programming/01-neural-nets-from-scratch/#regression-loss-function","text":"How bad is our model? We can quantify this by looking at a metric called the \"mean squared error\". The mean squared error is defined as \"the average of the sum of squared errors\". \"Mean squared error\" is but one of many loss functions that are available in deep learning frameworks. It is commonly used for regression tasks. Loss functions are designed to quantify how bad our model is in predicting the data.","title":"Regression Loss Function"},{"location":"01-differential-programming/01-neural-nets-from-scratch/#exercise-mean-squared-error","text":"Implement the mean squred error function in NumPy code. def mse ( y_true : np . ndarray , y_pred : np . ndarray ) -> float : \"\"\"Implement the function here\"\"\" from dl_workshop.answers import mse # Calculate the mean squared error between print ( mse ( y , y_est )) 713.3526","title":"Exercise: Mean Squared Error"},{"location":"01-differential-programming/01-neural-nets-from-scratch/#activity-optimize-model-by-hand","text":"Now, we're going to optimize this model by hand. If you're viewing this on the website, I'd encourage you to launch a binder session to play around! import pandas as pd from ipywidgets import interact , FloatSlider import seaborn as sns @interact ( w = FloatSlider ( value = 0 , min =- 10 , max = 10 ), b = FloatSlider ( value = 0 , min =- 10 , max = 30 )) def plot_model ( w , b ): y_est = w * x + b plt . scatter ( x , y ) plt . plot ( x , y_est ) plt . title ( f \"MSE: { mse ( y , y_est ) : .2f } \" ) sns . despine () var element = $('#d18de1a2-5008-41d7-804d-54ff0e3313e0'); {\"version_major\": 2, \"version_minor\": 0, \"model_id\": \"2be5c8ae157143109a914a0ea64562cb\"}","title":"Activity: Optimize model by hand."},{"location":"01-differential-programming/01-neural-nets-from-scratch/#loss-minimization","text":"As you were optimizing the model, notice what happens to the mean squared error score: it goes down! Implicit in what you were doing is gradient-based optimization. As a \"human\" doing the optimization, you were aware that you needed to move the sliders for w w and b b in particular directions in order to get a best-fit model. The thing we'd like to learn how to do now is to get a computer to automatically perform this procedure . Let's see how to make that happen. {\"state\": {\"42189a1123a14b0388acb219cc0195ee\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"75292878be2642888c1c07304604424d\": {\"model_name\": \"SliderStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"SliderStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\", \"handle_color\": null}}, \"382e8c734b3848e88929198e3b580500\": {\"model_name\": \"FloatSliderModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"FloatSliderModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"FloatSliderView\", \"continuous_update\": true, \"description\": \"w\", \"description_tooltip\": null, \"disabled\": false, \"layout\": \"IPY_MODEL_42189a1123a14b0388acb219cc0195ee\", \"max\": 10.0, \"min\": -10.0, \"orientation\": \"horizontal\", \"readout\": true, \"readout_format\": \".2f\", \"step\": 0.1, \"style\": \"IPY_MODEL_75292878be2642888c1c07304604424d\", \"value\": 0.0}}, \"ddd314fd716e43bc81544208866c6f7d\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"0ca6277947a946e19d8dcfda4e04f094\": {\"model_name\": \"SliderStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"SliderStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\", \"handle_color\": null}}, \"20089987abc24464853b91d842345a54\": {\"model_name\": \"FloatSliderModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"FloatSliderModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"FloatSliderView\", \"continuous_update\": true, \"description\": \"b\", \"description_tooltip\": null, \"disabled\": false, \"layout\": \"IPY_MODEL_ddd314fd716e43bc81544208866c6f7d\", \"max\": 30.0, \"min\": -10.0, \"orientation\": \"horizontal\", \"readout\": true, \"readout_format\": \".2f\", \"step\": 0.1, \"style\": \"IPY_MODEL_0ca6277947a946e19d8dcfda4e04f094\", \"value\": 0.0}}, \"dd2f4edf9f0040959fcc323f0f97b1bd\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"2be5c8ae157143109a914a0ea64562cb\": {\"model_name\": \"VBoxModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [\"widget-interact\"], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"VBoxModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"VBoxView\", \"box_style\": \"\", \"children\": [\"IPY_MODEL_382e8c734b3848e88929198e3b580500\", \"IPY_MODEL_20089987abc24464853b91d842345a54\", \"IPY_MODEL_5116f88287e1414aa2a2d83693096c49\"], \"layout\": \"IPY_MODEL_dd2f4edf9f0040959fcc323f0f97b1bd\"}}, \"60520a5f2d084fb7a4c946eb10a63e4b\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"5116f88287e1414aa2a2d83693096c49\": {\"model_name\": \"OutputModel\", \"model_module\": \"@jupyter-widgets/output\", \"model_module_version\": \"1.0.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/output\", \"_model_module_version\": \"1.0.0\", \"_model_name\": \"OutputModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/output\", \"_view_module_version\": \"1.0.0\", \"_view_name\": \"OutputView\", \"layout\": \"IPY_MODEL_60520a5f2d084fb7a4c946eb10a63e4b\", \"msg_id\": \"\", \"outputs\": [{\"output_type\": \"display_data\", \"metadata\": {\"image/png\": {\"width\": 368, \"height\": 263}, \"needs_background\": \"light\"}, \"data\": {\"text/plain\": \"<Figure size 432x288 with 1 Axes>\", \"image/png\": \"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\\n\"}}]}}}, \"version_major\": 2, \"version_minor\": 0}","title":"Loss Minimization"},{"location":"01-differential-programming/02-gradient-optimization/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Gradient-Based Optimization Implicit in what you were doing was something we formally call \"gradient-based optimization\". This is a very important point to understand. If you get this for a linear model, you will understand how this works for more complex models. Hence, we are going to go into a small crash-course detour on what gradient-based optimization is. Derivatives At the risk of ticking off mathematicians for a sloppy definition, for this book's purposes, a useful way of defining the derivative is: How much our output changes as we take a small step on the inputs, taken in the limit of going to very small steps. If we have a function: f(w) = w^2 + 3w - 5 f(w) = w^2 + 3w - 5 What is the derivative of f(x) f(x) with respect to w w ? From first-year undergraduate calculus, we should be able to calculate this: f'(w) = 2w + 3 f'(w) = 2w + 3 As a matter of style, we will use the apostrophe marks to indicate derivatives. 1 apostrophe mark means first derivative, 2nd apostrophe mark means 2nd derivative, and so on. Minimizing f(w) f(w) Analytically What is the value of w w that minimizes f(w) f(w) ? Again, from undergraduate calculus, we know that at a minima of a function (whether it is a global or local), the first derivative will be equal to zero, i.e. f'(w) = 0 f'(w) = 0 . By taking advantage of this property, we can analytically solve for the value of w w at the minima. 2w + 3 = 0 2w + 3 = 0 Hence, w = -\\frac{3}{2} = 1.5 w = -\\frac{3}{2} = 1.5 To check whether the value of w w at the place where f'(w) = 0 f'(w) = 0 is a minima or maxima, we can use another piece of knowledge from 1st year undergraduate calculus: The sign of the second derivative will tell us whether this is a minima or maxima. If the second derivative is positive regardless of the value of w w , then the point is a minima. (Smiley faces are positive!) If the second derivative is negative regardless of the value of w w , then the point is a maxima. (Frowning faces are negative!) Hence, f''(w) = 2 f''(w) = 2 We can see that f''(w) > 0 f''(w) > 0 for all w w , hence the stationary point we find is going to be a local minima. Minimizing f(w) f(w) Computationally An alternative way of looking at this is to take advantage of f'(w) f'(w) , the gradient, evaluated at a particular w w . A known property of the gradient is that if you take steps in the negative direction of the gradient, you will eventually reach a function's minima. If you take small steps in the positive direction of the gradient, you will reach a function's maxima (if it exists). Exercise: Implement gradient functions by hand Let's implement this using the function f(w) f(w) , done using NumPy. Firstly, implement the aforementioned function f f below. # Exercise: Write f(w) as a function. def f ( w ): \"\"\"Your answer here.\"\"\" return None from dl_workshop.answers import f f ( 2.5 ) /home/travis/build/ericmjl/dl-workshop/src/dl_workshop/answers.py:34: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console) from tqdm.autonotebook import tqdm WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) 8.75 This function is the objective function that we wish to optimize, where \"optimization\" means finding the minima or maxima. Now, implement the gradient function \\frac{df}{dw} \\frac{df}{dw} below in the function df : # Exercise: Write df(w) as a function. def df ( w ): \"\"\"Your answer here\"\"\" return None from dl_workshop.answers import df df ( 2.5 ) 8.0 This function is the gradient of the objective w.r.t. the parameter of interest . It will help us find out the direction in which to change the parameter w w in order to optimize the objective function. Now, pick a number at random to start with. You can specify a number explicitly, or use a random number generator to draw a number. # Exercise: Pick a number to start w at. w = 10.0 # start with a float This gives us a starting point for optimization. Finally, write an \"optimization loop\", in which you adjust the value of w w in the negative direction of the gradient of f f w.r.t. w w (i.e. \\frac{df}{dw} \\frac{df}{dw} ). # Now, adjust the value of w 1000 times, taking small steps in the negative direction of the gradient. for i in range ( 1000 ): w = w - df ( w ) * 0.01 # 0.01 is the size of the step taken. print ( w ) -1.4999999806458753 Congratulations, you have just implemented gradient descent ! Gradient descent is an optimization routine : a way of programming a computer to do optimization for you so that you don't have to do it by hand. Minimizing f(w) f(w) with jax jax is a Python package for automatically computing gradients; it provides what is known as an \"automatic differentiation\" system on top of the NumPy API. This way, we do not have to specify the gradient function by hand-calculating it; rather, jax will know how to automatically take the derivative of a Python function w.r.t. the first argument, leveraging the chain rule to help calculate gradients. With jax , our example above is modified in only a slightly different way: from jax import grad import jax from tqdm.autonotebook import tqdm # This is what changes: we use autograd's `grad` function to automatically return a gradient function. df = grad ( f ) # Exercise: Pick a number to start w at. w = - 10.0 # Now, adjust the value of w 1000 times, taking small steps in the negative direction of the gradient. for i in range ( 1000 ): w = w - df ( w ) * 0.01 # 0.01 is the size of the step taken. print ( w ) -1.5000029 Summary In this section, we saw one way to program a computer to automatically leverage gradients to find the optima of a polynomial function. This builds our knowledge and intuition for the next section, in which we find the optimal point of a linear regression loss function.","title":"Gradient Based Optimization"},{"location":"01-differential-programming/02-gradient-optimization/#gradient-based-optimization","text":"Implicit in what you were doing was something we formally call \"gradient-based optimization\". This is a very important point to understand. If you get this for a linear model, you will understand how this works for more complex models. Hence, we are going to go into a small crash-course detour on what gradient-based optimization is.","title":"Gradient-Based Optimization"},{"location":"01-differential-programming/02-gradient-optimization/#derivatives","text":"At the risk of ticking off mathematicians for a sloppy definition, for this book's purposes, a useful way of defining the derivative is: How much our output changes as we take a small step on the inputs, taken in the limit of going to very small steps. If we have a function: f(w) = w^2 + 3w - 5 f(w) = w^2 + 3w - 5 What is the derivative of f(x) f(x) with respect to w w ? From first-year undergraduate calculus, we should be able to calculate this: f'(w) = 2w + 3 f'(w) = 2w + 3 As a matter of style, we will use the apostrophe marks to indicate derivatives. 1 apostrophe mark means first derivative, 2nd apostrophe mark means 2nd derivative, and so on.","title":"Derivatives"},{"location":"01-differential-programming/02-gradient-optimization/#minimizing-fwfw-analytically","text":"What is the value of w w that minimizes f(w) f(w) ? Again, from undergraduate calculus, we know that at a minima of a function (whether it is a global or local), the first derivative will be equal to zero, i.e. f'(w) = 0 f'(w) = 0 . By taking advantage of this property, we can analytically solve for the value of w w at the minima. 2w + 3 = 0 2w + 3 = 0 Hence, w = -\\frac{3}{2} = 1.5 w = -\\frac{3}{2} = 1.5 To check whether the value of w w at the place where f'(w) = 0 f'(w) = 0 is a minima or maxima, we can use another piece of knowledge from 1st year undergraduate calculus: The sign of the second derivative will tell us whether this is a minima or maxima. If the second derivative is positive regardless of the value of w w , then the point is a minima. (Smiley faces are positive!) If the second derivative is negative regardless of the value of w w , then the point is a maxima. (Frowning faces are negative!) Hence, f''(w) = 2 f''(w) = 2 We can see that f''(w) > 0 f''(w) > 0 for all w w , hence the stationary point we find is going to be a local minima.","title":"Minimizing f(w)f(w) Analytically"},{"location":"01-differential-programming/02-gradient-optimization/#minimizing-fwfw-computationally","text":"An alternative way of looking at this is to take advantage of f'(w) f'(w) , the gradient, evaluated at a particular w w . A known property of the gradient is that if you take steps in the negative direction of the gradient, you will eventually reach a function's minima. If you take small steps in the positive direction of the gradient, you will reach a function's maxima (if it exists).","title":"Minimizing f(w)f(w) Computationally"},{"location":"01-differential-programming/02-gradient-optimization/#exercise-implement-gradient-functions-by-hand","text":"Let's implement this using the function f(w) f(w) , done using NumPy. Firstly, implement the aforementioned function f f below. # Exercise: Write f(w) as a function. def f ( w ): \"\"\"Your answer here.\"\"\" return None from dl_workshop.answers import f f ( 2.5 ) /home/travis/build/ericmjl/dl-workshop/src/dl_workshop/answers.py:34: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console) from tqdm.autonotebook import tqdm WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) 8.75 This function is the objective function that we wish to optimize, where \"optimization\" means finding the minima or maxima. Now, implement the gradient function \\frac{df}{dw} \\frac{df}{dw} below in the function df : # Exercise: Write df(w) as a function. def df ( w ): \"\"\"Your answer here\"\"\" return None from dl_workshop.answers import df df ( 2.5 ) 8.0 This function is the gradient of the objective w.r.t. the parameter of interest . It will help us find out the direction in which to change the parameter w w in order to optimize the objective function. Now, pick a number at random to start with. You can specify a number explicitly, or use a random number generator to draw a number. # Exercise: Pick a number to start w at. w = 10.0 # start with a float This gives us a starting point for optimization. Finally, write an \"optimization loop\", in which you adjust the value of w w in the negative direction of the gradient of f f w.r.t. w w (i.e. \\frac{df}{dw} \\frac{df}{dw} ). # Now, adjust the value of w 1000 times, taking small steps in the negative direction of the gradient. for i in range ( 1000 ): w = w - df ( w ) * 0.01 # 0.01 is the size of the step taken. print ( w ) -1.4999999806458753 Congratulations, you have just implemented gradient descent ! Gradient descent is an optimization routine : a way of programming a computer to do optimization for you so that you don't have to do it by hand.","title":"Exercise: Implement gradient functions by hand"},{"location":"01-differential-programming/02-gradient-optimization/#minimizing-fwfw-with-jax","text":"jax is a Python package for automatically computing gradients; it provides what is known as an \"automatic differentiation\" system on top of the NumPy API. This way, we do not have to specify the gradient function by hand-calculating it; rather, jax will know how to automatically take the derivative of a Python function w.r.t. the first argument, leveraging the chain rule to help calculate gradients. With jax , our example above is modified in only a slightly different way: from jax import grad import jax from tqdm.autonotebook import tqdm # This is what changes: we use autograd's `grad` function to automatically return a gradient function. df = grad ( f ) # Exercise: Pick a number to start w at. w = - 10.0 # Now, adjust the value of w 1000 times, taking small steps in the negative direction of the gradient. for i in range ( 1000 ): w = w - df ( w ) * 0.01 # 0.01 is the size of the step taken. print ( w ) -1.5000029","title":"Minimizing f(w)f(w) with jax"},{"location":"01-differential-programming/02-gradient-optimization/#summary","text":"In this section, we saw one way to program a computer to automatically leverage gradients to find the optima of a polynomial function. This builds our knowledge and intuition for the next section, in which we find the optimal point of a linear regression loss function.","title":"Summary"},{"location":"01-differential-programming/03-linear-model-optimization/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Optimizing Linear Models What are we optimizing? In linear regression, we are: minimizing (i.e. optimizing) the loss function with respect to the linear regression parameters. Here are the parallels to the example above: In the example above, we minimized f(w) f(w) , the polynomial function. With linear regression, we are minimizing the mean squared error. In the example above, we minimized f(w) f(w) with respect to w w , where w w is the key parameter of f f . With linear regression, we minimize mean squared error of our model prediction with respect to the linear regression parameters. (Let's call the parameters collectively \\theta \\theta , such that \\theta = (w, b) \\theta = (w, b) . Ingredients for \"Optimizing\" a Model At this point, we have learned what the ingredients are for optimizing a model: A model, which is a function that maps inputs x x to outputs y y , and its parameters of the model. Not to belabour the point, but in our linear regression case, this is w w and b b ; Usually, in the literature, we call this parameter set \\theta \\theta , such that \\theta \\theta encompasses all parameters of the model. Loss function, which tells us how bad our predictions are. Optimization routine, which tells the computer how to adjust the parameter values to minimize the loss function. Keep note: Because we are optimizing the loss w.r.t. two parameters, finding the w w and b b coordinates that minimize the loss is like finding the minima of a bowl. The latter point, which is \"how to adjust the parameter values to minimize the loss function\", is the key point to understand here. Writing this in JAX/NumPy How do we optimize the parameters of our linear regression model using JAX? Let's explore how to do this. Exercise: Define the linear regression model Firstly, let's define our model function. Write it out as a Python function, named linear_model , such that the parameters \\theta \\theta are the first argument, and the data x are the second argument. It should return the model prediction. What should the data type of \\theta \\theta be? You can decide, as long as it's a built-in Python data type, or NumPy data type, or some combination of. # Exercise: Define the model in this function def linear_model ( theta , x ): pass from dl_workshop.answers import linear_model /home/travis/build/ericmjl/dl-workshop/src/dl_workshop/answers.py:34: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console) from tqdm.autonotebook import tqdm WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) Exercise: Initialize linear regression model parameters using random numbers Using a random number generator, such as the numpy.random.normal function, write a function that returns random number starting points for each linear model parameter. Make sure it returns params in the form that are accepted by the linear_model function defined above. Hint: NumPy's random module (which is distinct from JAX's) has been imported for you in the namespace npr . def initialize_linear_params (): pass # Comment this out if you fill in your answer above. from dl_workshop.answers import initialize_linear_params theta = initialize_linear_params () Exercise: Define the mean squared error loss function with linear model parameters as first argument Now, define the mean squared error loss function, called mseloss , such that 1. the parameters \\theta \\theta are accepted as the first argument, 2. model function as the second argument, 3. x as the third argument, 4. y as the fourth argument, and 5. returns a scalar valued result. This is the function we will be differentiating, and JAX's grad function will take the derivative of the function w.r.t. the first argument. Thus, \\theta \\theta must be the first argument! # Differentiable loss function w.r.t. 1st argument def mseloss ( theta , model , x , y ): pass from dl_workshop.answers import mseloss Now, we generate a new function called dmseloss , by calling grad on mseloss ! The new function dmseloss will have the exact same signature as mseloss , but will instead return the value of the gradient evaluated at each of the parameters in \\theta \\theta , in the same data structure as \\theta \\theta . # Put your answer here. # The actual dmseloss function is also present in the answers, # but _seriously_, go fill the one-liner to get dmseloss defined! # If you fill out the one-liner above, # remember to comment out the answer below # so that mine doesn't clobber over yours! from dl_workshop.answers import dmseloss I've provided an execution of the function below, so that you have an intuition of what's being returned. In my implementation, because theta are passed in as a 2-tuple, the gradients are returned as a 2-tuple as well. The return type will match up with how you pass in the parameters. from dl_workshop.answers import x , make_y , b_true , w_true # Create y by replacing my b_true and w_true with whatever you want y = make_y ( x , w_true , b_true ) dmseloss ( dict ( w = 0.3 , b = 0.5 ), linear_model , x , y ) {'b': DeviceArray(-38.470425, dtype=float32), 'w': DeviceArray(-29.11816, dtype=float32)} Exercise: Write the optimization routine Finally, write the optimization routine! Make it run for 3,000 iterations, and record the loss on each iteration. Don't forget to update your parameters! (How you do so will depend on how you've set up the parameters.) # Write your optimization routine below. # And if you implemented your optimization loop, # feel free to comment out the next two lines from dl_workshop.answers import model_optimization_loop losses , theta = model_optimization_loop ( theta , linear_model , mseloss , x , y , n_steps = 3000 ) var element = $('#95287778-a28d-489c-8bef-e75abe50a590'); {\"version_major\": 2, \"version_minor\": 0, \"model_id\": \"57ffd6c789ea4298bfadbf0869188e0a\"} Now, let's plot the loss score over time. It should be going downwards. import matplotlib.pyplot as plt plt . plot ( losses ) plt . xlabel ( 'iteration' ) plt . ylabel ( 'mse' ); Text(0, 0.5, 'mse') Inspect your parameters to see if they've become close to the true values! print ( theta ) {'w': DeviceArray(2.0125005, dtype=float32), 'b': DeviceArray(19.687845, dtype=float32)} Summary Ingredients of Linear Model From these first three sections, have seen how the following components play inside a linear model: Model specification (\"equations\", e.g. y = wx + b y = wx + b ) and the parameters of the model to be optimized ( w w and b b , or more generally, \\theta \\theta ). Loss function: tells us how wrong our model parameters are w.r.t. the data ( MSE MSE ) Optimization routine (for-loop) Let's now explore a few pictorial representations of the model. Linear Regression In Pictures Linear regression can be expressed pictorially, not just in equation form. Here are two ways of visualizing linear regression. Matrix Form Linear regression in one dimension looks like this: Linear regression in higher dimensions looks like this: This is also known in the statistical world as \"multiple linear regression\". The general idea, though, should be pretty easy to catch. You can do linear regression that projects any arbitrary number of input dimensions to any arbitrary number of output dimensions. Neural Diagram We can draw a \"neural diagram\" based on the matrix view, with the implicit \"identity\" function included in orange. The neural diagram is one that we commonly see in the introductions to deep learning. As you can see here, linear regression, when visualized this way, can be conceptually thought of as the baseline model for understanding deep learning. The neural diagram also expresses the \"compute graph\" that transforms input variables to output variables. {\"state\": {\"0b4bb5630f194a90849f770526a48ebd\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"c25bc75c23e64ff9bf51ed126bcb9d85\": {\"model_name\": \"ProgressStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"ProgressStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"bar_color\": null, \"description_width\": \"\"}}, \"39776312291347edbd67f705b7e08bd3\": {\"model_name\": \"FloatProgressModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"FloatProgressModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"ProgressView\", \"bar_style\": \"success\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_0b4bb5630f194a90849f770526a48ebd\", \"max\": 3000.0, \"min\": 0.0, \"orientation\": \"horizontal\", \"style\": \"IPY_MODEL_c25bc75c23e64ff9bf51ed126bcb9d85\", \"value\": 3000.0}}, \"7e7fe9836f07443b85bf0984693cd456\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"ead0defc95e4416d981da53a6fdb9119\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"052f4b9d0bd94eb3b10db3c979f176f1\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_7e7fe9836f07443b85bf0984693cd456\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_ead0defc95e4416d981da53a6fdb9119\", \"value\": \"100%\"}}, \"2a30fbe0f95e47a5ba8380e6b62f93e9\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"5c543323a45e427c9acd471bc91b0f1d\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"88033fcd6754415b8ea1ac2504ab287c\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_2a30fbe0f95e47a5ba8380e6b62f93e9\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_5c543323a45e427c9acd471bc91b0f1d\", \"value\": \" 3000/3000 [00:16&lt;00:00, 178.76it/s]\"}}, \"c00c577b3ada45689de96f1d466a4ee3\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"57ffd6c789ea4298bfadbf0869188e0a\": {\"model_name\": \"HBoxModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HBoxModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HBoxView\", \"box_style\": \"\", \"children\": [\"IPY_MODEL_052f4b9d0bd94eb3b10db3c979f176f1\", \"IPY_MODEL_39776312291347edbd67f705b7e08bd3\", \"IPY_MODEL_88033fcd6754415b8ea1ac2504ab287c\"], \"layout\": \"IPY_MODEL_c00c577b3ada45689de96f1d466a4ee3\"}}}, \"version_major\": 2, \"version_minor\": 0}","title":"Linear Model"},{"location":"01-differential-programming/03-linear-model-optimization/#optimizing-linear-models","text":"","title":"Optimizing Linear Models"},{"location":"01-differential-programming/03-linear-model-optimization/#what-are-we-optimizing","text":"In linear regression, we are: minimizing (i.e. optimizing) the loss function with respect to the linear regression parameters. Here are the parallels to the example above: In the example above, we minimized f(w) f(w) , the polynomial function. With linear regression, we are minimizing the mean squared error. In the example above, we minimized f(w) f(w) with respect to w w , where w w is the key parameter of f f . With linear regression, we minimize mean squared error of our model prediction with respect to the linear regression parameters. (Let's call the parameters collectively \\theta \\theta , such that \\theta = (w, b) \\theta = (w, b) .","title":"What are we optimizing?"},{"location":"01-differential-programming/03-linear-model-optimization/#ingredients-for-optimizing-a-model","text":"At this point, we have learned what the ingredients are for optimizing a model: A model, which is a function that maps inputs x x to outputs y y , and its parameters of the model. Not to belabour the point, but in our linear regression case, this is w w and b b ; Usually, in the literature, we call this parameter set \\theta \\theta , such that \\theta \\theta encompasses all parameters of the model. Loss function, which tells us how bad our predictions are. Optimization routine, which tells the computer how to adjust the parameter values to minimize the loss function. Keep note: Because we are optimizing the loss w.r.t. two parameters, finding the w w and b b coordinates that minimize the loss is like finding the minima of a bowl. The latter point, which is \"how to adjust the parameter values to minimize the loss function\", is the key point to understand here.","title":"Ingredients for \"Optimizing\" a Model"},{"location":"01-differential-programming/03-linear-model-optimization/#writing-this-in-jaxnumpy","text":"How do we optimize the parameters of our linear regression model using JAX? Let's explore how to do this.","title":"Writing this in JAX/NumPy"},{"location":"01-differential-programming/03-linear-model-optimization/#exercise-define-the-linear-regression-model","text":"Firstly, let's define our model function. Write it out as a Python function, named linear_model , such that the parameters \\theta \\theta are the first argument, and the data x are the second argument. It should return the model prediction. What should the data type of \\theta \\theta be? You can decide, as long as it's a built-in Python data type, or NumPy data type, or some combination of. # Exercise: Define the model in this function def linear_model ( theta , x ): pass from dl_workshop.answers import linear_model /home/travis/build/ericmjl/dl-workshop/src/dl_workshop/answers.py:34: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console) from tqdm.autonotebook import tqdm WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)","title":"Exercise: Define the linear regression model"},{"location":"01-differential-programming/03-linear-model-optimization/#exercise-initialize-linear-regression-model-parameters-using-random-numbers","text":"Using a random number generator, such as the numpy.random.normal function, write a function that returns random number starting points for each linear model parameter. Make sure it returns params in the form that are accepted by the linear_model function defined above. Hint: NumPy's random module (which is distinct from JAX's) has been imported for you in the namespace npr . def initialize_linear_params (): pass # Comment this out if you fill in your answer above. from dl_workshop.answers import initialize_linear_params theta = initialize_linear_params ()","title":"Exercise: Initialize linear regression model parameters using random numbers"},{"location":"01-differential-programming/03-linear-model-optimization/#exercise-define-the-mean-squared-error-loss-function-with-linear-model-parameters-as-first-argument","text":"Now, define the mean squared error loss function, called mseloss , such that 1. the parameters \\theta \\theta are accepted as the first argument, 2. model function as the second argument, 3. x as the third argument, 4. y as the fourth argument, and 5. returns a scalar valued result. This is the function we will be differentiating, and JAX's grad function will take the derivative of the function w.r.t. the first argument. Thus, \\theta \\theta must be the first argument! # Differentiable loss function w.r.t. 1st argument def mseloss ( theta , model , x , y ): pass from dl_workshop.answers import mseloss Now, we generate a new function called dmseloss , by calling grad on mseloss ! The new function dmseloss will have the exact same signature as mseloss , but will instead return the value of the gradient evaluated at each of the parameters in \\theta \\theta , in the same data structure as \\theta \\theta . # Put your answer here. # The actual dmseloss function is also present in the answers, # but _seriously_, go fill the one-liner to get dmseloss defined! # If you fill out the one-liner above, # remember to comment out the answer below # so that mine doesn't clobber over yours! from dl_workshop.answers import dmseloss I've provided an execution of the function below, so that you have an intuition of what's being returned. In my implementation, because theta are passed in as a 2-tuple, the gradients are returned as a 2-tuple as well. The return type will match up with how you pass in the parameters. from dl_workshop.answers import x , make_y , b_true , w_true # Create y by replacing my b_true and w_true with whatever you want y = make_y ( x , w_true , b_true ) dmseloss ( dict ( w = 0.3 , b = 0.5 ), linear_model , x , y ) {'b': DeviceArray(-38.470425, dtype=float32), 'w': DeviceArray(-29.11816, dtype=float32)}","title":"Exercise: Define the mean squared error loss function with linear model parameters as first argument"},{"location":"01-differential-programming/03-linear-model-optimization/#exercise-write-the-optimization-routine","text":"Finally, write the optimization routine! Make it run for 3,000 iterations, and record the loss on each iteration. Don't forget to update your parameters! (How you do so will depend on how you've set up the parameters.) # Write your optimization routine below. # And if you implemented your optimization loop, # feel free to comment out the next two lines from dl_workshop.answers import model_optimization_loop losses , theta = model_optimization_loop ( theta , linear_model , mseloss , x , y , n_steps = 3000 ) var element = $('#95287778-a28d-489c-8bef-e75abe50a590'); {\"version_major\": 2, \"version_minor\": 0, \"model_id\": \"57ffd6c789ea4298bfadbf0869188e0a\"} Now, let's plot the loss score over time. It should be going downwards. import matplotlib.pyplot as plt plt . plot ( losses ) plt . xlabel ( 'iteration' ) plt . ylabel ( 'mse' ); Text(0, 0.5, 'mse') Inspect your parameters to see if they've become close to the true values! print ( theta ) {'w': DeviceArray(2.0125005, dtype=float32), 'b': DeviceArray(19.687845, dtype=float32)}","title":"Exercise: Write the optimization routine"},{"location":"01-differential-programming/03-linear-model-optimization/#summary","text":"","title":"Summary"},{"location":"01-differential-programming/03-linear-model-optimization/#ingredients-of-linear-model","text":"From these first three sections, have seen how the following components play inside a linear model: Model specification (\"equations\", e.g. y = wx + b y = wx + b ) and the parameters of the model to be optimized ( w w and b b , or more generally, \\theta \\theta ). Loss function: tells us how wrong our model parameters are w.r.t. the data ( MSE MSE ) Optimization routine (for-loop) Let's now explore a few pictorial representations of the model.","title":"Ingredients of  Linear Model"},{"location":"01-differential-programming/03-linear-model-optimization/#linear-regression-in-pictures","text":"Linear regression can be expressed pictorially, not just in equation form. Here are two ways of visualizing linear regression.","title":"Linear Regression In Pictures"},{"location":"01-differential-programming/03-linear-model-optimization/#matrix-form","text":"Linear regression in one dimension looks like this: Linear regression in higher dimensions looks like this: This is also known in the statistical world as \"multiple linear regression\". The general idea, though, should be pretty easy to catch. You can do linear regression that projects any arbitrary number of input dimensions to any arbitrary number of output dimensions.","title":"Matrix Form"},{"location":"01-differential-programming/03-linear-model-optimization/#neural-diagram","text":"We can draw a \"neural diagram\" based on the matrix view, with the implicit \"identity\" function included in orange. The neural diagram is one that we commonly see in the introductions to deep learning. As you can see here, linear regression, when visualized this way, can be conceptually thought of as the baseline model for understanding deep learning. The neural diagram also expresses the \"compute graph\" that transforms input variables to output variables. {\"state\": {\"0b4bb5630f194a90849f770526a48ebd\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"c25bc75c23e64ff9bf51ed126bcb9d85\": {\"model_name\": \"ProgressStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"ProgressStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"bar_color\": null, \"description_width\": \"\"}}, \"39776312291347edbd67f705b7e08bd3\": {\"model_name\": \"FloatProgressModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"FloatProgressModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"ProgressView\", \"bar_style\": \"success\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_0b4bb5630f194a90849f770526a48ebd\", \"max\": 3000.0, \"min\": 0.0, \"orientation\": \"horizontal\", \"style\": \"IPY_MODEL_c25bc75c23e64ff9bf51ed126bcb9d85\", \"value\": 3000.0}}, \"7e7fe9836f07443b85bf0984693cd456\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"ead0defc95e4416d981da53a6fdb9119\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"052f4b9d0bd94eb3b10db3c979f176f1\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_7e7fe9836f07443b85bf0984693cd456\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_ead0defc95e4416d981da53a6fdb9119\", \"value\": \"100%\"}}, \"2a30fbe0f95e47a5ba8380e6b62f93e9\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"5c543323a45e427c9acd471bc91b0f1d\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"88033fcd6754415b8ea1ac2504ab287c\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_2a30fbe0f95e47a5ba8380e6b62f93e9\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_5c543323a45e427c9acd471bc91b0f1d\", \"value\": \" 3000/3000 [00:16&lt;00:00, 178.76it/s]\"}}, \"c00c577b3ada45689de96f1d466a4ee3\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"57ffd6c789ea4298bfadbf0869188e0a\": {\"model_name\": \"HBoxModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HBoxModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HBoxView\", \"box_style\": \"\", \"children\": [\"IPY_MODEL_052f4b9d0bd94eb3b10db3c979f176f1\", \"IPY_MODEL_39776312291347edbd67f705b7e08bd3\", \"IPY_MODEL_88033fcd6754415b8ea1ac2504ab287c\"], \"layout\": \"IPY_MODEL_c00c577b3ada45689de96f1d466a4ee3\"}}}, \"version_major\": 2, \"version_minor\": 0}","title":"Neural Diagram"},{"location":"01-differential-programming/04-logistic-regression/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Logistic Regression Logistic regression builds upon linear regression. We use logistic regression to perform binary classification , that is, distinguishing between two classes. Typically, we label one of the classes with the integer 0, and the other class with the integer 1. What does the model look like? To help you build intuition, let's visualize logistic regression using pictures again. Matrix Form Here is logistic regression in matrix form. Neural Diagram Now, here's logistic regression in a neural diagram: Interactive Activity As should be evident from the pictures, logistic regression builds upon linear regression simply by changing the activation function from an \"identity\" function to a \"logistic\" function . In the one-dimensional case, it has the same two parameters as one-dimensional linear regression, w w and b b . Let's use an interactive visualization to visualize how the parameters w w and b b affect the shape of the curve. (Note: this exercise is best done in a live notebook!) from dl_workshop.answers import logistic logistic ?? /home/travis/build/ericmjl/dl-workshop/src/dl_workshop/answers.py:34: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console) from tqdm.autonotebook import tqdm WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) import jax.numpy as np import matplotlib.pyplot as plt from ipywidgets import interact , FloatSlider @interact ( w = FloatSlider ( value = 0 , min =- 5 , max = 5 ), b = FloatSlider ( value = 0 , min =- 5 , max = 5 )) def plot_logistic ( w , b ): x = np . linspace ( - 10 , 10 , 1000 ) z = w * x + b # linear transform on x y = logistic ( z ) plt . plot ( x , y ) var element = $('#755886b1-df05-4bc0-859b-0fd63c932d69'); {\"version_major\": 2, \"version_minor\": 0, \"model_id\": \"7fd796252f234c20bca72e20ab7d1636\"} Parameters of the model As we can see, there are two parameters w w and b b . For those who may be encountering the model for the first time, this is what each of them control: w w controls how steep the step function is between 0 and 1 on the y-axis. Its sign also controls whether class 1 is associated with smaller values or larger values. b b controls the midpoint location of the curve. More negative values of b b shift it to the left; more positive values of b b shift it to the right. Make simulated data Once again, we are going to use simulated data to help us anchor our understanding. Along the way, we will see how logistic regression, once again, fits inside the same framework of \"model, loss, optimizer\". import numpy.random as npr x = np . linspace ( - 5 , 5 , 100 ) w = 2 b = 1 z = w * x + b + npr . random ( size = len ( x )) y_true = np . round ( logistic ( z )) plt . scatter ( x , y_true , alpha = 0.3 ); <matplotlib.collections.PathCollection at 0x7f35c05c8590> Here, we set w w to 2 and b b to 1, added some noise in there too, and rounded off the logistic-transformed values to between 0 and 1. Binary Classification Loss Function How would we quantify how good or bad our model is? In this case, we use the logistic loss function, also known as the binary cross entropy loss function. Expressed in equation form, it looks like this: L = -\\sum (y \\log(p) + (1-y)\\log(1-p) L = -\\sum (y \\log(p) + (1-y)\\log(1-p) Here: y y is the actual class, namely 1 1 or 0 0 . p p is the predicted class. If you're staring at this equation, and thinking that it looks a lot like the Bernoulli distribution log likelihood, you are right! Discussion Let's think about the loss function for a moment: What happens to the term y \\log(p) y \\log(p) when y=0 y=0 and y=1 y=1 ? What about the (1-y)\\log(1-p) (1-y)\\log(1-p) term? What happens to both terms when p \\approx 0 p \\approx 0 and when p \\approx 1 p \\approx 1 (but still bounded between 0 and 1)? The answers are as follows: When y=0 y=0 , $y \\log(p) = $, and when y=1 y=1 , (1-y)\\log(1-p) = 0 (1-y)\\log(1-p) = 0 . When p \\rightarrow 0 p \\rightarrow 0 , then \\log(p) \\log(p) approaches negative infinity. Likewise for \\log(1-p) \\log(1-p) when p \\rightarrow 1 p \\rightarrow 1 Exercise: Write down the logistic regression model Using the same patterns as you did before for the linear model, define a function called logistic_model , which accepts parameters theta and data x . # Exercise: Define logistic model def logistic_model ( theta , x ): pass from dl_workshop.answers import logistic_model Exercise: Write down the logistic loss function Now, write down the logistic loss function. It is defined as the negative binary cross entropy between the ground truth and the predicted. The binary cross entropy function is available for you to use: from dl_workshop.answers import binary_cross_entropy binary_cross_entropy ?? # Exercise: Define logistic loss function def logistic_loss ( params , model , x , y ): pass from dl_workshop.answers import logistic_loss logistic_loss ?? Now define the gradient of the loss function, using grad ! from jax import grad from dl_workshop.answers import dlogistic_loss # Exercise: Define gradient of loss function. # dlogistic_loss = ... Exercise: Initialize logistic regression model parameters using random numbers Because the parameters are identical to linear regression, you probably can use the same initialize_linear_params function. from dl_workshop.answers import initialize_linear_params theta = initialize_linear_params () theta {'w': -1.0499536814354014, 'b': 0.847987866837381} Exercise: Write the training loop! This will look very similar to the linear model training loop, because the same two parameters are being optimized. The thing that should change is the loss function and gradient of the loss function. from dl_workshop.answers import model_optimization_loop losses , theta = model_optimization_loop ( theta , logistic_model , logistic_loss , x , y_true , n_steps = 5000 , step_size = 0.0001 ) var element = $('#087b92b1-11da-4e62-9b1f-c1d12108a1f7'); {\"version_major\": 2, \"version_minor\": 0, \"model_id\": \"d3545474259940c1996a68a6cccc6cef\"} print ( theta ) {'w': DeviceArray(2.513813, dtype=float32), 'b': DeviceArray(1.6595516, dtype=float32)} You'll notice that the values are off from the true value. Why is this so? Partly it's because of the noise that we added, and we also rounded off values. Let's also print out the losses to check that \"learning\" has happened. plt . plot ( losses ); [<matplotlib.lines.Line2D at 0x7f35c0690ed0>] We might argue that the model hasn't yet converged, so we haven't yet figured out the parameters that best explain the data, given the model. And finally, checking the model against the actual values: plt . scatter ( x , y_true , alpha = 0.3 ) plt . plot ( x , logistic_model ( theta , x ), color = 'red' ); [<matplotlib.lines.Line2D at 0x7f35c0399650>] Indeed, we might say that the model parameters could have been optimized further, but as it stands, I'd say we did a pretty good job. Exercise What if we did not round off the values, and did not add noise to the original data? Try re-running the model without those two. Summary Let's recap what we learned here. We saw that logistic regression is nothing more than a natural extension of linear regression. We saw the introduction of the logistic loss function, and some of its properties. We finally saw that we could optimize a model, leveraging the same grad function from JAX. To reinforce point 1, let's look at logistic regression in matrix form again. See how there is an extra function g g (in yellow), which is the logistic function, that is tacked on. To further reinforce the ideas, we should look at the neural diagram once more. Once again, it's linear model + one more function. Remember this pattern: it will make neural networks much clearer in the next section! {\"state\": {\"85b80a2bb3a84945a41ef81db05f87fe\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"d36f491934e44616a25c528da6d82f66\": {\"model_name\": \"SliderStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"SliderStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\", \"handle_color\": null}}, \"e08a44053b27497c8c9d7c718fc68d3d\": {\"model_name\": \"FloatSliderModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"FloatSliderModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"FloatSliderView\", \"continuous_update\": true, \"description\": \"w\", \"description_tooltip\": null, \"disabled\": false, \"layout\": \"IPY_MODEL_85b80a2bb3a84945a41ef81db05f87fe\", \"max\": 5.0, \"min\": -5.0, \"orientation\": \"horizontal\", \"readout\": true, \"readout_format\": \".2f\", \"step\": 0.1, \"style\": \"IPY_MODEL_d36f491934e44616a25c528da6d82f66\", \"value\": 0.0}}, \"6e8043c021c6489d880842d0fab8b9b2\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"faee8720080e45bcad42b204e80f40f9\": {\"model_name\": \"SliderStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"SliderStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\", \"handle_color\": null}}, \"e1f9337d69754a3f95ad98ff9744c651\": {\"model_name\": \"FloatSliderModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"FloatSliderModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"FloatSliderView\", \"continuous_update\": true, \"description\": \"b\", \"description_tooltip\": null, \"disabled\": false, \"layout\": \"IPY_MODEL_6e8043c021c6489d880842d0fab8b9b2\", \"max\": 5.0, \"min\": -5.0, \"orientation\": \"horizontal\", \"readout\": true, \"readout_format\": \".2f\", \"step\": 0.1, \"style\": \"IPY_MODEL_faee8720080e45bcad42b204e80f40f9\", \"value\": 0.0}}, \"89e76504e74849489783e6af5c3f6eab\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"7fd796252f234c20bca72e20ab7d1636\": {\"model_name\": \"VBoxModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [\"widget-interact\"], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"VBoxModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"VBoxView\", \"box_style\": \"\", \"children\": [\"IPY_MODEL_e08a44053b27497c8c9d7c718fc68d3d\", \"IPY_MODEL_e1f9337d69754a3f95ad98ff9744c651\", \"IPY_MODEL_43ee0829040b45bdb113b4af0774a411\"], \"layout\": \"IPY_MODEL_89e76504e74849489783e6af5c3f6eab\"}}, \"0724c19a75b44a6898e7f141584524f5\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"43ee0829040b45bdb113b4af0774a411\": {\"model_name\": \"OutputModel\", \"model_module\": \"@jupyter-widgets/output\", \"model_module_version\": \"1.0.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/output\", \"_model_module_version\": \"1.0.0\", \"_model_name\": \"OutputModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/output\", \"_view_module_version\": \"1.0.0\", \"_view_name\": \"OutputView\", \"layout\": \"IPY_MODEL_0724c19a75b44a6898e7f141584524f5\", \"msg_id\": \"\", \"outputs\": [{\"output_type\": \"display_data\", \"metadata\": {\"image/png\": {\"width\": 378, \"height\": 248}, \"needs_background\": \"light\"}, \"data\": {\"text/plain\": \"<Figure size 432x288 with 1 Axes>\", \"image/png\": \"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\\n\"}}]}}, \"6517c7c9abb84b28bf9feb5eb352eaa4\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"ffd6ec3430f24af292d4253452622fd2\": {\"model_name\": \"ProgressStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"ProgressStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"bar_color\": null, \"description_width\": \"\"}}, \"cd86198e6bc045a887fda15d3f835742\": {\"model_name\": \"FloatProgressModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"FloatProgressModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"ProgressView\", \"bar_style\": \"success\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_6517c7c9abb84b28bf9feb5eb352eaa4\", \"max\": 5000.0, \"min\": 0.0, \"orientation\": \"horizontal\", \"style\": \"IPY_MODEL_ffd6ec3430f24af292d4253452622fd2\", \"value\": 5000.0}}, \"0fecbc1bda474013be3225b8176135b3\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"8956e18f717d40319c60c71600bf4ceb\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"deee57f6e7a9458f9aa4b8314f3a398d\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_0fecbc1bda474013be3225b8176135b3\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_8956e18f717d40319c60c71600bf4ceb\", \"value\": \"100%\"}}, \"3e6e2d706e504e339dedfccd6e78d4ef\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"9f141e1dc3184925ae2fe65451937450\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"205f2b105956436c980ca06b9d2e65ad\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_3e6e2d706e504e339dedfccd6e78d4ef\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_9f141e1dc3184925ae2fe65451937450\", \"value\": \" 5000/5000 [00:59&lt;00:00, 84.04it/s]\"}}, \"e1fc1c0a6e3f42fbaa31f892bce76786\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"d3545474259940c1996a68a6cccc6cef\": {\"model_name\": \"HBoxModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HBoxModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HBoxView\", \"box_style\": \"\", \"children\": [\"IPY_MODEL_deee57f6e7a9458f9aa4b8314f3a398d\", \"IPY_MODEL_cd86198e6bc045a887fda15d3f835742\", \"IPY_MODEL_205f2b105956436c980ca06b9d2e65ad\"], \"layout\": \"IPY_MODEL_e1fc1c0a6e3f42fbaa31f892bce76786\"}}}, \"version_major\": 2, \"version_minor\": 0}","title":"Logistic Regression"},{"location":"01-differential-programming/04-logistic-regression/#logistic-regression","text":"Logistic regression builds upon linear regression. We use logistic regression to perform binary classification , that is, distinguishing between two classes. Typically, we label one of the classes with the integer 0, and the other class with the integer 1. What does the model look like? To help you build intuition, let's visualize logistic regression using pictures again.","title":"Logistic Regression"},{"location":"01-differential-programming/04-logistic-regression/#matrix-form","text":"Here is logistic regression in matrix form.","title":"Matrix Form"},{"location":"01-differential-programming/04-logistic-regression/#neural-diagram","text":"Now, here's logistic regression in a neural diagram:","title":"Neural Diagram"},{"location":"01-differential-programming/04-logistic-regression/#interactive-activity","text":"As should be evident from the pictures, logistic regression builds upon linear regression simply by changing the activation function from an \"identity\" function to a \"logistic\" function . In the one-dimensional case, it has the same two parameters as one-dimensional linear regression, w w and b b . Let's use an interactive visualization to visualize how the parameters w w and b b affect the shape of the curve. (Note: this exercise is best done in a live notebook!) from dl_workshop.answers import logistic logistic ?? /home/travis/build/ericmjl/dl-workshop/src/dl_workshop/answers.py:34: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console) from tqdm.autonotebook import tqdm WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) import jax.numpy as np import matplotlib.pyplot as plt from ipywidgets import interact , FloatSlider @interact ( w = FloatSlider ( value = 0 , min =- 5 , max = 5 ), b = FloatSlider ( value = 0 , min =- 5 , max = 5 )) def plot_logistic ( w , b ): x = np . linspace ( - 10 , 10 , 1000 ) z = w * x + b # linear transform on x y = logistic ( z ) plt . plot ( x , y ) var element = $('#755886b1-df05-4bc0-859b-0fd63c932d69'); {\"version_major\": 2, \"version_minor\": 0, \"model_id\": \"7fd796252f234c20bca72e20ab7d1636\"}","title":"Interactive Activity"},{"location":"01-differential-programming/04-logistic-regression/#parameters-of-the-model","text":"As we can see, there are two parameters w w and b b . For those who may be encountering the model for the first time, this is what each of them control: w w controls how steep the step function is between 0 and 1 on the y-axis. Its sign also controls whether class 1 is associated with smaller values or larger values. b b controls the midpoint location of the curve. More negative values of b b shift it to the left; more positive values of b b shift it to the right.","title":"Parameters of the model"},{"location":"01-differential-programming/04-logistic-regression/#make-simulated-data","text":"Once again, we are going to use simulated data to help us anchor our understanding. Along the way, we will see how logistic regression, once again, fits inside the same framework of \"model, loss, optimizer\". import numpy.random as npr x = np . linspace ( - 5 , 5 , 100 ) w = 2 b = 1 z = w * x + b + npr . random ( size = len ( x )) y_true = np . round ( logistic ( z )) plt . scatter ( x , y_true , alpha = 0.3 ); <matplotlib.collections.PathCollection at 0x7f35c05c8590> Here, we set w w to 2 and b b to 1, added some noise in there too, and rounded off the logistic-transformed values to between 0 and 1.","title":"Make simulated data"},{"location":"01-differential-programming/04-logistic-regression/#binary-classification-loss-function","text":"How would we quantify how good or bad our model is? In this case, we use the logistic loss function, also known as the binary cross entropy loss function. Expressed in equation form, it looks like this: L = -\\sum (y \\log(p) + (1-y)\\log(1-p) L = -\\sum (y \\log(p) + (1-y)\\log(1-p) Here: y y is the actual class, namely 1 1 or 0 0 . p p is the predicted class. If you're staring at this equation, and thinking that it looks a lot like the Bernoulli distribution log likelihood, you are right!","title":"Binary Classification Loss Function"},{"location":"01-differential-programming/04-logistic-regression/#discussion","text":"Let's think about the loss function for a moment: What happens to the term y \\log(p) y \\log(p) when y=0 y=0 and y=1 y=1 ? What about the (1-y)\\log(1-p) (1-y)\\log(1-p) term? What happens to both terms when p \\approx 0 p \\approx 0 and when p \\approx 1 p \\approx 1 (but still bounded between 0 and 1)? The answers are as follows: When y=0 y=0 , $y \\log(p) = $, and when y=1 y=1 , (1-y)\\log(1-p) = 0 (1-y)\\log(1-p) = 0 . When p \\rightarrow 0 p \\rightarrow 0 , then \\log(p) \\log(p) approaches negative infinity. Likewise for \\log(1-p) \\log(1-p) when p \\rightarrow 1 p \\rightarrow 1","title":"Discussion"},{"location":"01-differential-programming/04-logistic-regression/#exercise-write-down-the-logistic-regression-model","text":"Using the same patterns as you did before for the linear model, define a function called logistic_model , which accepts parameters theta and data x . # Exercise: Define logistic model def logistic_model ( theta , x ): pass from dl_workshop.answers import logistic_model","title":"Exercise: Write down the logistic regression model"},{"location":"01-differential-programming/04-logistic-regression/#exercise-write-down-the-logistic-loss-function","text":"Now, write down the logistic loss function. It is defined as the negative binary cross entropy between the ground truth and the predicted. The binary cross entropy function is available for you to use: from dl_workshop.answers import binary_cross_entropy binary_cross_entropy ?? # Exercise: Define logistic loss function def logistic_loss ( params , model , x , y ): pass from dl_workshop.answers import logistic_loss logistic_loss ?? Now define the gradient of the loss function, using grad ! from jax import grad from dl_workshop.answers import dlogistic_loss # Exercise: Define gradient of loss function. # dlogistic_loss = ...","title":"Exercise: Write down the logistic loss function"},{"location":"01-differential-programming/04-logistic-regression/#exercise-initialize-logistic-regression-model-parameters-using-random-numbers","text":"Because the parameters are identical to linear regression, you probably can use the same initialize_linear_params function. from dl_workshop.answers import initialize_linear_params theta = initialize_linear_params () theta {'w': -1.0499536814354014, 'b': 0.847987866837381}","title":"Exercise: Initialize logistic regression model parameters using random numbers"},{"location":"01-differential-programming/04-logistic-regression/#exercise-write-the-training-loop","text":"This will look very similar to the linear model training loop, because the same two parameters are being optimized. The thing that should change is the loss function and gradient of the loss function. from dl_workshop.answers import model_optimization_loop losses , theta = model_optimization_loop ( theta , logistic_model , logistic_loss , x , y_true , n_steps = 5000 , step_size = 0.0001 ) var element = $('#087b92b1-11da-4e62-9b1f-c1d12108a1f7'); {\"version_major\": 2, \"version_minor\": 0, \"model_id\": \"d3545474259940c1996a68a6cccc6cef\"} print ( theta ) {'w': DeviceArray(2.513813, dtype=float32), 'b': DeviceArray(1.6595516, dtype=float32)} You'll notice that the values are off from the true value. Why is this so? Partly it's because of the noise that we added, and we also rounded off values. Let's also print out the losses to check that \"learning\" has happened. plt . plot ( losses ); [<matplotlib.lines.Line2D at 0x7f35c0690ed0>] We might argue that the model hasn't yet converged, so we haven't yet figured out the parameters that best explain the data, given the model. And finally, checking the model against the actual values: plt . scatter ( x , y_true , alpha = 0.3 ) plt . plot ( x , logistic_model ( theta , x ), color = 'red' ); [<matplotlib.lines.Line2D at 0x7f35c0399650>] Indeed, we might say that the model parameters could have been optimized further, but as it stands, I'd say we did a pretty good job.","title":"Exercise: Write the training loop!"},{"location":"01-differential-programming/04-logistic-regression/#exercise","text":"What if we did not round off the values, and did not add noise to the original data? Try re-running the model without those two.","title":"Exercise"},{"location":"01-differential-programming/04-logistic-regression/#summary","text":"Let's recap what we learned here. We saw that logistic regression is nothing more than a natural extension of linear regression. We saw the introduction of the logistic loss function, and some of its properties. We finally saw that we could optimize a model, leveraging the same grad function from JAX. To reinforce point 1, let's look at logistic regression in matrix form again. See how there is an extra function g g (in yellow), which is the logistic function, that is tacked on. To further reinforce the ideas, we should look at the neural diagram once more. Once again, it's linear model + one more function. Remember this pattern: it will make neural networks much clearer in the next section! {\"state\": {\"85b80a2bb3a84945a41ef81db05f87fe\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"d36f491934e44616a25c528da6d82f66\": {\"model_name\": \"SliderStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"SliderStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\", \"handle_color\": null}}, \"e08a44053b27497c8c9d7c718fc68d3d\": {\"model_name\": \"FloatSliderModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"FloatSliderModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"FloatSliderView\", \"continuous_update\": true, \"description\": \"w\", \"description_tooltip\": null, \"disabled\": false, \"layout\": \"IPY_MODEL_85b80a2bb3a84945a41ef81db05f87fe\", \"max\": 5.0, \"min\": -5.0, \"orientation\": \"horizontal\", \"readout\": true, \"readout_format\": \".2f\", \"step\": 0.1, \"style\": \"IPY_MODEL_d36f491934e44616a25c528da6d82f66\", \"value\": 0.0}}, \"6e8043c021c6489d880842d0fab8b9b2\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"faee8720080e45bcad42b204e80f40f9\": {\"model_name\": \"SliderStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"SliderStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\", \"handle_color\": null}}, \"e1f9337d69754a3f95ad98ff9744c651\": {\"model_name\": \"FloatSliderModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"FloatSliderModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"FloatSliderView\", \"continuous_update\": true, \"description\": \"b\", \"description_tooltip\": null, \"disabled\": false, \"layout\": \"IPY_MODEL_6e8043c021c6489d880842d0fab8b9b2\", \"max\": 5.0, \"min\": -5.0, \"orientation\": \"horizontal\", \"readout\": true, \"readout_format\": \".2f\", \"step\": 0.1, \"style\": \"IPY_MODEL_faee8720080e45bcad42b204e80f40f9\", \"value\": 0.0}}, \"89e76504e74849489783e6af5c3f6eab\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"7fd796252f234c20bca72e20ab7d1636\": {\"model_name\": \"VBoxModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [\"widget-interact\"], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"VBoxModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"VBoxView\", \"box_style\": \"\", \"children\": [\"IPY_MODEL_e08a44053b27497c8c9d7c718fc68d3d\", \"IPY_MODEL_e1f9337d69754a3f95ad98ff9744c651\", \"IPY_MODEL_43ee0829040b45bdb113b4af0774a411\"], \"layout\": \"IPY_MODEL_89e76504e74849489783e6af5c3f6eab\"}}, \"0724c19a75b44a6898e7f141584524f5\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"43ee0829040b45bdb113b4af0774a411\": {\"model_name\": \"OutputModel\", \"model_module\": \"@jupyter-widgets/output\", \"model_module_version\": \"1.0.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/output\", \"_model_module_version\": \"1.0.0\", \"_model_name\": \"OutputModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/output\", \"_view_module_version\": \"1.0.0\", \"_view_name\": \"OutputView\", \"layout\": \"IPY_MODEL_0724c19a75b44a6898e7f141584524f5\", \"msg_id\": \"\", \"outputs\": [{\"output_type\": \"display_data\", \"metadata\": {\"image/png\": {\"width\": 378, \"height\": 248}, \"needs_background\": \"light\"}, \"data\": {\"text/plain\": \"<Figure size 432x288 with 1 Axes>\", \"image/png\": \"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\\n\"}}]}}, \"6517c7c9abb84b28bf9feb5eb352eaa4\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"ffd6ec3430f24af292d4253452622fd2\": {\"model_name\": \"ProgressStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"ProgressStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"bar_color\": null, \"description_width\": \"\"}}, \"cd86198e6bc045a887fda15d3f835742\": {\"model_name\": \"FloatProgressModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"FloatProgressModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"ProgressView\", \"bar_style\": \"success\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_6517c7c9abb84b28bf9feb5eb352eaa4\", \"max\": 5000.0, \"min\": 0.0, \"orientation\": \"horizontal\", \"style\": \"IPY_MODEL_ffd6ec3430f24af292d4253452622fd2\", \"value\": 5000.0}}, \"0fecbc1bda474013be3225b8176135b3\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"8956e18f717d40319c60c71600bf4ceb\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"deee57f6e7a9458f9aa4b8314f3a398d\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_0fecbc1bda474013be3225b8176135b3\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_8956e18f717d40319c60c71600bf4ceb\", \"value\": \"100%\"}}, \"3e6e2d706e504e339dedfccd6e78d4ef\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"9f141e1dc3184925ae2fe65451937450\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"205f2b105956436c980ca06b9d2e65ad\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_3e6e2d706e504e339dedfccd6e78d4ef\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_9f141e1dc3184925ae2fe65451937450\", \"value\": \" 5000/5000 [00:59&lt;00:00, 84.04it/s]\"}}, \"e1fc1c0a6e3f42fbaa31f892bce76786\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"d3545474259940c1996a68a6cccc6cef\": {\"model_name\": \"HBoxModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HBoxModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HBoxView\", \"box_style\": \"\", \"children\": [\"IPY_MODEL_deee57f6e7a9458f9aa4b8314f3a398d\", \"IPY_MODEL_cd86198e6bc045a887fda15d3f835742\", \"IPY_MODEL_205f2b105956436c980ca06b9d2e65ad\"], \"layout\": \"IPY_MODEL_e1fc1c0a6e3f42fbaa31f892bce76786\"}}}, \"version_major\": 2, \"version_minor\": 0}","title":"Summary"},{"location":"01-differential-programming/05-neural-networks/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Neural Networks Neural networks are basically very powerful versions of logistic regressions. Like linear and logistic regression, they also take our data and map it to some output, but does so without ever knowing what the true equation form is. That's all a neural network model is: an arbitrarily powerful model. How do feed forward neural networks look like? To give you an intuition for this, let's see one example of a deep neural network in pictures. Pictorial form Matrix diagram As usual, in a matrix diagram. If this looks like a stack of logistic regression-like models, then you're right! This is the essence of what a neural network model is underneath the hood. Neural diagram And for correspondence, let's visualize this in a neural diagram. There's some syntax/nomenclature that we need to introduce here. Notice how w_1 w_1 reshapes our data to go from 4 inputs to 3 outputs. In neural network lingo, we call the 4 inputs \"4 input nodes\", and the 3 outputs \"3 hidden nodes\". If you are familiar with linear algebra operations, you'll know that this operation is a projection of data from 4 dimensions to 3 dimensions. The second set of weights, w_2 w_2 , take us from 3 dimensions to 1, and the 1 dimension at the end of the relu function is called the \"output node\". The orange functions are called activation functions , and they are a transformation on the linear projection steps (red and blue) that precede them. We've drawn the computation graph in a very explicit fashion, documenting every math transform in there. However, in the literature, you'll find that most authors omit the blue and orange steps, and instead leave them as implicit in their model illustrations, especially when they have, as a modelling choice, opted for identical activation functions. Using neural networks on some real data We are going to try using some real data from the UCI Machine Learning Repository to something related to the work that I am engaged in: predicting molecular properties from molecule descriptors. With the dataset below, we want to predict whether a compound is biodegradable based on a series of 41 chemical descriptors. This is a classical \"classification\" problem, where the output is a 1/0 result, much like the logistic regression problem from before. The dataset was taken from: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation#. I have also prepared the data such that it is split into X (the predictors) and Y (the class that we are trying to predict), so that you do not have to worry about manipulating pandas DataFrames. Let's read in the data. import pandas as pd from pyprojroot import here X = pd . read_csv ( here () / 'data/biodeg_X.csv' , index_col = 0 ) y = pd . read_csv ( here () / 'data/biodeg_y.csv' , index_col = 0 ) Neural network model definition Now, let's write a neural network model. The neural network model that we'll design must start with 41 input nodes, because there are 41 input values. As a modelling choice, we might decide to have 1 hidden layer with 20 nodes. Generally, this is arbitrary, but one general rule-of-thumb is to compress the data projection with fewer outputs than inputs. Finally, we must have an output layer with 1 node, as there is only column of data to predict on. Because this is a classification problem, we will use a logistic activation function right at the end. We'll start by instantiating a bunch of parameters. from dl_workshop.answers import noise params = dict () params [ 'w1' ] = noise (( 41 , 20 )) params [ 'b1' ] = noise (( 20 ,)) params [ 'w2' ] = noise (( 20 , 1 )) params [ 'b2' ] = noise (( 1 ,)) /home/travis/build/ericmjl/dl-workshop/src/dl_workshop/answers.py:34: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console) from tqdm.autonotebook import tqdm WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) Now, let's define the model as a Python function: import jax.numpy as np from dl_workshop.answers import logistic def neural_network_model ( theta , x ): # \"a1\" is the activation from layer 1 a1 = np . tanh ( np . dot ( x , theta [ 'w1' ]) + theta [ 'b1' ]) # \"a2\" is the activation from layer 2. a2 = logistic ( np . dot ( a1 , theta [ 'w2' ]) + theta [ 'b2' ]) return a2 Why do we need a \"logistic\" function at the end, rather than follow what was in the diagram above (relu)? This is because we are doing a classification problem, therefore we must squash the output to be between 0 and 1. Optimization loop Now, write the optimization loop! It will look very similar to the optimization loop that we wrote for the logistic regression classification model. The difference here is the model that is used, as well as the initialized set of parameters. from dl_workshop.answers import model_optimization_loop , logistic_loss losses , params = model_optimization_loop ( params , neural_network_model , logistic_loss , X . values , y . values , step_size = 0.0001 ) var element = $('#b36dbd69-7277-48ae-ba57-c51b8c711ad3'); {\"version_major\": 2, \"version_minor\": 0, \"model_id\": \"985186af77d44e1fbcf79dfda5f4ff04\"} import matplotlib.pyplot as plt plt . plot ( losses ) plt . title ( f \"final loss: { losses [ - 1 ] : .2f } \" ); Text(0.5, 1.0, 'final loss: 195.28') Visualize trained model performance We can use a confusion matrix to see how \"confused\" a model was. Read more on Wikipedia . from sklearn.metrics import confusion_matrix y_pred = neural_network_model ( params , X . values ) confusion_matrix ( y , np . round ( y_pred )) array([[665, 34], [ 40, 316]]) import seaborn as sns sns . heatmap ( confusion_matrix ( y , np . round ( y_pred ))) plt . xlabel ( 'predicted' ) plt . ylabel ( 'actual' ); Text(33.0, 0.5, 'actual') Recap Deep learning, and more generally any modelling, has the following ingredients: A model and its associated parameters to be optimized. A loss function against which we are optimizing parameters. An optimization routine. You have seen these three ingredients at play with 3 different models: a linear regression model, a logistic regression model, and a deep feed forward neural network model. In Pictures Here is a summary of what we've learned in this tutorial! Caveats thus far Deep learning is an active field of research. I have only shown you the basics here. In addition, I have also intentionally omitted certain aspects of machine learning practice, such as splitting our data into training and testing sets, performing model selection using cross-validation tuning hyperparameters, such as trying out optimizers regularizing the model, using L1/L2 regularization or dropout etc. Parting Thoughts Deep learning is nothing more than optimization of a model with a really large number of parameters. In its current state, it is not artificial intelligence. You should not be afraid of it; it is nothing more than a really powerful model that maps X to Y. {\"state\": {\"a193c7bc1c4d43e9944909d5d109141d\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"ccba909c80254c71a16b8c83db6fa321\": {\"model_name\": \"ProgressStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"ProgressStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"bar_color\": null, \"description_width\": \"\"}}, \"2152d4026037461186e590b8c7892d9c\": {\"model_name\": \"FloatProgressModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"FloatProgressModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"ProgressView\", \"bar_style\": \"success\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_a193c7bc1c4d43e9944909d5d109141d\", \"max\": 3000.0, \"min\": 0.0, \"orientation\": \"horizontal\", \"style\": \"IPY_MODEL_ccba909c80254c71a16b8c83db6fa321\", \"value\": 3000.0}}, \"f3e0253b76b444cbaadeb2eaf858174b\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"91b18de5b8844a4abc63470107f5b076\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"d859fe0f08c44388aa6a411db2254fa0\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_f3e0253b76b444cbaadeb2eaf858174b\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_91b18de5b8844a4abc63470107f5b076\", \"value\": \"100%\"}}, \"3a3a52cb79674d1a8233a044a246d6f4\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"1d8540399a87413488a425f1f820a93c\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"54eeb01cd4c34aafa8722eb2a0ed7a53\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_3a3a52cb79674d1a8233a044a246d6f4\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_1d8540399a87413488a425f1f820a93c\", \"value\": \" 3000/3000 [01:05&lt;00:00, 46.10it/s]\"}}, \"1c1bd3e1bcc947a8a50d5dd616485dfc\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"985186af77d44e1fbcf79dfda5f4ff04\": {\"model_name\": \"HBoxModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HBoxModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HBoxView\", \"box_style\": \"\", \"children\": [\"IPY_MODEL_d859fe0f08c44388aa6a411db2254fa0\", \"IPY_MODEL_2152d4026037461186e590b8c7892d9c\", \"IPY_MODEL_54eeb01cd4c34aafa8722eb2a0ed7a53\"], \"layout\": \"IPY_MODEL_1c1bd3e1bcc947a8a50d5dd616485dfc\"}}}, \"version_major\": 2, \"version_minor\": 0}","title":"Neural Networks"},{"location":"01-differential-programming/05-neural-networks/#neural-networks","text":"Neural networks are basically very powerful versions of logistic regressions. Like linear and logistic regression, they also take our data and map it to some output, but does so without ever knowing what the true equation form is. That's all a neural network model is: an arbitrarily powerful model. How do feed forward neural networks look like? To give you an intuition for this, let's see one example of a deep neural network in pictures.","title":"Neural Networks"},{"location":"01-differential-programming/05-neural-networks/#pictorial-form","text":"","title":"Pictorial form"},{"location":"01-differential-programming/05-neural-networks/#matrix-diagram","text":"As usual, in a matrix diagram. If this looks like a stack of logistic regression-like models, then you're right! This is the essence of what a neural network model is underneath the hood.","title":"Matrix diagram"},{"location":"01-differential-programming/05-neural-networks/#neural-diagram","text":"And for correspondence, let's visualize this in a neural diagram. There's some syntax/nomenclature that we need to introduce here. Notice how w_1 w_1 reshapes our data to go from 4 inputs to 3 outputs. In neural network lingo, we call the 4 inputs \"4 input nodes\", and the 3 outputs \"3 hidden nodes\". If you are familiar with linear algebra operations, you'll know that this operation is a projection of data from 4 dimensions to 3 dimensions. The second set of weights, w_2 w_2 , take us from 3 dimensions to 1, and the 1 dimension at the end of the relu function is called the \"output node\". The orange functions are called activation functions , and they are a transformation on the linear projection steps (red and blue) that precede them. We've drawn the computation graph in a very explicit fashion, documenting every math transform in there. However, in the literature, you'll find that most authors omit the blue and orange steps, and instead leave them as implicit in their model illustrations, especially when they have, as a modelling choice, opted for identical activation functions.","title":"Neural diagram"},{"location":"01-differential-programming/05-neural-networks/#using-neural-networks-on-some-real-data","text":"We are going to try using some real data from the UCI Machine Learning Repository to something related to the work that I am engaged in: predicting molecular properties from molecule descriptors. With the dataset below, we want to predict whether a compound is biodegradable based on a series of 41 chemical descriptors. This is a classical \"classification\" problem, where the output is a 1/0 result, much like the logistic regression problem from before. The dataset was taken from: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation#. I have also prepared the data such that it is split into X (the predictors) and Y (the class that we are trying to predict), so that you do not have to worry about manipulating pandas DataFrames. Let's read in the data. import pandas as pd from pyprojroot import here X = pd . read_csv ( here () / 'data/biodeg_X.csv' , index_col = 0 ) y = pd . read_csv ( here () / 'data/biodeg_y.csv' , index_col = 0 )","title":"Using neural networks on some real data"},{"location":"01-differential-programming/05-neural-networks/#neural-network-model-definition","text":"Now, let's write a neural network model. The neural network model that we'll design must start with 41 input nodes, because there are 41 input values. As a modelling choice, we might decide to have 1 hidden layer with 20 nodes. Generally, this is arbitrary, but one general rule-of-thumb is to compress the data projection with fewer outputs than inputs. Finally, we must have an output layer with 1 node, as there is only column of data to predict on. Because this is a classification problem, we will use a logistic activation function right at the end. We'll start by instantiating a bunch of parameters. from dl_workshop.answers import noise params = dict () params [ 'w1' ] = noise (( 41 , 20 )) params [ 'b1' ] = noise (( 20 ,)) params [ 'w2' ] = noise (( 20 , 1 )) params [ 'b2' ] = noise (( 1 ,)) /home/travis/build/ericmjl/dl-workshop/src/dl_workshop/answers.py:34: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console) from tqdm.autonotebook import tqdm WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) Now, let's define the model as a Python function: import jax.numpy as np from dl_workshop.answers import logistic def neural_network_model ( theta , x ): # \"a1\" is the activation from layer 1 a1 = np . tanh ( np . dot ( x , theta [ 'w1' ]) + theta [ 'b1' ]) # \"a2\" is the activation from layer 2. a2 = logistic ( np . dot ( a1 , theta [ 'w2' ]) + theta [ 'b2' ]) return a2 Why do we need a \"logistic\" function at the end, rather than follow what was in the diagram above (relu)? This is because we are doing a classification problem, therefore we must squash the output to be between 0 and 1.","title":"Neural network model definition"},{"location":"01-differential-programming/05-neural-networks/#optimization-loop","text":"Now, write the optimization loop! It will look very similar to the optimization loop that we wrote for the logistic regression classification model. The difference here is the model that is used, as well as the initialized set of parameters. from dl_workshop.answers import model_optimization_loop , logistic_loss losses , params = model_optimization_loop ( params , neural_network_model , logistic_loss , X . values , y . values , step_size = 0.0001 ) var element = $('#b36dbd69-7277-48ae-ba57-c51b8c711ad3'); {\"version_major\": 2, \"version_minor\": 0, \"model_id\": \"985186af77d44e1fbcf79dfda5f4ff04\"} import matplotlib.pyplot as plt plt . plot ( losses ) plt . title ( f \"final loss: { losses [ - 1 ] : .2f } \" ); Text(0.5, 1.0, 'final loss: 195.28')","title":"Optimization loop"},{"location":"01-differential-programming/05-neural-networks/#visualize-trained-model-performance","text":"We can use a confusion matrix to see how \"confused\" a model was. Read more on Wikipedia . from sklearn.metrics import confusion_matrix y_pred = neural_network_model ( params , X . values ) confusion_matrix ( y , np . round ( y_pred )) array([[665, 34], [ 40, 316]]) import seaborn as sns sns . heatmap ( confusion_matrix ( y , np . round ( y_pred ))) plt . xlabel ( 'predicted' ) plt . ylabel ( 'actual' ); Text(33.0, 0.5, 'actual')","title":"Visualize trained model performance"},{"location":"01-differential-programming/05-neural-networks/#recap","text":"Deep learning, and more generally any modelling, has the following ingredients: A model and its associated parameters to be optimized. A loss function against which we are optimizing parameters. An optimization routine. You have seen these three ingredients at play with 3 different models: a linear regression model, a logistic regression model, and a deep feed forward neural network model.","title":"Recap"},{"location":"01-differential-programming/05-neural-networks/#in-pictures","text":"Here is a summary of what we've learned in this tutorial!","title":"In Pictures"},{"location":"01-differential-programming/05-neural-networks/#caveats-thus-far","text":"Deep learning is an active field of research. I have only shown you the basics here. In addition, I have also intentionally omitted certain aspects of machine learning practice, such as splitting our data into training and testing sets, performing model selection using cross-validation tuning hyperparameters, such as trying out optimizers regularizing the model, using L1/L2 regularization or dropout etc.","title":"Caveats thus far"},{"location":"01-differential-programming/05-neural-networks/#parting-thoughts","text":"Deep learning is nothing more than optimization of a model with a really large number of parameters. In its current state, it is not artificial intelligence. You should not be afraid of it; it is nothing more than a really powerful model that maps X to Y. {\"state\": {\"a193c7bc1c4d43e9944909d5d109141d\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"ccba909c80254c71a16b8c83db6fa321\": {\"model_name\": \"ProgressStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"ProgressStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"bar_color\": null, \"description_width\": \"\"}}, \"2152d4026037461186e590b8c7892d9c\": {\"model_name\": \"FloatProgressModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"FloatProgressModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"ProgressView\", \"bar_style\": \"success\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_a193c7bc1c4d43e9944909d5d109141d\", \"max\": 3000.0, \"min\": 0.0, \"orientation\": \"horizontal\", \"style\": \"IPY_MODEL_ccba909c80254c71a16b8c83db6fa321\", \"value\": 3000.0}}, \"f3e0253b76b444cbaadeb2eaf858174b\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"91b18de5b8844a4abc63470107f5b076\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"d859fe0f08c44388aa6a411db2254fa0\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_f3e0253b76b444cbaadeb2eaf858174b\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_91b18de5b8844a4abc63470107f5b076\", \"value\": \"100%\"}}, \"3a3a52cb79674d1a8233a044a246d6f4\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"1d8540399a87413488a425f1f820a93c\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"54eeb01cd4c34aafa8722eb2a0ed7a53\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_3a3a52cb79674d1a8233a044a246d6f4\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_1d8540399a87413488a425f1f820a93c\", \"value\": \" 3000/3000 [01:05&lt;00:00, 46.10it/s]\"}}, \"1c1bd3e1bcc947a8a50d5dd616485dfc\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"985186af77d44e1fbcf79dfda5f4ff04\": {\"model_name\": \"HBoxModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HBoxModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HBoxView\", \"box_style\": \"\", \"children\": [\"IPY_MODEL_d859fe0f08c44388aa6a411db2254fa0\", \"IPY_MODEL_2152d4026037461186e590b8c7892d9c\", \"IPY_MODEL_54eeb01cd4c34aafa8722eb2a0ed7a53\"], \"layout\": \"IPY_MODEL_1c1bd3e1bcc947a8a50d5dd616485dfc\"}}}, \"version_major\": 2, \"version_minor\": 0}","title":"Parting Thoughts"},{"location":"02-jax-idioms/00-introduction/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Introduction In this chapter, we'll introduce you to some idiomatic JAX tools that will help you write performant numerical array programs. The main takeaways you should get from this notebook are how you can: Replace slow Python for loop constructs with fast, just-in-time compiled JAX loop constructs, Create deterministic random numbers (and yes, this is not an oxymoron!) for reproducibility, Freely mix-and-match through this idea of composable transforms . Because contrasts to what we might be used to doing are the the most effective way to teach and learn, in each section, we'll be explicit about what exactly we're replacing when we write these numerical array programs. In doing so, my hope is that you'll see very clearly that structuring your array programs in a composable and atomic fashion will help you take advantage of JAX's composable function transforms to write really fast and compiled functions. And for good measure, we'll contrast this against pure Python programs, so you can witness for yourself how powerful JAX's ideas are... and appreciate how much effort has gone into making the whole thing NumPy compatible! Prerequisites To get the most out of this notebook, you need only be familiar with the NumPy API, and writing functions. Having an appreciation of functools.partial , will help a bit, because we use it a lot in writing JAX programs. However, I know that not everybody has had prior experience with partial -ed functions, so we will introduce the idea mid-way, in a just-in-time fashion as well. If you've gone through tutorial.ipynb , which is the main tutorial notebook for this repository, then you'll have some appreciation of JAX's composable transforms. You'll also see how we wrote some loops in there, and hopefully have an appreciation of how much faster things will run when we use JAX's looping constructs instead.","title":"Introduction"},{"location":"02-jax-idioms/00-introduction/#introduction","text":"In this chapter, we'll introduce you to some idiomatic JAX tools that will help you write performant numerical array programs. The main takeaways you should get from this notebook are how you can: Replace slow Python for loop constructs with fast, just-in-time compiled JAX loop constructs, Create deterministic random numbers (and yes, this is not an oxymoron!) for reproducibility, Freely mix-and-match through this idea of composable transforms . Because contrasts to what we might be used to doing are the the most effective way to teach and learn, in each section, we'll be explicit about what exactly we're replacing when we write these numerical array programs. In doing so, my hope is that you'll see very clearly that structuring your array programs in a composable and atomic fashion will help you take advantage of JAX's composable function transforms to write really fast and compiled functions. And for good measure, we'll contrast this against pure Python programs, so you can witness for yourself how powerful JAX's ideas are... and appreciate how much effort has gone into making the whole thing NumPy compatible!","title":"Introduction"},{"location":"02-jax-idioms/00-introduction/#prerequisites","text":"To get the most out of this notebook, you need only be familiar with the NumPy API, and writing functions. Having an appreciation of functools.partial , will help a bit, because we use it a lot in writing JAX programs. However, I know that not everybody has had prior experience with partial -ed functions, so we will introduce the idea mid-way, in a just-in-time fashion as well. If you've gone through tutorial.ipynb , which is the main tutorial notebook for this repository, then you'll have some appreciation of JAX's composable transforms. You'll also see how we wrote some loops in there, and hopefully have an appreciation of how much faster things will run when we use JAX's looping constructs instead.","title":"Prerequisites"},{"location":"02-jax-idioms/01-vmap/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Replacing simple for-loops with vmap The first JAX thing we will look at is the vmap function. What does vmap do? From the JAX docs on vmap : Vectorizing map. Creates a function which maps fun over argument axes. What does that mean? Well, let's take a look at a few classic examples. Mapping an elementwise function over an array's leading axis The first example is mapping a function over an array axis. The simplest example, which is a bit trivial, is doing elementwise application of a function. Say we have uniformly spaced numbers from 0 to 1 in an array: import jax.numpy as np from jax import vmap from time import time arr = np . linspace ( 0 , 1 , 10000 ) arr WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) DeviceArray([0.0000000e+00, 1.0001000e-04, 2.0002000e-04, ..., 9.9979997e-01, 9.9989998e-01, 1.0000000e+00], dtype=float32) If we wanted to apply an exponential transform on every element, the \"dumb\", pure Python way to do so is to write a for-loop: start = time () new_arr = [] for element in arr : new_arr . append ( np . exp ( element )) new_arr = np . array ( new_arr ) end = time () print ( f \" { end - start : .2f } seconds\" ) new_arr 3.55 seconds DeviceArray([1. , 1.0001 , 1.0002 , ..., 2.7177382, 2.71801 , 2.7182817], dtype=float32) Because np.exp is a NumPy ufunc that operates on individual elements, we can call np.exp on arr directly: start = time () new_arr = np . exp ( arr ) end = time () print ( f \" { end - start : .4f } seconds\" ) new_arr 0.0318 seconds DeviceArray([1. , 1.0001 , 1.0002 , ..., 2.7177382, 2.71801 , 2.7182817], dtype=float32) As you can see, this is much faster. This, incidentally, is equivalent to using vmap to map the function across all elements in the array: start = time () new_arr = vmap ( np . exp )( arr ) end = time () print ( f \" { end - start : .4f } seconds\" ) new_arr 0.0010 seconds DeviceArray([1. , 1.0001 , 1.0002 , ..., 2.7177382, 2.71801 , 2.7182817], dtype=float32) It's a bit slower, but one thing we gain from using vmap is the ability to ignore the leading (first) array axis of every element that is passed into the vmap -ed function. To see that, we're going to look at another example. Mapping a row-wise function across an array's leading axis In this example let's say we have a matrix of values that we measured in an experiment. There were n_samples measured, and 3 unique properties that we collected, thereby giving us a matrix of shape (n_samples, 3) . If we needed to find their sum, we could do the following in pure NumPy: def row_sum ( data ): \"\"\"Given one dataset, calculate row-wise sum of data.\"\"\" return np . sum ( data , axis = 1 ) data = np . array ([ [ 1 , 3 , 1 ,], [ 3 , 5 , 1 ,], [ 1 , 2 , 5 ,], [ 7 , 1 , 3 ,], [ 11 , 2 , 3 ,], ]) start = time () result = row_sum ( data ) end = time () print ( f \" { end - start : .4f } seconds\" ) result 0.0125 seconds DeviceArray([ 5, 9, 8, 11, 16], dtype=int32) This would give us the correct answer... but we had to worry about the \"axis\" argument, which is a bit irritating. Instead, we could use first transform np.sum into a vmapped function that is mapped across the leading axis of data : def row_sum_one_data ( data ): \"\"\"Given one dataset, calculate row-wise sum of data.\"\"\" return vmap ( np . sum )( data ) start = time () result = row_sum_one_data ( data ) end = time () print ( f \" { end - start : .4f } seconds\" ) result 0.0012 seconds DeviceArray([ 5, 9, 8, 11, 16], dtype=int32) Thereby giving us the exact same result. While the syntax does take some time to get used to, it does more explicitly and clearly expresses the idea that we don't really care about summing over the leading axis . Now, let's say we had multiple datasets for which we wanted to calculate the row-wise sum. How would we do this in pure NumPy? Well, let's first create this dataset. data2 = np . array ([ [ 1 , 3 , 7 ,], [ 3 , 5 , 11 ,], [ 3 , 2 , 5 ,], [ 7 , 5 , 3 ,], [ 11 , 5 , 3 ,], ]) combined_data = np . moveaxis ( np . dstack ([ data , data2 ], ), 2 , 0 ) combined_data . shape (2, 5, 3) Our shapes tell us that we have 2 stacks of data, each with 5 rows and 3 columns. Since we want row-wise summation, but want to preserve the 2 stacks of data, we have to now worry about which axes to collapse: np . sum ( combined_data , axis = 2 ) DeviceArray([[ 5, 9, 8, 11, 16], [11, 19, 10, 15, 19]], dtype=int32) This is all cool, but we now have a \"magic number\" in our program. We can eliminate this magic number by instead doing vmapping row_sum_over_data across the combined_data array: def row_sum_all_data ( data ): return vmap ( row_sum_one_data )( data ) row_sum_all_data ( combined_data ) DeviceArray([[ 5, 9, 8, 11, 16], [11, 19, 10, 15, 19]], dtype=int32) And voil\u00e0, just like that, magic numbers were removed from our program, and the hierarchical structure of our functions are a bit more explicit: The elementary function, np.sum , operates on a per-row basis. We map the elementary function across all rows of a single dataset, giving us a higher-order function that calculates row-wise summation for a single dataset, row_sum_one_data . We then map the row_sum_one_data across all of the datasets that have been stacked together in a single 3D array. Mapping a function over two arrays simultaneously Let's look at another example. Say we are given two arrays, and we wanted to elementwise multiply them together. For example: a1 = np . array ([ 1 , 2 , 3 , 4 ,]) a2 = np . array ([ 2 , 3 , 4 , 5 ,]) As the NumPy-idiomatic option, we could do: a1 * a2 DeviceArray([ 2, 6, 12, 20], dtype=int32) Another option is that we can define a function called multiply , which multiplies two scalars together and gives us back another scalar, which we then apply over each element in a zip of the two arrays. This is the extremely naive way of handling the problem: result = [] def multiply ( a , b ): return a * b for val1 , val2 in zip ( a1 , a2 ): result . append ( multiply ( val1 , val2 )) np . array ( result ) DeviceArray([ 2, 6, 12, 20], dtype=int32) On the other hand, if we consider this to be the elementary operation of our function, we could instead multiply them pairwise: vmap ( multiply )( a1 , a2 ) DeviceArray([ 2, 6, 12, 20], dtype=int32) As usual, we are able to not care about the leading array axis for each array. Once again, we also broke down the problem into its elementary components, and then leveraged vmap to build out the program to do what we wanted it to do. (This general pattern will show up!) In general, vmap -ing over the leading array axis is the idiomatic thing to do with JAX. It's possibleto vmap over other axes, but those are not the defaults. The implication is that we are nudged towards writing programs that at their core begin with an \"elementary\" function that operate \"elementwise\", where the definition of an \"element\" is not necessarily an array element, but problem-dependent. We then progressively vmap them outwards on array data structures. Example 1: vmap -ing a dot product over square matrices Let's try getting some practice with the following exercises. The first one is to vmap a dot product of a square matrix against itself across a stack of square matrices. An example square matrix called sq_matrix is provided for you to jog your memory on how dot products work if you need to. from jax import random key = random . PRNGKey ( 42 ) data = random . normal ( key , shape = ( 11 , 5 , 5 )) sq_matrix = random . normal ( key , shape = ( 5 , 5 )) vmap ( np . dot )( data , data ) . shape (11, 5, 5) Example 2: Constructing a more complex program We're going to try our hand at constructing a program that first calculates a cumulative product vector for each row in each dataset, sums them up column-wise across each dataset, and applies this same operation across all datasets stacked together. This one is a bit more challenging! To help you along here, the shape of the data are such: There are 11 stacks of data. Each stack of data has 31 rows, and 7 columns. The result of this program still should have 11 stacks and 31 rows, but now each column is not the original data, but the cumulative product of the previous columns. To get this answer write, no magic numbers are allows (e.g. for accessing particular axes). At least two vmap s are necessary here. data = random . normal ( key , shape = ( 11 , 31 , 7 )) def row_wise_cumprod ( row ): return np . cumprod ( row ) def dataset_wise_sum_cumprod ( data ): row_cumprods = vmap ( row_wise_cumprod )( data ) return vmap ( np . sum )( row_cumprods ) vmap ( dataset_wise_sum_cumprod )( data ) . shape (11, 31)","title":"Mapping"},{"location":"02-jax-idioms/01-vmap/#replacing-simple-for-loops-with-vmap","text":"The first JAX thing we will look at is the vmap function. What does vmap do? From the JAX docs on vmap : Vectorizing map. Creates a function which maps fun over argument axes. What does that mean? Well, let's take a look at a few classic examples.","title":"Replacing simple for-loops with vmap"},{"location":"02-jax-idioms/01-vmap/#mapping-an-elementwise-function-over-an-arrays-leading-axis","text":"The first example is mapping a function over an array axis. The simplest example, which is a bit trivial, is doing elementwise application of a function. Say we have uniformly spaced numbers from 0 to 1 in an array: import jax.numpy as np from jax import vmap from time import time arr = np . linspace ( 0 , 1 , 10000 ) arr WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) DeviceArray([0.0000000e+00, 1.0001000e-04, 2.0002000e-04, ..., 9.9979997e-01, 9.9989998e-01, 1.0000000e+00], dtype=float32) If we wanted to apply an exponential transform on every element, the \"dumb\", pure Python way to do so is to write a for-loop: start = time () new_arr = [] for element in arr : new_arr . append ( np . exp ( element )) new_arr = np . array ( new_arr ) end = time () print ( f \" { end - start : .2f } seconds\" ) new_arr 3.55 seconds DeviceArray([1. , 1.0001 , 1.0002 , ..., 2.7177382, 2.71801 , 2.7182817], dtype=float32) Because np.exp is a NumPy ufunc that operates on individual elements, we can call np.exp on arr directly: start = time () new_arr = np . exp ( arr ) end = time () print ( f \" { end - start : .4f } seconds\" ) new_arr 0.0318 seconds DeviceArray([1. , 1.0001 , 1.0002 , ..., 2.7177382, 2.71801 , 2.7182817], dtype=float32) As you can see, this is much faster. This, incidentally, is equivalent to using vmap to map the function across all elements in the array: start = time () new_arr = vmap ( np . exp )( arr ) end = time () print ( f \" { end - start : .4f } seconds\" ) new_arr 0.0010 seconds DeviceArray([1. , 1.0001 , 1.0002 , ..., 2.7177382, 2.71801 , 2.7182817], dtype=float32) It's a bit slower, but one thing we gain from using vmap is the ability to ignore the leading (first) array axis of every element that is passed into the vmap -ed function. To see that, we're going to look at another example.","title":"Mapping an elementwise function over an array's leading axis"},{"location":"02-jax-idioms/01-vmap/#mapping-a-row-wise-function-across-an-arrays-leading-axis","text":"In this example let's say we have a matrix of values that we measured in an experiment. There were n_samples measured, and 3 unique properties that we collected, thereby giving us a matrix of shape (n_samples, 3) . If we needed to find their sum, we could do the following in pure NumPy: def row_sum ( data ): \"\"\"Given one dataset, calculate row-wise sum of data.\"\"\" return np . sum ( data , axis = 1 ) data = np . array ([ [ 1 , 3 , 1 ,], [ 3 , 5 , 1 ,], [ 1 , 2 , 5 ,], [ 7 , 1 , 3 ,], [ 11 , 2 , 3 ,], ]) start = time () result = row_sum ( data ) end = time () print ( f \" { end - start : .4f } seconds\" ) result 0.0125 seconds DeviceArray([ 5, 9, 8, 11, 16], dtype=int32) This would give us the correct answer... but we had to worry about the \"axis\" argument, which is a bit irritating. Instead, we could use first transform np.sum into a vmapped function that is mapped across the leading axis of data : def row_sum_one_data ( data ): \"\"\"Given one dataset, calculate row-wise sum of data.\"\"\" return vmap ( np . sum )( data ) start = time () result = row_sum_one_data ( data ) end = time () print ( f \" { end - start : .4f } seconds\" ) result 0.0012 seconds DeviceArray([ 5, 9, 8, 11, 16], dtype=int32) Thereby giving us the exact same result. While the syntax does take some time to get used to, it does more explicitly and clearly expresses the idea that we don't really care about summing over the leading axis . Now, let's say we had multiple datasets for which we wanted to calculate the row-wise sum. How would we do this in pure NumPy? Well, let's first create this dataset. data2 = np . array ([ [ 1 , 3 , 7 ,], [ 3 , 5 , 11 ,], [ 3 , 2 , 5 ,], [ 7 , 5 , 3 ,], [ 11 , 5 , 3 ,], ]) combined_data = np . moveaxis ( np . dstack ([ data , data2 ], ), 2 , 0 ) combined_data . shape (2, 5, 3) Our shapes tell us that we have 2 stacks of data, each with 5 rows and 3 columns. Since we want row-wise summation, but want to preserve the 2 stacks of data, we have to now worry about which axes to collapse: np . sum ( combined_data , axis = 2 ) DeviceArray([[ 5, 9, 8, 11, 16], [11, 19, 10, 15, 19]], dtype=int32) This is all cool, but we now have a \"magic number\" in our program. We can eliminate this magic number by instead doing vmapping row_sum_over_data across the combined_data array: def row_sum_all_data ( data ): return vmap ( row_sum_one_data )( data ) row_sum_all_data ( combined_data ) DeviceArray([[ 5, 9, 8, 11, 16], [11, 19, 10, 15, 19]], dtype=int32) And voil\u00e0, just like that, magic numbers were removed from our program, and the hierarchical structure of our functions are a bit more explicit: The elementary function, np.sum , operates on a per-row basis. We map the elementary function across all rows of a single dataset, giving us a higher-order function that calculates row-wise summation for a single dataset, row_sum_one_data . We then map the row_sum_one_data across all of the datasets that have been stacked together in a single 3D array.","title":"Mapping a row-wise function across an array's leading axis"},{"location":"02-jax-idioms/01-vmap/#mapping-a-function-over-two-arrays-simultaneously","text":"Let's look at another example. Say we are given two arrays, and we wanted to elementwise multiply them together. For example: a1 = np . array ([ 1 , 2 , 3 , 4 ,]) a2 = np . array ([ 2 , 3 , 4 , 5 ,]) As the NumPy-idiomatic option, we could do: a1 * a2 DeviceArray([ 2, 6, 12, 20], dtype=int32) Another option is that we can define a function called multiply , which multiplies two scalars together and gives us back another scalar, which we then apply over each element in a zip of the two arrays. This is the extremely naive way of handling the problem: result = [] def multiply ( a , b ): return a * b for val1 , val2 in zip ( a1 , a2 ): result . append ( multiply ( val1 , val2 )) np . array ( result ) DeviceArray([ 2, 6, 12, 20], dtype=int32) On the other hand, if we consider this to be the elementary operation of our function, we could instead multiply them pairwise: vmap ( multiply )( a1 , a2 ) DeviceArray([ 2, 6, 12, 20], dtype=int32) As usual, we are able to not care about the leading array axis for each array. Once again, we also broke down the problem into its elementary components, and then leveraged vmap to build out the program to do what we wanted it to do. (This general pattern will show up!) In general, vmap -ing over the leading array axis is the idiomatic thing to do with JAX. It's possibleto vmap over other axes, but those are not the defaults. The implication is that we are nudged towards writing programs that at their core begin with an \"elementary\" function that operate \"elementwise\", where the definition of an \"element\" is not necessarily an array element, but problem-dependent. We then progressively vmap them outwards on array data structures.","title":"Mapping a function over two arrays simultaneously"},{"location":"02-jax-idioms/01-vmap/#example-1-vmap-ing-a-dot-product-over-square-matrices","text":"Let's try getting some practice with the following exercises. The first one is to vmap a dot product of a square matrix against itself across a stack of square matrices. An example square matrix called sq_matrix is provided for you to jog your memory on how dot products work if you need to. from jax import random key = random . PRNGKey ( 42 ) data = random . normal ( key , shape = ( 11 , 5 , 5 )) sq_matrix = random . normal ( key , shape = ( 5 , 5 )) vmap ( np . dot )( data , data ) . shape (11, 5, 5)","title":"Example 1: vmap-ing a dot product over square matrices"},{"location":"02-jax-idioms/01-vmap/#example-2-constructing-a-more-complex-program","text":"We're going to try our hand at constructing a program that first calculates a cumulative product vector for each row in each dataset, sums them up column-wise across each dataset, and applies this same operation across all datasets stacked together. This one is a bit more challenging! To help you along here, the shape of the data are such: There are 11 stacks of data. Each stack of data has 31 rows, and 7 columns. The result of this program still should have 11 stacks and 31 rows, but now each column is not the original data, but the cumulative product of the previous columns. To get this answer write, no magic numbers are allows (e.g. for accessing particular axes). At least two vmap s are necessary here. data = random . normal ( key , shape = ( 11 , 31 , 7 )) def row_wise_cumprod ( row ): return np . cumprod ( row ) def dataset_wise_sum_cumprod ( data ): row_cumprods = vmap ( row_wise_cumprod )( data ) return vmap ( np . sum )( row_cumprods ) vmap ( dataset_wise_sum_cumprod )( data ) . shape (11, 31)","title":"Example 2: Constructing a more complex program"},{"location":"02-jax-idioms/02-partials/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Closures and Partials We're going to take a quick detour and look at this idea of \"partially evaluating a function\". This is going to be important, as it'll allow us to construct functions that are compatible with the requirements of vmap and lax.scan and others in JAX, i.e. they have the correct function signature, but still allow us the flexibility to put in arbitrary things that might be needed for the function to work correctly. There are two ways to do this: you can either use functools.partial , or you can use function closures. Let's see how to do this. Partially evaluating a function using functools.partial For simplicity's sake, let's explore the idea using a function that adds two numbers: def add ( a , b ): return a + b Now, let's say we wanted to fix b to the value 3 , thus generating an add_three function. We can do this two ways. The first is by functools.partial : from functools import partial add_three = partial ( add , b = 3 ) We can now call add_three on any value of a : add_three ( 20 ) 23 If we inspect the function add_three : add_three ? We see that add_three accepts one positional argument, a , and its value of b has been set to a default of 3 . What if we wanted to fix a to 3 instead? add_three_v2 = partial ( add , a = 3 ) add_three_v2 ? Notice how now the function signature has changed, such that b is not set while a has been. This has implications for how we use the function. Calling the function this way will error out: >>> add_three_v2 ( 3 ) --------------------------------------------------------------------------- TypeError Traceback ( most recent call last ) < ipython - input - 109 - e78f540eb25e > in < module > ----> 1 add_three_v2 ( 3 ) TypeError : add () got multiple values for argument 'a' That is because when we pass in the argument with no keyword specified, it is interpreted as the first positional argument, which as you can see, has already been set. On the other hand, calling the function this way will not: add_three_v2 ( b = 3 ) 6 Creating closures Another pattern that we can use is to use closures. Closures are functions that return a closed function that contains information from the closing function. Confused? Let me illustrate: def closing_function ( a ): def closed_function ( b ): return a + b return closed_function Using this pattern, we can rewrite add_three using closures: def make_add_something ( value ): def closed_function ( b ): return b + value return closed_function add_three_v3 = make_add_something ( 3 ) add_three_v3 ( 5 ) 8 add_three_v3 ? Now, you'll notice that the signature of add_three_v3 follows that exactly of the closed function. When writing array programs using JAX, this is the key design pattern you'll want to implement: Always return a function that has the function signature that you need. Naming things is the hardest activity in programming, because we are giving categorical names to things, and sometimes their category of thing isn't always clear. Fret not: the pattern I'll give you is the following: def SOME_FUNCTION_generator ( argument1 , argument2 , keyword_arugment1 = default_value1 ): \"\"\"To simplify things, just give the name of the closing function <some_function>_generator.\"\"\" def inner ( arg1 , arg2 , kwarg1 = default_value1 ): \"\"\"This function should follow the API that is neeed.\"\"\" return something return inner","title":"Partials and closures"},{"location":"02-jax-idioms/02-partials/#closures-and-partials","text":"We're going to take a quick detour and look at this idea of \"partially evaluating a function\". This is going to be important, as it'll allow us to construct functions that are compatible with the requirements of vmap and lax.scan and others in JAX, i.e. they have the correct function signature, but still allow us the flexibility to put in arbitrary things that might be needed for the function to work correctly. There are two ways to do this: you can either use functools.partial , or you can use function closures. Let's see how to do this.","title":"Closures and Partials"},{"location":"02-jax-idioms/02-partials/#partially-evaluating-a-function-using-functoolspartial","text":"For simplicity's sake, let's explore the idea using a function that adds two numbers: def add ( a , b ): return a + b Now, let's say we wanted to fix b to the value 3 , thus generating an add_three function. We can do this two ways. The first is by functools.partial : from functools import partial add_three = partial ( add , b = 3 ) We can now call add_three on any value of a : add_three ( 20 ) 23 If we inspect the function add_three : add_three ? We see that add_three accepts one positional argument, a , and its value of b has been set to a default of 3 . What if we wanted to fix a to 3 instead? add_three_v2 = partial ( add , a = 3 ) add_three_v2 ? Notice how now the function signature has changed, such that b is not set while a has been. This has implications for how we use the function. Calling the function this way will error out: >>> add_three_v2 ( 3 ) --------------------------------------------------------------------------- TypeError Traceback ( most recent call last ) < ipython - input - 109 - e78f540eb25e > in < module > ----> 1 add_three_v2 ( 3 ) TypeError : add () got multiple values for argument 'a' That is because when we pass in the argument with no keyword specified, it is interpreted as the first positional argument, which as you can see, has already been set. On the other hand, calling the function this way will not: add_three_v2 ( b = 3 ) 6","title":"Partially evaluating a function using functools.partial"},{"location":"02-jax-idioms/02-partials/#creating-closures","text":"Another pattern that we can use is to use closures. Closures are functions that return a closed function that contains information from the closing function. Confused? Let me illustrate: def closing_function ( a ): def closed_function ( b ): return a + b return closed_function Using this pattern, we can rewrite add_three using closures: def make_add_something ( value ): def closed_function ( b ): return b + value return closed_function add_three_v3 = make_add_something ( 3 ) add_three_v3 ( 5 ) 8 add_three_v3 ? Now, you'll notice that the signature of add_three_v3 follows that exactly of the closed function. When writing array programs using JAX, this is the key design pattern you'll want to implement: Always return a function that has the function signature that you need. Naming things is the hardest activity in programming, because we are giving categorical names to things, and sometimes their category of thing isn't always clear. Fret not: the pattern I'll give you is the following: def SOME_FUNCTION_generator ( argument1 , argument2 , keyword_arugment1 = default_value1 ): \"\"\"To simplify things, just give the name of the closing function <some_function>_generator.\"\"\" def inner ( arg1 , arg2 , kwarg1 = default_value1 ): \"\"\"This function should follow the API that is neeed.\"\"\" return something return inner","title":"Creating closures"},{"location":"02-jax-idioms/03-lax-scan/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Eliminating for-loops that have carry-over using lax.scan We are now going to see how we can eliminate for-loops that have carry-over using lax.scan . From the JAX docs, lax.scan replaces a for-loop with carry-over: Scan a function over leading array axes while carrying along state. ... python def scan(f, init, xs, length=None): if xs is None: xs = [None] * length carry = init ys = [] for x in xs: carry, y = f(carry, x) ys.append(y) return carry, np.stack(ys) A key requirement of the function f is that it must have only two positional arguments in there, one for carry and one for x . You'll see how we can thus apply functools.partial to construct functions that have this signature from other functions that have other Let's see some concrete examples of this in action. Updating a variable with new info on each loop iteration One classic case where we might use a for-loop is in the cumulative sum or product. Here, we need the current loop information to update the information from the previous loop. Let's see it in action for the cumulative sum: import jax.numpy as np a = np . array ([ 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 ]) result = [] res = 0 for el in a : res += el result . append ( res ) np . array ( result ) WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) DeviceArray([ 1, 3, 6, 11, 18, 29, 42, 59], dtype=int32) This is identical to the cumulative sum: np . cumsum ( a ) DeviceArray([ 1, 3, 6, 11, 18, 29, 42, 59], dtype=int32) Now, let's write it using lax.scan , so we can see the pattern in action: from jax import lax def scanfunc ( res , el ): res = res + el return res , res # (\"carryover\", \"accumulated\") result_init = 0 final , result = lax . scan ( scanfunc , result_init , a ) result DeviceArray([ 1, 3, 6, 11, 18, 29, 42, 59], dtype=int32) As you can see, scanned function has to return two things: One object that gets carried over to the next loop ( carryover ), and Another object that gets \"accumulated\" into an array ( accumulated ). The starting initial value, result_init , is passed into the scanfunc as res on the first call of the scanfunc . On subsequent calls, the first res is passed back into the scanfunc as the new res . Example 1: Simulating compound interest We can use lax.scan to generate data that simulates the generation of wealth by compound interest. Here's an implementation using a plain vanilla for-loop: wealth_record = [] starting_wealth = 100. interest_factor = 1.01 prev_wealth = starting_wealth for t in range ( 100 ): new_wealth = prev_wealth * interest_factor wealth_record . append ( prev_wealth ) prev_wealth = new_wealth np . array ( wealth_record ) DeviceArray([100. , 101. , 102.01 , 103.0301 , 104.0604 , 105.101006, 106.152016, 107.21353 , 108.28567 , 109.36853 , 110.46221 , 111.56683 , 112.6825 , 113.809326, 114.94742 , 116.09689 , 117.257866, 118.43044 , 119.614746, 120.8109 , 122.019005, 123.2392 , 124.47159 , 125.7163 , 126.973465, 128.2432 , 129.52563 , 130.82089 , 132.12909 , 133.4504 , 134.7849 , 136.13274 , 137.49406 , 138.869 , 140.2577 , 141.66028 , 143.07687 , 144.50764 , 145.95273 , 147.41225 , 148.88637 , 150.37524 , 151.87898 , 153.39778 , 154.93176 , 156.48108 , 158.04588 , 159.62634 , 161.22261 , 162.83484 , 164.46318 , 166.10782 , 167.76889 , 169.44658 , 171.14105 , 172.85246 , 174.58098 , 176.3268 , 178.09006 , 179.87096 , 181.66966 , 183.48637 , 185.32123 , 187.17444 , 189.04619 , 190.93665 , 192.84601 , 194.77448 , 196.72221 , 198.68944 , 200.67633 , 202.6831 , 204.70993 , 206.75703 , 208.8246 , 210.91284 , 213.02197 , 215.15219 , 217.30371 , 219.47676 , 221.67152 , 223.88823 , 226.12712 , 228.3884 , 230.67227 , 232.979 , 235.30879 , 237.66188 , 240.0385 , 242.43887 , 244.86327 , 247.3119 , 249.78502 , 252.28287 , 254.8057 , 257.35376 , 259.9273 , 262.52655 , 265.15182 , 267.80334 ], dtype=float32) Now, we'll try implementing it in a lax.scan form: from functools import partial starting_wealth = 100. interest_factor = 1.01 timesteps = np . arange ( 100 ) def make_wealth_at_time_func ( interest_factor ): def wealth_at_time ( prev_wealth , time ): new_wealth = prev_wealth * interest_factor return new_wealth , prev_wealth return wealth_at_time wealth_func = make_wealth_at_time_func ( interest_factor ) final , result = lax . scan ( wealth_func , init = starting_wealth , xs = timesteps ) result DeviceArray([100. , 101. , 102.01 , 103.0301 , 104.060394, 105.101 , 106.15201 , 107.213524, 108.28566 , 109.368515, 110.4622 , 111.56682 , 112.68249 , 113.80931 , 114.9474 , 116.09688 , 117.25784 , 118.43042 , 119.61472 , 120.81087 , 122.018974, 123.23917 , 124.47156 , 125.71627 , 126.973434, 128.24316 , 129.52559 , 130.82085 , 132.12906 , 133.45035 , 134.78485 , 136.1327 , 137.49403 , 138.86897 , 140.25766 , 141.66023 , 143.07683 , 144.5076 , 145.95267 , 147.41219 , 148.8863 , 150.37517 , 151.87892 , 153.3977 , 154.93169 , 156.481 , 158.0458 , 159.62627 , 161.22253 , 162.83476 , 164.4631 , 166.10773 , 167.7688 , 169.44649 , 171.14095 , 172.85236 , 174.58087 , 176.32668 , 178.08994 , 179.87083 , 181.66954 , 183.48624 , 185.32109 , 187.1743 , 189.04604 , 190.9365 , 192.84586 , 194.7743 , 196.72205 , 198.68927 , 200.67616 , 202.68292 , 204.70975 , 206.75684 , 208.8244 , 210.91264 , 213.02177 , 215.152 , 217.30351 , 219.47655 , 221.67131 , 223.88802 , 226.12689 , 228.38815 , 230.67203 , 232.97874 , 235.30853 , 237.66162 , 240.03824 , 242.43861 , 244.86299 , 247.31161 , 249.78473 , 252.28258 , 254.8054 , 257.35345 , 259.92697 , 262.52625 , 265.15152 , 267.80304 ], dtype=float32) The two are equivalent, so we know we have the lax.scan implementation right. import matplotlib.pyplot as plt plt . plot ( wealth_record , label = \"for-loop\" ) plt . plot ( result , label = \"lax.scan\" ) plt . legend (); <matplotlib.legend.Legend at 0x7f20d073ed90> Example 2: Compose vmap and lax.scan together That was one simulation of wealth generation by compound interest for one individual. Now, let's simulate the wealth generation for different starting wealth levels (you may choose any 300 starting points however you'd like). To do so, you'll likely want to start with a function that accepts a scalar starting wealth and generates the simulated time series from there, and then vmap that function across multiple starting points (which is an array itself). from jax import vmap def make_simulation_func ( timesteps ): def inner ( starting_wealth ): final , result = lax . scan ( wealth_func , init = starting_wealth , xs = timesteps ) return final , result return inner simulation_func = make_simulation_func ( timesteps = np . arange ( 200 )) starting_wealth = np . arange ( 300 ) . astype ( float ) final , growth = vmap ( simulation_func )( starting_wealth ) growth /home/travis/miniconda/envs/dl-workshop/lib/python3.7/site-packages/jax/_src/lax/lax.py:6126: UserWarning: Explicitly requested dtype <class 'float'> requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more. warnings.warn(msg.format(dtype, fun_name , truncated_dtype)) DeviceArray([[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00, 0.0000000e+00, 0.0000000e+00], [1.0000000e+00, 1.0100000e+00, 1.0201000e+00, ..., 7.1008401e+00, 7.1718483e+00, 7.2435665e+00], [2.0000000e+00, 2.0200000e+00, 2.0402000e+00, ..., 1.4201680e+01, 1.4343697e+01, 1.4487133e+01], ..., [2.9700000e+02, 2.9997000e+02, 3.0296970e+02, ..., 2.1089490e+03, 2.1300383e+03, 2.1513386e+03], [2.9800000e+02, 3.0098001e+02, 3.0398981e+02, ..., 2.1160518e+03, 2.1372122e+03, 2.1585842e+03], [2.9900000e+02, 3.0198999e+02, 3.0500989e+02, ..., 2.1231504e+03, 2.1443818e+03, 2.1658257e+03]], dtype=float32) plt . plot ( growth [ 1 ]) plt . plot ( growth [ 2 ]) plt . plot ( growth [ 3 ]); [<matplotlib.lines.Line2D at 0x7f20d0618bd0>]","title":"Scanning"},{"location":"02-jax-idioms/03-lax-scan/#eliminating-for-loops-that-have-carry-over-using-laxscan","text":"We are now going to see how we can eliminate for-loops that have carry-over using lax.scan . From the JAX docs, lax.scan replaces a for-loop with carry-over: Scan a function over leading array axes while carrying along state. ... python def scan(f, init, xs, length=None): if xs is None: xs = [None] * length carry = init ys = [] for x in xs: carry, y = f(carry, x) ys.append(y) return carry, np.stack(ys) A key requirement of the function f is that it must have only two positional arguments in there, one for carry and one for x . You'll see how we can thus apply functools.partial to construct functions that have this signature from other functions that have other Let's see some concrete examples of this in action.","title":"Eliminating for-loops that have carry-over using lax.scan"},{"location":"02-jax-idioms/03-lax-scan/#updating-a-variable-with-new-info-on-each-loop-iteration","text":"One classic case where we might use a for-loop is in the cumulative sum or product. Here, we need the current loop information to update the information from the previous loop. Let's see it in action for the cumulative sum: import jax.numpy as np a = np . array ([ 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 ]) result = [] res = 0 for el in a : res += el result . append ( res ) np . array ( result ) WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) DeviceArray([ 1, 3, 6, 11, 18, 29, 42, 59], dtype=int32) This is identical to the cumulative sum: np . cumsum ( a ) DeviceArray([ 1, 3, 6, 11, 18, 29, 42, 59], dtype=int32) Now, let's write it using lax.scan , so we can see the pattern in action: from jax import lax def scanfunc ( res , el ): res = res + el return res , res # (\"carryover\", \"accumulated\") result_init = 0 final , result = lax . scan ( scanfunc , result_init , a ) result DeviceArray([ 1, 3, 6, 11, 18, 29, 42, 59], dtype=int32) As you can see, scanned function has to return two things: One object that gets carried over to the next loop ( carryover ), and Another object that gets \"accumulated\" into an array ( accumulated ). The starting initial value, result_init , is passed into the scanfunc as res on the first call of the scanfunc . On subsequent calls, the first res is passed back into the scanfunc as the new res .","title":"Updating a variable with new info on each loop iteration"},{"location":"02-jax-idioms/03-lax-scan/#example-1-simulating-compound-interest","text":"We can use lax.scan to generate data that simulates the generation of wealth by compound interest. Here's an implementation using a plain vanilla for-loop: wealth_record = [] starting_wealth = 100. interest_factor = 1.01 prev_wealth = starting_wealth for t in range ( 100 ): new_wealth = prev_wealth * interest_factor wealth_record . append ( prev_wealth ) prev_wealth = new_wealth np . array ( wealth_record ) DeviceArray([100. , 101. , 102.01 , 103.0301 , 104.0604 , 105.101006, 106.152016, 107.21353 , 108.28567 , 109.36853 , 110.46221 , 111.56683 , 112.6825 , 113.809326, 114.94742 , 116.09689 , 117.257866, 118.43044 , 119.614746, 120.8109 , 122.019005, 123.2392 , 124.47159 , 125.7163 , 126.973465, 128.2432 , 129.52563 , 130.82089 , 132.12909 , 133.4504 , 134.7849 , 136.13274 , 137.49406 , 138.869 , 140.2577 , 141.66028 , 143.07687 , 144.50764 , 145.95273 , 147.41225 , 148.88637 , 150.37524 , 151.87898 , 153.39778 , 154.93176 , 156.48108 , 158.04588 , 159.62634 , 161.22261 , 162.83484 , 164.46318 , 166.10782 , 167.76889 , 169.44658 , 171.14105 , 172.85246 , 174.58098 , 176.3268 , 178.09006 , 179.87096 , 181.66966 , 183.48637 , 185.32123 , 187.17444 , 189.04619 , 190.93665 , 192.84601 , 194.77448 , 196.72221 , 198.68944 , 200.67633 , 202.6831 , 204.70993 , 206.75703 , 208.8246 , 210.91284 , 213.02197 , 215.15219 , 217.30371 , 219.47676 , 221.67152 , 223.88823 , 226.12712 , 228.3884 , 230.67227 , 232.979 , 235.30879 , 237.66188 , 240.0385 , 242.43887 , 244.86327 , 247.3119 , 249.78502 , 252.28287 , 254.8057 , 257.35376 , 259.9273 , 262.52655 , 265.15182 , 267.80334 ], dtype=float32) Now, we'll try implementing it in a lax.scan form: from functools import partial starting_wealth = 100. interest_factor = 1.01 timesteps = np . arange ( 100 ) def make_wealth_at_time_func ( interest_factor ): def wealth_at_time ( prev_wealth , time ): new_wealth = prev_wealth * interest_factor return new_wealth , prev_wealth return wealth_at_time wealth_func = make_wealth_at_time_func ( interest_factor ) final , result = lax . scan ( wealth_func , init = starting_wealth , xs = timesteps ) result DeviceArray([100. , 101. , 102.01 , 103.0301 , 104.060394, 105.101 , 106.15201 , 107.213524, 108.28566 , 109.368515, 110.4622 , 111.56682 , 112.68249 , 113.80931 , 114.9474 , 116.09688 , 117.25784 , 118.43042 , 119.61472 , 120.81087 , 122.018974, 123.23917 , 124.47156 , 125.71627 , 126.973434, 128.24316 , 129.52559 , 130.82085 , 132.12906 , 133.45035 , 134.78485 , 136.1327 , 137.49403 , 138.86897 , 140.25766 , 141.66023 , 143.07683 , 144.5076 , 145.95267 , 147.41219 , 148.8863 , 150.37517 , 151.87892 , 153.3977 , 154.93169 , 156.481 , 158.0458 , 159.62627 , 161.22253 , 162.83476 , 164.4631 , 166.10773 , 167.7688 , 169.44649 , 171.14095 , 172.85236 , 174.58087 , 176.32668 , 178.08994 , 179.87083 , 181.66954 , 183.48624 , 185.32109 , 187.1743 , 189.04604 , 190.9365 , 192.84586 , 194.7743 , 196.72205 , 198.68927 , 200.67616 , 202.68292 , 204.70975 , 206.75684 , 208.8244 , 210.91264 , 213.02177 , 215.152 , 217.30351 , 219.47655 , 221.67131 , 223.88802 , 226.12689 , 228.38815 , 230.67203 , 232.97874 , 235.30853 , 237.66162 , 240.03824 , 242.43861 , 244.86299 , 247.31161 , 249.78473 , 252.28258 , 254.8054 , 257.35345 , 259.92697 , 262.52625 , 265.15152 , 267.80304 ], dtype=float32) The two are equivalent, so we know we have the lax.scan implementation right. import matplotlib.pyplot as plt plt . plot ( wealth_record , label = \"for-loop\" ) plt . plot ( result , label = \"lax.scan\" ) plt . legend (); <matplotlib.legend.Legend at 0x7f20d073ed90>","title":"Example 1: Simulating compound interest"},{"location":"02-jax-idioms/03-lax-scan/#example-2-compose-vmap-and-laxscan-together","text":"That was one simulation of wealth generation by compound interest for one individual. Now, let's simulate the wealth generation for different starting wealth levels (you may choose any 300 starting points however you'd like). To do so, you'll likely want to start with a function that accepts a scalar starting wealth and generates the simulated time series from there, and then vmap that function across multiple starting points (which is an array itself). from jax import vmap def make_simulation_func ( timesteps ): def inner ( starting_wealth ): final , result = lax . scan ( wealth_func , init = starting_wealth , xs = timesteps ) return final , result return inner simulation_func = make_simulation_func ( timesteps = np . arange ( 200 )) starting_wealth = np . arange ( 300 ) . astype ( float ) final , growth = vmap ( simulation_func )( starting_wealth ) growth /home/travis/miniconda/envs/dl-workshop/lib/python3.7/site-packages/jax/_src/lax/lax.py:6126: UserWarning: Explicitly requested dtype <class 'float'> requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more. warnings.warn(msg.format(dtype, fun_name , truncated_dtype)) DeviceArray([[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00, 0.0000000e+00, 0.0000000e+00], [1.0000000e+00, 1.0100000e+00, 1.0201000e+00, ..., 7.1008401e+00, 7.1718483e+00, 7.2435665e+00], [2.0000000e+00, 2.0200000e+00, 2.0402000e+00, ..., 1.4201680e+01, 1.4343697e+01, 1.4487133e+01], ..., [2.9700000e+02, 2.9997000e+02, 3.0296970e+02, ..., 2.1089490e+03, 2.1300383e+03, 2.1513386e+03], [2.9800000e+02, 3.0098001e+02, 3.0398981e+02, ..., 2.1160518e+03, 2.1372122e+03, 2.1585842e+03], [2.9900000e+02, 3.0198999e+02, 3.0500989e+02, ..., 2.1231504e+03, 2.1443818e+03, 2.1658257e+03]], dtype=float32) plt . plot ( growth [ 1 ]) plt . plot ( growth [ 2 ]) plt . plot ( growth [ 3 ]); [<matplotlib.lines.Line2D at 0x7f20d0618bd0>]","title":"Example 2: Compose vmap and lax.scan together"},{"location":"02-jax-idioms/04-random-numbers/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Fully Controllable Random Number Generation In this section, we'll explore how to create programs that use random number generation in a fashion that is fully deterministic conditioned on a single starting random number generation key. But first, let's explore what happens when we use NumPy's vanilla random number generation protocol to generate numbers. import numpy as onp # original numpy Let's draw a random number from a Gaussian in NumPy. onp . random . seed ( 42 ) a = onp . random . normal () a 0.4967141530112327 And for good measure, let's draw another one. b = onp . random . normal () b -0.13826430117118466 This is intuitive behaviour, because we expect that each time we call on a random number generator, we should get back a different number from before. However, this behaviour is problematic when we are trying to debug programs, which essentially are deterministic. This is because stochastically , we might hit a setting where we encounter an error in our program, and we are unable to reproduce it because we are relying on a random number generator that relies on global state, and hence that doesn't behave in a fully controllable fashion. How then can we get \"the best of both worlds\": random number generation that is controllable? The way that JAX's developers went about doing this is to use pseudo-random number generators that require explicit passing in of a pseudo-random number generation key, rather than relying on a global state being set. Each unique key will deterministically give a unique drawn value explicitly. Let's see that in action: from jax import random key = random . PRNGKey ( 42 ) a = random . normal ( key = key ) a WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) DeviceArray(-0.18471184, dtype=float32) To show you that passing in the same key gives us the same values as before: b = random . normal ( key = key ) b DeviceArray(-0.18471184, dtype=float32) That should already be a stark difference from what you're used to with vanilla NumPy, and this is one key crucial difference between JAX's random module and NumPy's random module. Everything else is very similar, but this is a key difference, and for good reason -- this should hint to you the idea that we can have explicity reproducibility, rather than merely implicit, over our stochastic programs within the same session. How do we get a new draw? Well, we can either create a new key manually, or we can programmatically split the key into two, and use one of the newly split keys to generate a new random number. Let's see that in action: k1 , k2 = random . split ( key ) c = random . normal ( key = k2 ) c DeviceArray(1.3694694, dtype=float32) k3 , k4 , k5 = random . split ( k2 , num = 3 ) d = random . normal ( key = k3 ) d DeviceArray(0.04692494, dtype=float32) By splitting the key into two, three, or even 1000 parts, we can get new keys that are derived from a parent key that generate different random numbers from the same random number generating function. Let's explore how we can use this in the generation of a Gaussian random walk. Example: Simulating a Gaussian random walk A Gaussian random walk is one where we start at a point that is drawn from a Gaussian, and then we draw another point from a Gausian using the first point as the starting Gaussian point. Does that loop structure sound familiar? Well... yeah, it sounds like a classic lax.scan setup! Here's how we might set it up. Firstly, JAX's random.normal function doesn't allow us to specify the location and scale, and only gives us a draw from a unit Gaussian. We can work around this, because any unit Gaussian draw can be shifted and scaled to a N(\\mu, \\sigma) N(\\mu, \\sigma) by multiplying the draw by \\sigma \\sigma and adding \\mu \\mu . To get a length 1000 random draw, we can split the key 1000 ways, and use lax.scan to scan a new Gaussian generator across the keys, thereby giving us 1000 unique draws. We then add the old value of the Gaussian to the new draw. We return the tuple ( new_gaussian, old_gaussian ), as we want to have the new gaussian passed into the next iteration, and accumulate the history of the old gaussians. from dl_workshop.jax_idioms import generate_new_gaussian generate_new_gaussian ?? from jax import lax keys = random . split ( key , num = 1000 ) final , result = lax . scan ( generate_new_gaussian , 0. , keys ) result DeviceArray([ 0.0000000e+00, 2.9308948e-01, 9.0737927e-01, -1.8574587e+00, -9.3475866e-01, 5.3657484e-01, -1.6153860e+00, -2.0659945e+00, -1.8341075e+00, -1.4049507e-02, -1.8038048e+00, -1.4442403e+00, -1.5271288e+00, -2.0419145e+00, 5.9077743e-02, -1.3251271e+00, -2.7179704e+00, -2.5978544e+00, -4.0130644e+00, -3.7746308e+00, -4.5405092e+00, -5.8234315e+00, -6.2098603e+00, -5.1640716e+00, -5.3078022e+00, -6.4008999e+00, -6.1982379e+00, -6.6912217e+00, -6.9285545e+00, -6.7963529e+00, -7.4145002e+00, -7.5791554e+00, -6.9574161e+00, -6.8424754e+00, -7.0481572e+00, -8.4638453e+00, -8.5200520e+00, -1.0150510e+01, -1.0404550e+01, -1.0354767e+01, -1.1202571e+01, -9.8503466e+00, -1.0906091e+01, -9.7488117e+00, -9.0915432e+00, -7.0813093e+00, -8.7363491e+00, -6.5654454e+00, -5.8719378e+00, -7.7279820e+00, -7.0762358e+00, -6.9824843e+00, -6.2720618e+00, -6.5822825e+00, -5.1869078e+00, -6.6846662e+00, -6.5237956e+00, -5.5986729e+00, -5.2407198e+00, -5.4419765e+00, -6.3954625e+00, -6.6313639e+00, -7.5193448e+00, -1.0097613e+01, -1.0111365e+01, -9.4919729e+00, -1.1689382e+01, -1.2609113e+01, -1.3132720e+01, -1.2729928e+01, -1.3554560e+01, -1.4612862e+01, -1.3765321e+01, -1.2790593e+01, -1.2499749e+01, -1.0198821e+01, -9.9114904e+00, -8.6181431e+00, -1.0463602e+01, -1.0668167e+01, -1.1387536e+01, -1.0559889e+01, -1.1779433e+01, -8.8845911e+00, -9.3539925e+00, -1.1195022e+01, -8.7970819e+00, -6.6903763e+00, -6.7867746e+00, -7.0319462e+00, -7.5244737e+00, -6.9895887e+00, -7.4406929e+00, -5.9965134e+00, -5.0314608e+00, -4.4997315e+00, -3.6206439e+00, -3.0368054e+00, -4.8258009e+00, -5.9843774e+00, -6.1343188e+00, -6.9391813e+00, -6.3957014e+00, -7.9022403e+00, -6.8429284e+00, -6.3052950e+00, -5.7587605e+00, -5.0461640e+00, -4.3869100e+00, -3.0319991e+00, -3.5835326e+00, -3.4561212e+00, -2.7506545e+00, -3.9921327e+00, -3.1952002e+00, -2.0107675e+00, -2.0431397e+00, -1.2036926e+00, -2.5120826e+00, 1.5073891e-01, 5.9526514e-02, -6.1567672e-02, -1.7971006e-01, -6.4089805e-02, -1.7376822e-01, -3.3485657e-01, -1.5660967e+00, -3.2925894e+00, -3.2538462e+00, -4.1834068e+00, -6.1261258e+00, -5.7395034e+00, -6.7661810e+00, -7.1050177e+00, -5.4329929e+00, -5.6401262e+00, -5.1202645e+00, -5.7162919e+00, -5.2094598e+00, -6.0466599e+00, -4.9979591e+00, -4.4890556e+00, -3.7108121e+00, -4.1257849e+00, -4.3740158e+00, -3.0525262e+00, -3.6016676e+00, -2.9378812e+00, -2.7627447e+00, -3.2016382e+00, -5.1802568e+00, -4.7830458e+00, -6.3186502e+00, -5.6700902e+00, -5.7391586e+00, -6.4026637e+00, -8.0447359e+00, -8.9100914e+00, -7.9058552e+00, -8.8494577e+00, -9.4514074e+00, -9.7416124e+00, -1.1343502e+01, -1.0725481e+01, -1.1037007e+01, -1.2457513e+01, -1.1892594e+01, -1.1273226e+01, -1.1881676e+01, -1.2089799e+01, -1.2382518e+01, -1.1811073e+01, -1.2355193e+01, -1.2284500e+01, -1.2150561e+01, -1.3035995e+01, -1.1861339e+01, -9.7929296e+00, -8.5769711e+00, -8.8338213e+00, -8.9625845e+00, -9.3008270e+00, -8.2740698e+00, -7.2355332e+00, -7.6913581e+00, -8.1070204e+00, -7.8158293e+00, -6.7811961e+00, -7.8087449e+00, -7.1342974e+00, -6.4980474e+00, -6.0770731e+00, -7.3229432e+00, -8.4346581e+00, -7.7853637e+00, -9.2040215e+00, -9.1392860e+00, -8.4487629e+00, -7.6234417e+00, -7.6274519e+00, -8.6342812e+00, -8.8896389e+00, -9.0145798e+00, -1.0174156e+01, -1.0948184e+01, -1.1148661e+01, -1.1817743e+01, -1.2380128e+01, -1.0779567e+01, -1.0633921e+01, -1.0624221e+01, -1.1308586e+01, -1.0950401e+01, -1.1113399e+01, -1.0885623e+01, -1.0229873e+01, -9.5668888e+00, -8.4545860e+00, -6.5746989e+00, -8.2771807e+00, -8.9303951e+00, -8.9252548e+00, -9.0623932e+00, -1.1962385e+01, -9.9415340e+00, -1.0192524e+01, -8.0894470e+00, -7.0265064e+00, -6.7623253e+00, -6.9903803e+00, -7.1455112e+00, -7.8920078e+00, -7.7239509e+00, -7.3774877e+00, -6.4395394e+00, -7.0436544e+00, -8.7256002e+00, -7.1755939e+00, -5.7905116e+00, -4.3698101e+00, -5.1510201e+00, -4.8481994e+00, -6.6298528e+00, -6.7535200e+00, -6.8409166e+00, -6.5135756e+00, -5.4256835e+00, -4.0130773e+00, -3.5815592e+00, -2.3260942e+00, -2.6673090e+00, -3.8151481e+00, -4.9169331e+00, -5.2914739e+00, -6.0849295e+00, -6.6498380e+00, -6.8321829e+00, -6.6892276e+00, -4.2620564e+00, -3.6063349e+00, -3.6935492e+00, -3.2239606e+00, -1.9226974e+00, -2.8547025e+00, -3.1886687e+00, -3.4799323e+00, -2.7632964e+00, -3.8803239e+00, -5.2894616e+00, -3.7164781e+00, -2.1130731e+00, -2.7807741e+00, -3.8356600e+00, -4.4654922e+00, -6.1223135e+00, -6.2291455e+00, -5.2452497e+00, -3.8544254e+00, -4.0835938e+00, -4.7132416e+00, -4.7189741e+00, -3.3033028e+00, -2.0330689e+00, -1.9273376e+00, -2.0315263e+00, -2.7490623e+00, -2.5552733e+00, -2.6076877e+00, -2.4953003e+00, -1.6205913e+00, -5.6036735e-01, 1.3941172e+00, 1.9810356e+00, 3.4585705e+00, 4.6223598e+00, 5.6333132e+00, 5.6642418e+00, 5.4901705e+00, 6.0357161e+00, 5.4238672e+00, 4.9301081e+00, 5.6282086e+00, 6.5141969e+00, 6.3569307e+00, 7.2397623e+00, 8.4335928e+00, 1.0527872e+01, 1.0277812e+01, 1.1162391e+01, 9.9877901e+00, 1.1897879e+01, 1.1802931e+01, 1.3891153e+01, 1.2607279e+01, 1.3688196e+01, 1.4280282e+01, 1.4194159e+01, 1.5063095e+01, 1.5630124e+01, 1.6656620e+01, 1.8520769e+01, 1.7245922e+01, 1.7332148e+01, 1.8136528e+01, 1.7593718e+01, 1.7247683e+01, 1.6601835e+01, 1.8719860e+01, 1.8267977e+01, 1.9116158e+01, 2.0761889e+01, 2.0569153e+01, 2.3161409e+01, 2.2082083e+01, 2.2013521e+01, 2.2380838e+01, 2.1141018e+01, 2.0381895e+01, 2.0923218e+01, 2.1501842e+01, 2.1908739e+01, 2.2140768e+01, 2.1900915e+01, 2.0821558e+01, 1.9753613e+01, 2.0594179e+01, 2.1206045e+01, 2.3502974e+01, 2.3857637e+01, 2.5092167e+01, 2.5346411e+01, 2.5012743e+01, 2.5705475e+01, 2.4005598e+01, 2.3668116e+01, 2.3274820e+01, 2.3368999e+01, 2.2466444e+01, 2.3110544e+01, 2.3757801e+01, 2.3500526e+01, 2.2233465e+01, 2.1470026e+01, 2.1215025e+01, 2.0534557e+01, 2.0794485e+01, 2.1000740e+01, 1.9642010e+01, 1.9264942e+01, 1.9692028e+01, 2.0378180e+01, 2.1518808e+01, 2.0830177e+01, 2.0563187e+01, 2.0399698e+01, 2.1520786e+01, 2.2611856e+01, 2.3350994e+01, 2.3495277e+01, 2.3201815e+01, 2.2468166e+01, 2.2388708e+01, 2.3538897e+01, 2.2670794e+01, 2.3710402e+01, 2.4161610e+01, 2.2987537e+01, 2.2397484e+01, 2.2795181e+01, 2.2931574e+01, 2.4216108e+01, 2.4123053e+01, 2.5571371e+01, 2.6668819e+01, 2.7405973e+01, 2.7994703e+01, 2.8459429e+01, 2.7073235e+01, 2.7061722e+01, 2.8760845e+01, 2.8883043e+01, 2.8625389e+01, 2.9911112e+01, 3.0024462e+01, 2.9932728e+01, 3.0289661e+01, 3.1156015e+01, 3.1827835e+01, 3.1586716e+01, 3.3310631e+01, 3.4046471e+01, 3.4243004e+01, 3.4956837e+01, 3.5139080e+01, 3.6184502e+01, 3.5955849e+01, 3.7509201e+01, 3.7216148e+01, 3.6132656e+01, 3.6760956e+01, 3.5242069e+01, 3.5308361e+01, 3.6642490e+01, 3.6514305e+01, 3.6897408e+01, 3.5683262e+01, 3.4114861e+01, 3.3112411e+01, 3.3603756e+01, 3.2959377e+01, 3.3436260e+01, 3.4570690e+01, 3.5626965e+01, 3.4386700e+01, 3.7291370e+01, 3.7587612e+01, 3.8173309e+01, 3.7969139e+01, 3.7319889e+01, 3.7574493e+01, 3.7078999e+01, 3.7892895e+01, 3.8195210e+01, 3.9049164e+01, 4.0805832e+01, 4.1425289e+01, 4.1006306e+01, 4.2599464e+01, 4.2570702e+01, 4.1265091e+01, 3.9659935e+01, 3.6838615e+01, 3.7152439e+01, 3.4707485e+01, 3.4948238e+01, 3.5298897e+01, 3.5688854e+01, 3.5597324e+01, 3.6016472e+01, 3.4955692e+01, 3.5937595e+01, 3.6321556e+01, 3.6158390e+01, 3.5102604e+01, 3.5943344e+01, 3.5029987e+01, 3.5126171e+01, 3.5180359e+01, 3.3256603e+01, 3.1358570e+01, 3.1270344e+01, 3.2284927e+01, 3.1823986e+01, 3.1956850e+01, 3.1575148e+01, 3.1685295e+01, 2.9895905e+01, 2.9540310e+01, 3.0184551e+01, 3.0167349e+01, 3.2061939e+01, 3.0725494e+01, 3.0744898e+01, 3.0956152e+01, 3.0580080e+01, 3.1252678e+01, 3.0915356e+01, 3.0126585e+01, 3.0106441e+01, 2.9090900e+01, 3.0437056e+01, 3.0409433e+01, 2.9945259e+01, 3.0450762e+01, 3.2142242e+01, 3.1455513e+01, 3.0856722e+01, 3.0757288e+01, 3.1934263e+01, 3.0583834e+01, 3.1049152e+01, 3.1009058e+01, 3.0781527e+01, 3.0927502e+01, 3.1616207e+01, 3.2949509e+01, 3.3827129e+01, 3.4099651e+01, 3.3860271e+01, 3.2565521e+01, 3.2352459e+01, 3.1877657e+01, 3.1700710e+01, 3.1149137e+01, 3.3191036e+01, 3.3546593e+01, 3.4550777e+01, 3.6293682e+01, 3.6673988e+01, 3.6357235e+01, 3.5923759e+01, 3.6504375e+01, 3.7487831e+01, 3.7691296e+01, 3.8078178e+01, 3.8593243e+01, 3.9208504e+01, 3.9726727e+01, 3.8830025e+01, 3.8074921e+01, 3.8185421e+01, 3.9926697e+01, 3.8049431e+01, 3.7234890e+01, 3.6851467e+01, 3.4246452e+01, 3.3483982e+01, 3.3542747e+01, 3.3847820e+01, 3.5174644e+01, 3.5427490e+01, 3.4421764e+01, 3.4347435e+01, 3.4626266e+01, 3.4442036e+01, 3.4479492e+01, 3.4569195e+01, 3.5695679e+01, 3.5770096e+01, 3.3925434e+01, 3.3344734e+01, 3.4328148e+01, 3.3246941e+01, 3.6386784e+01, 3.5068905e+01, 3.6650124e+01, 3.8195084e+01, 3.8098965e+01, 3.8483913e+01, 3.8413033e+01, 3.8648891e+01, 3.8181110e+01, 3.9143925e+01, 3.7722179e+01, 3.6982872e+01, 3.6395641e+01, 3.5374462e+01, 3.5677132e+01, 3.6783710e+01, 3.7833107e+01, 3.6720718e+01, 3.8577248e+01, 3.8477531e+01, 3.8318790e+01, 3.8823742e+01, 3.8284389e+01, 3.9372498e+01, 3.9304340e+01, 3.9745270e+01, 4.0708485e+01, 4.2130310e+01, 4.2701813e+01, 4.2850170e+01, 4.4041939e+01, 4.4493820e+01, 4.4333946e+01, 4.4279903e+01, 4.4928108e+01, 4.6226978e+01, 4.7380543e+01, 4.4302620e+01, 4.4585323e+01, 4.4197090e+01, 4.4005562e+01, 4.5030525e+01, 4.6177292e+01, 4.5057133e+01, 4.4796387e+01, 4.5758343e+01, 4.6264778e+01, 4.5833244e+01, 4.5269356e+01, 4.4239578e+01, 4.4179420e+01, 4.3310829e+01, 4.2324654e+01, 4.2570541e+01, 4.2294926e+01, 4.2662281e+01, 4.1898159e+01, 4.3488464e+01, 4.2308685e+01, 4.0471664e+01, 4.1282433e+01, 4.1386139e+01, 4.1974152e+01, 4.2820770e+01, 4.3028973e+01, 4.2441658e+01, 4.1195194e+01, 4.2222683e+01, 4.2629620e+01, 4.2836639e+01, 4.0610840e+01, 4.0618824e+01, 4.0622406e+01, 4.2302402e+01, 4.1653931e+01, 4.1998516e+01, 4.2532280e+01, 4.3407074e+01, 4.4703434e+01, 4.5959232e+01, 4.4900124e+01, 4.3607147e+01, 4.4185852e+01, 4.3930374e+01, 4.2671288e+01, 4.2647823e+01, 4.3074402e+01, 4.0779964e+01, 3.9422771e+01, 3.9817394e+01, 4.0903217e+01, 4.0945713e+01, 4.0179401e+01, 3.9954971e+01, 3.8774868e+01, 3.9389870e+01, 3.8990650e+01, 4.0301315e+01, 4.0336689e+01, 3.9112911e+01, 3.8991299e+01, 3.9274616e+01, 3.8024837e+01, 3.8186394e+01, 3.6334290e+01, 3.5846634e+01, 3.4716499e+01, 3.5293205e+01, 3.6128590e+01, 3.5766850e+01, 3.5954765e+01, 3.6176323e+01, 3.5974739e+01, 3.5190342e+01, 3.3506847e+01, 3.2950245e+01, 3.5488556e+01, 3.5793999e+01, 3.5961895e+01, 3.5924114e+01, 3.6066338e+01, 3.5062145e+01, 3.4197170e+01, 3.5094486e+01, 3.4696815e+01, 3.5347645e+01, 3.5017605e+01, 3.5452106e+01, 3.4469566e+01, 3.3246662e+01, 3.2555874e+01, 3.1336807e+01, 3.0030834e+01, 2.9280752e+01, 3.0166458e+01, 3.0552605e+01, 3.2284317e+01, 3.1299629e+01, 3.1654791e+01, 3.2651154e+01, 3.1412001e+01, 3.1104824e+01, 2.9514833e+01, 2.8710270e+01, 2.7595047e+01, 2.8285002e+01, 2.7775763e+01, 2.7963326e+01, 2.6885824e+01, 2.5964947e+01, 2.8124084e+01, 3.0352627e+01, 3.1669094e+01, 2.9583199e+01, 2.9716843e+01, 2.9729292e+01, 3.1786596e+01, 3.1789055e+01, 3.2763531e+01, 3.3053158e+01, 3.1871616e+01, 3.2073021e+01, 3.0871887e+01, 2.9844360e+01, 3.0924242e+01, 3.1848522e+01, 3.3157806e+01, 3.0890354e+01, 3.1631508e+01, 3.0394520e+01, 3.0855812e+01, 2.8997591e+01, 3.0913607e+01, 3.1538607e+01, 3.2221794e+01, 3.1800053e+01, 3.2862667e+01, 3.3099739e+01, 3.3546024e+01, 3.2815430e+01, 3.2776962e+01, 3.2717190e+01, 3.4095726e+01, 3.4460094e+01, 3.4477234e+01, 3.2751495e+01, 3.1662571e+01, 3.2135162e+01, 3.1407804e+01, 3.0974413e+01, 3.1359869e+01, 2.9897268e+01, 2.8611977e+01, 2.7841516e+01, 2.8652159e+01, 2.7959852e+01, 2.7184647e+01, 2.5211527e+01, 2.3363140e+01, 2.4084505e+01, 2.2724066e+01, 2.2931398e+01, 2.2261568e+01, 2.1220295e+01, 2.2457872e+01, 2.3890150e+01, 2.3751272e+01, 2.3846703e+01, 2.2932783e+01, 2.3326431e+01, 2.3571524e+01, 2.4509272e+01, 2.2720484e+01, 2.1190624e+01, 2.2890141e+01, 2.2580996e+01, 2.1119761e+01, 2.1426302e+01, 2.1359793e+01, 2.0951298e+01, 2.0533409e+01, 1.9864664e+01, 1.9137741e+01, 2.0104788e+01, 2.0991695e+01, 2.0703228e+01, 2.0457403e+01, 2.0432844e+01, 1.9983459e+01, 2.0347317e+01, 2.1548651e+01, 2.1746424e+01, 2.3398228e+01, 2.4359446e+01, 2.3417252e+01, 2.4232868e+01, 2.5758299e+01, 2.6161476e+01, 2.5474260e+01, 2.6110472e+01, 2.5719536e+01, 2.6882660e+01, 2.6393394e+01, 2.5338243e+01, 2.5338694e+01, 2.4715746e+01, 2.5425732e+01, 2.5330101e+01, 2.4126286e+01, 2.4346094e+01, 2.5075274e+01, 2.6083353e+01, 2.5672998e+01, 2.6789873e+01, 2.7441189e+01, 2.7152185e+01, 2.8773258e+01, 2.8286983e+01, 2.7623968e+01, 2.7085136e+01, 2.7892982e+01, 2.8239511e+01, 3.0850424e+01, 3.0474251e+01, 2.9058525e+01, 2.9551945e+01, 3.1022072e+01, 3.2846928e+01, 3.2378555e+01, 3.1974035e+01, 3.1732973e+01, 3.2823322e+01, 3.1534229e+01, 3.2405807e+01, 3.1176874e+01, 3.1158363e+01, 2.9453104e+01, 2.9252224e+01, 2.9300753e+01, 2.8109919e+01, 2.7284943e+01, 2.6386570e+01, 2.6772263e+01, 2.6448494e+01, 2.7085169e+01, 2.7579880e+01, 2.8534266e+01, 2.6776207e+01, 2.6352282e+01, 2.7356855e+01, 2.5677477e+01, 2.7271610e+01, 2.5735304e+01, 2.4935097e+01, 2.4828808e+01, 2.3865839e+01, 2.5935564e+01, 2.5382603e+01, 2.6312302e+01, 2.6516724e+01, 2.7956665e+01, 2.8309921e+01, 2.8551727e+01, 2.8265148e+01, 2.8631514e+01, 2.8650980e+01, 2.9362616e+01, 2.9412687e+01, 3.0437040e+01, 2.9622648e+01, 2.9777632e+01, 2.8744820e+01, 2.7734375e+01, 2.6796537e+01, 2.6956509e+01, 2.8200478e+01, 2.8237026e+01, 3.0127491e+01, 3.1191160e+01, 2.9725546e+01, 2.8540089e+01, 2.8666386e+01, 2.7104782e+01, 2.9712494e+01, 3.0739010e+01, 3.2321617e+01, 3.2645321e+01, 3.1430027e+01, 3.1431028e+01, 3.3137547e+01, 3.2631096e+01, 3.2854687e+01, 3.2398182e+01, 3.3396172e+01, 3.2681847e+01, 3.4913078e+01, 3.6094872e+01, 3.6287483e+01, 3.6486488e+01, 3.6492893e+01, 3.7116772e+01, 3.8242214e+01, 4.0165955e+01, 3.7979900e+01, 3.9059998e+01, 3.9028103e+01, 4.0075943e+01, 3.8624214e+01, 3.8290436e+01, 3.7825928e+01, 3.6552616e+01, 3.7390743e+01, 3.9012035e+01, 3.9298553e+01, 3.9827915e+01, 4.1841003e+01, 4.0262543e+01, 4.0187325e+01, 4.0574299e+01, 4.1681656e+01, 4.2406036e+01, 4.4020794e+01, 4.5037449e+01, 4.3865047e+01, 4.2779461e+01, 4.1440739e+01, 4.1341339e+01, 3.9818481e+01, 4.0123234e+01, 3.9883667e+01, 4.0748554e+01, 4.2278969e+01, 4.2600727e+01, 4.2560783e+01, 4.3364643e+01, 4.3650795e+01, 4.2917637e+01, 4.2937355e+01, 4.2296829e+01, 4.2014278e+01, 4.2042435e+01, 4.3259327e+01, 4.3390633e+01, 4.4372753e+01, 4.4892471e+01, 4.5385017e+01, 4.7565838e+01, 4.7109280e+01, 4.6290051e+01, 4.5906693e+01, 4.4330467e+01, 4.4608757e+01, 4.3309235e+01, 4.2039761e+01, 4.3436153e+01, 4.3031368e+01, 4.3179710e+01, 4.1960804e+01, 4.2161831e+01, 4.2877914e+01, 4.2087650e+01, 4.2310814e+01, 4.3533005e+01, 4.4523136e+01, 4.4735619e+01, 4.4101280e+01, 4.4951130e+01, 4.5912594e+01, 4.7106377e+01, 4.6318672e+01, 4.5454987e+01, 4.4767693e+01, 4.4964012e+01, 4.4671551e+01, 4.5825607e+01, 4.6738506e+01, 4.4972668e+01, 4.4546360e+01, 4.3239277e+01, 4.4169300e+01, 4.4169174e+01, 4.3997723e+01, 4.3268627e+01, 4.1956364e+01, 4.1147232e+01, 4.1304501e+01, 4.1476833e+01, 4.1862213e+01, 4.2731663e+01, 4.3843063e+01, 4.5468189e+01, 4.6342617e+01, 4.7410614e+01, 4.7741390e+01, 4.7063366e+01, 4.7882603e+01, 4.8219383e+01, 4.9347443e+01, 4.6957180e+01, 4.7834709e+01, 4.7918716e+01, 4.7106808e+01, 4.6122921e+01, 4.5530235e+01, 4.5815086e+01, 4.5350376e+01, 4.5075825e+01], dtype=float32) import matplotlib.pyplot as plt plt . plot ( result ) [<matplotlib.lines.Line2D at 0x7fcb18564510>] Looks like we did it! Definitely looks like a proper Gaussian random walk to me. Let's encapsulate this inside a funciton generator, because the next thing we're going to do is to generate multiple realizations of the Gaussian random walk. from dl_workshop.jax_idioms import make_gaussian_random_walk_func make_gaussian_random_walk_func ?? Now, what if we wanted to generate multiple realizations of the Gaussian random walk? Does this sound familiar? If so... yeah, it's a vanilla for-loop, which directly brings us to vmap ! from jax import vmap num_realizations = 200 keys = random . split ( key , num_realizations ) grw_1000_steps = make_gaussian_random_walk_func ( 1000 ) final , trajectories = vmap ( grw_1000_steps )( keys ) trajectories . shape (200, 1000) We did it! We have 200 trajectories of a 1000-step Gaussian random walk. Notice also how the program is structured very nicely: Each layer of abstraction in the program corresponds to a new axis dimension along which we are working. The onion layering of the program has very natural structure for the problem at hand. Enough prosyletizing from me, let's visualize the Gaussian random walk to make sure it genuinely is a GRW. import seaborn as sns fig , ax = plt . subplots () for trajectory in trajectories [ 0 : 20 ]: ax . plot ( trajectory ) sns . despine () Now, note how if you were to re-run the entire program from top-to-bottom again, you would get exactly the same plot . This is what we mean by \"reproducible\". Traditional array programs are not fully reproducible, they are only \"kind of\" reproducible in the limit of many runs of the same program. With JAX's random number generation paradigm, any random number generation program is 100% reproducible, down to the level of the exact sequence of random number draws, as long as the seed(s) controlling the program are 100% identical. When an error shows up in a program, as long as its stochastic components are controlled by hand-set seeds, that error is 100% reproducible. For those who have tried working with stochastic programs before, this is an extremely desirable property, as it means we gain the ability to reliably debug our program -- absolutely crucial especially when it comes to working with probabilistic models. Also notice how we finally wrote our first productive for-loop -- but it was only to plot something, not for some form of calculations :).","title":"Random Numbers"},{"location":"02-jax-idioms/04-random-numbers/#fully-controllable-random-number-generation","text":"In this section, we'll explore how to create programs that use random number generation in a fashion that is fully deterministic conditioned on a single starting random number generation key. But first, let's explore what happens when we use NumPy's vanilla random number generation protocol to generate numbers. import numpy as onp # original numpy Let's draw a random number from a Gaussian in NumPy. onp . random . seed ( 42 ) a = onp . random . normal () a 0.4967141530112327 And for good measure, let's draw another one. b = onp . random . normal () b -0.13826430117118466 This is intuitive behaviour, because we expect that each time we call on a random number generator, we should get back a different number from before. However, this behaviour is problematic when we are trying to debug programs, which essentially are deterministic. This is because stochastically , we might hit a setting where we encounter an error in our program, and we are unable to reproduce it because we are relying on a random number generator that relies on global state, and hence that doesn't behave in a fully controllable fashion. How then can we get \"the best of both worlds\": random number generation that is controllable? The way that JAX's developers went about doing this is to use pseudo-random number generators that require explicit passing in of a pseudo-random number generation key, rather than relying on a global state being set. Each unique key will deterministically give a unique drawn value explicitly. Let's see that in action: from jax import random key = random . PRNGKey ( 42 ) a = random . normal ( key = key ) a WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) DeviceArray(-0.18471184, dtype=float32) To show you that passing in the same key gives us the same values as before: b = random . normal ( key = key ) b DeviceArray(-0.18471184, dtype=float32) That should already be a stark difference from what you're used to with vanilla NumPy, and this is one key crucial difference between JAX's random module and NumPy's random module. Everything else is very similar, but this is a key difference, and for good reason -- this should hint to you the idea that we can have explicity reproducibility, rather than merely implicit, over our stochastic programs within the same session. How do we get a new draw? Well, we can either create a new key manually, or we can programmatically split the key into two, and use one of the newly split keys to generate a new random number. Let's see that in action: k1 , k2 = random . split ( key ) c = random . normal ( key = k2 ) c DeviceArray(1.3694694, dtype=float32) k3 , k4 , k5 = random . split ( k2 , num = 3 ) d = random . normal ( key = k3 ) d DeviceArray(0.04692494, dtype=float32) By splitting the key into two, three, or even 1000 parts, we can get new keys that are derived from a parent key that generate different random numbers from the same random number generating function. Let's explore how we can use this in the generation of a Gaussian random walk.","title":"Fully Controllable Random Number Generation"},{"location":"02-jax-idioms/04-random-numbers/#example-simulating-a-gaussian-random-walk","text":"A Gaussian random walk is one where we start at a point that is drawn from a Gaussian, and then we draw another point from a Gausian using the first point as the starting Gaussian point. Does that loop structure sound familiar? Well... yeah, it sounds like a classic lax.scan setup! Here's how we might set it up. Firstly, JAX's random.normal function doesn't allow us to specify the location and scale, and only gives us a draw from a unit Gaussian. We can work around this, because any unit Gaussian draw can be shifted and scaled to a N(\\mu, \\sigma) N(\\mu, \\sigma) by multiplying the draw by \\sigma \\sigma and adding \\mu \\mu . To get a length 1000 random draw, we can split the key 1000 ways, and use lax.scan to scan a new Gaussian generator across the keys, thereby giving us 1000 unique draws. We then add the old value of the Gaussian to the new draw. We return the tuple ( new_gaussian, old_gaussian ), as we want to have the new gaussian passed into the next iteration, and accumulate the history of the old gaussians. from dl_workshop.jax_idioms import generate_new_gaussian generate_new_gaussian ?? from jax import lax keys = random . split ( key , num = 1000 ) final , result = lax . scan ( generate_new_gaussian , 0. , keys ) result DeviceArray([ 0.0000000e+00, 2.9308948e-01, 9.0737927e-01, -1.8574587e+00, -9.3475866e-01, 5.3657484e-01, -1.6153860e+00, -2.0659945e+00, -1.8341075e+00, -1.4049507e-02, -1.8038048e+00, -1.4442403e+00, -1.5271288e+00, -2.0419145e+00, 5.9077743e-02, -1.3251271e+00, -2.7179704e+00, -2.5978544e+00, -4.0130644e+00, -3.7746308e+00, -4.5405092e+00, -5.8234315e+00, -6.2098603e+00, -5.1640716e+00, -5.3078022e+00, -6.4008999e+00, -6.1982379e+00, -6.6912217e+00, -6.9285545e+00, -6.7963529e+00, -7.4145002e+00, -7.5791554e+00, -6.9574161e+00, -6.8424754e+00, -7.0481572e+00, -8.4638453e+00, -8.5200520e+00, -1.0150510e+01, -1.0404550e+01, -1.0354767e+01, -1.1202571e+01, -9.8503466e+00, -1.0906091e+01, -9.7488117e+00, -9.0915432e+00, -7.0813093e+00, -8.7363491e+00, -6.5654454e+00, -5.8719378e+00, -7.7279820e+00, -7.0762358e+00, -6.9824843e+00, -6.2720618e+00, -6.5822825e+00, -5.1869078e+00, -6.6846662e+00, -6.5237956e+00, -5.5986729e+00, -5.2407198e+00, -5.4419765e+00, -6.3954625e+00, -6.6313639e+00, -7.5193448e+00, -1.0097613e+01, -1.0111365e+01, -9.4919729e+00, -1.1689382e+01, -1.2609113e+01, -1.3132720e+01, -1.2729928e+01, -1.3554560e+01, -1.4612862e+01, -1.3765321e+01, -1.2790593e+01, -1.2499749e+01, -1.0198821e+01, -9.9114904e+00, -8.6181431e+00, -1.0463602e+01, -1.0668167e+01, -1.1387536e+01, -1.0559889e+01, -1.1779433e+01, -8.8845911e+00, -9.3539925e+00, -1.1195022e+01, -8.7970819e+00, -6.6903763e+00, -6.7867746e+00, -7.0319462e+00, -7.5244737e+00, -6.9895887e+00, -7.4406929e+00, -5.9965134e+00, -5.0314608e+00, -4.4997315e+00, -3.6206439e+00, -3.0368054e+00, -4.8258009e+00, -5.9843774e+00, -6.1343188e+00, -6.9391813e+00, -6.3957014e+00, -7.9022403e+00, -6.8429284e+00, -6.3052950e+00, -5.7587605e+00, -5.0461640e+00, -4.3869100e+00, -3.0319991e+00, -3.5835326e+00, -3.4561212e+00, -2.7506545e+00, -3.9921327e+00, -3.1952002e+00, -2.0107675e+00, -2.0431397e+00, -1.2036926e+00, -2.5120826e+00, 1.5073891e-01, 5.9526514e-02, -6.1567672e-02, -1.7971006e-01, -6.4089805e-02, -1.7376822e-01, -3.3485657e-01, -1.5660967e+00, -3.2925894e+00, -3.2538462e+00, -4.1834068e+00, -6.1261258e+00, -5.7395034e+00, -6.7661810e+00, -7.1050177e+00, -5.4329929e+00, -5.6401262e+00, -5.1202645e+00, -5.7162919e+00, -5.2094598e+00, -6.0466599e+00, -4.9979591e+00, -4.4890556e+00, -3.7108121e+00, -4.1257849e+00, -4.3740158e+00, -3.0525262e+00, -3.6016676e+00, -2.9378812e+00, -2.7627447e+00, -3.2016382e+00, -5.1802568e+00, -4.7830458e+00, -6.3186502e+00, -5.6700902e+00, -5.7391586e+00, -6.4026637e+00, -8.0447359e+00, -8.9100914e+00, -7.9058552e+00, -8.8494577e+00, -9.4514074e+00, -9.7416124e+00, -1.1343502e+01, -1.0725481e+01, -1.1037007e+01, -1.2457513e+01, -1.1892594e+01, -1.1273226e+01, -1.1881676e+01, -1.2089799e+01, -1.2382518e+01, -1.1811073e+01, -1.2355193e+01, -1.2284500e+01, -1.2150561e+01, -1.3035995e+01, -1.1861339e+01, -9.7929296e+00, -8.5769711e+00, -8.8338213e+00, -8.9625845e+00, -9.3008270e+00, -8.2740698e+00, -7.2355332e+00, -7.6913581e+00, -8.1070204e+00, -7.8158293e+00, -6.7811961e+00, -7.8087449e+00, -7.1342974e+00, -6.4980474e+00, -6.0770731e+00, -7.3229432e+00, -8.4346581e+00, -7.7853637e+00, -9.2040215e+00, -9.1392860e+00, -8.4487629e+00, -7.6234417e+00, -7.6274519e+00, -8.6342812e+00, -8.8896389e+00, -9.0145798e+00, -1.0174156e+01, -1.0948184e+01, -1.1148661e+01, -1.1817743e+01, -1.2380128e+01, -1.0779567e+01, -1.0633921e+01, -1.0624221e+01, -1.1308586e+01, -1.0950401e+01, -1.1113399e+01, -1.0885623e+01, -1.0229873e+01, -9.5668888e+00, -8.4545860e+00, -6.5746989e+00, -8.2771807e+00, -8.9303951e+00, -8.9252548e+00, -9.0623932e+00, -1.1962385e+01, -9.9415340e+00, -1.0192524e+01, -8.0894470e+00, -7.0265064e+00, -6.7623253e+00, -6.9903803e+00, -7.1455112e+00, -7.8920078e+00, -7.7239509e+00, -7.3774877e+00, -6.4395394e+00, -7.0436544e+00, -8.7256002e+00, -7.1755939e+00, -5.7905116e+00, -4.3698101e+00, -5.1510201e+00, -4.8481994e+00, -6.6298528e+00, -6.7535200e+00, -6.8409166e+00, -6.5135756e+00, -5.4256835e+00, -4.0130773e+00, -3.5815592e+00, -2.3260942e+00, -2.6673090e+00, -3.8151481e+00, -4.9169331e+00, -5.2914739e+00, -6.0849295e+00, -6.6498380e+00, -6.8321829e+00, -6.6892276e+00, -4.2620564e+00, -3.6063349e+00, -3.6935492e+00, -3.2239606e+00, -1.9226974e+00, -2.8547025e+00, -3.1886687e+00, -3.4799323e+00, -2.7632964e+00, -3.8803239e+00, -5.2894616e+00, -3.7164781e+00, -2.1130731e+00, -2.7807741e+00, -3.8356600e+00, -4.4654922e+00, -6.1223135e+00, -6.2291455e+00, -5.2452497e+00, -3.8544254e+00, -4.0835938e+00, -4.7132416e+00, -4.7189741e+00, -3.3033028e+00, -2.0330689e+00, -1.9273376e+00, -2.0315263e+00, -2.7490623e+00, -2.5552733e+00, -2.6076877e+00, -2.4953003e+00, -1.6205913e+00, -5.6036735e-01, 1.3941172e+00, 1.9810356e+00, 3.4585705e+00, 4.6223598e+00, 5.6333132e+00, 5.6642418e+00, 5.4901705e+00, 6.0357161e+00, 5.4238672e+00, 4.9301081e+00, 5.6282086e+00, 6.5141969e+00, 6.3569307e+00, 7.2397623e+00, 8.4335928e+00, 1.0527872e+01, 1.0277812e+01, 1.1162391e+01, 9.9877901e+00, 1.1897879e+01, 1.1802931e+01, 1.3891153e+01, 1.2607279e+01, 1.3688196e+01, 1.4280282e+01, 1.4194159e+01, 1.5063095e+01, 1.5630124e+01, 1.6656620e+01, 1.8520769e+01, 1.7245922e+01, 1.7332148e+01, 1.8136528e+01, 1.7593718e+01, 1.7247683e+01, 1.6601835e+01, 1.8719860e+01, 1.8267977e+01, 1.9116158e+01, 2.0761889e+01, 2.0569153e+01, 2.3161409e+01, 2.2082083e+01, 2.2013521e+01, 2.2380838e+01, 2.1141018e+01, 2.0381895e+01, 2.0923218e+01, 2.1501842e+01, 2.1908739e+01, 2.2140768e+01, 2.1900915e+01, 2.0821558e+01, 1.9753613e+01, 2.0594179e+01, 2.1206045e+01, 2.3502974e+01, 2.3857637e+01, 2.5092167e+01, 2.5346411e+01, 2.5012743e+01, 2.5705475e+01, 2.4005598e+01, 2.3668116e+01, 2.3274820e+01, 2.3368999e+01, 2.2466444e+01, 2.3110544e+01, 2.3757801e+01, 2.3500526e+01, 2.2233465e+01, 2.1470026e+01, 2.1215025e+01, 2.0534557e+01, 2.0794485e+01, 2.1000740e+01, 1.9642010e+01, 1.9264942e+01, 1.9692028e+01, 2.0378180e+01, 2.1518808e+01, 2.0830177e+01, 2.0563187e+01, 2.0399698e+01, 2.1520786e+01, 2.2611856e+01, 2.3350994e+01, 2.3495277e+01, 2.3201815e+01, 2.2468166e+01, 2.2388708e+01, 2.3538897e+01, 2.2670794e+01, 2.3710402e+01, 2.4161610e+01, 2.2987537e+01, 2.2397484e+01, 2.2795181e+01, 2.2931574e+01, 2.4216108e+01, 2.4123053e+01, 2.5571371e+01, 2.6668819e+01, 2.7405973e+01, 2.7994703e+01, 2.8459429e+01, 2.7073235e+01, 2.7061722e+01, 2.8760845e+01, 2.8883043e+01, 2.8625389e+01, 2.9911112e+01, 3.0024462e+01, 2.9932728e+01, 3.0289661e+01, 3.1156015e+01, 3.1827835e+01, 3.1586716e+01, 3.3310631e+01, 3.4046471e+01, 3.4243004e+01, 3.4956837e+01, 3.5139080e+01, 3.6184502e+01, 3.5955849e+01, 3.7509201e+01, 3.7216148e+01, 3.6132656e+01, 3.6760956e+01, 3.5242069e+01, 3.5308361e+01, 3.6642490e+01, 3.6514305e+01, 3.6897408e+01, 3.5683262e+01, 3.4114861e+01, 3.3112411e+01, 3.3603756e+01, 3.2959377e+01, 3.3436260e+01, 3.4570690e+01, 3.5626965e+01, 3.4386700e+01, 3.7291370e+01, 3.7587612e+01, 3.8173309e+01, 3.7969139e+01, 3.7319889e+01, 3.7574493e+01, 3.7078999e+01, 3.7892895e+01, 3.8195210e+01, 3.9049164e+01, 4.0805832e+01, 4.1425289e+01, 4.1006306e+01, 4.2599464e+01, 4.2570702e+01, 4.1265091e+01, 3.9659935e+01, 3.6838615e+01, 3.7152439e+01, 3.4707485e+01, 3.4948238e+01, 3.5298897e+01, 3.5688854e+01, 3.5597324e+01, 3.6016472e+01, 3.4955692e+01, 3.5937595e+01, 3.6321556e+01, 3.6158390e+01, 3.5102604e+01, 3.5943344e+01, 3.5029987e+01, 3.5126171e+01, 3.5180359e+01, 3.3256603e+01, 3.1358570e+01, 3.1270344e+01, 3.2284927e+01, 3.1823986e+01, 3.1956850e+01, 3.1575148e+01, 3.1685295e+01, 2.9895905e+01, 2.9540310e+01, 3.0184551e+01, 3.0167349e+01, 3.2061939e+01, 3.0725494e+01, 3.0744898e+01, 3.0956152e+01, 3.0580080e+01, 3.1252678e+01, 3.0915356e+01, 3.0126585e+01, 3.0106441e+01, 2.9090900e+01, 3.0437056e+01, 3.0409433e+01, 2.9945259e+01, 3.0450762e+01, 3.2142242e+01, 3.1455513e+01, 3.0856722e+01, 3.0757288e+01, 3.1934263e+01, 3.0583834e+01, 3.1049152e+01, 3.1009058e+01, 3.0781527e+01, 3.0927502e+01, 3.1616207e+01, 3.2949509e+01, 3.3827129e+01, 3.4099651e+01, 3.3860271e+01, 3.2565521e+01, 3.2352459e+01, 3.1877657e+01, 3.1700710e+01, 3.1149137e+01, 3.3191036e+01, 3.3546593e+01, 3.4550777e+01, 3.6293682e+01, 3.6673988e+01, 3.6357235e+01, 3.5923759e+01, 3.6504375e+01, 3.7487831e+01, 3.7691296e+01, 3.8078178e+01, 3.8593243e+01, 3.9208504e+01, 3.9726727e+01, 3.8830025e+01, 3.8074921e+01, 3.8185421e+01, 3.9926697e+01, 3.8049431e+01, 3.7234890e+01, 3.6851467e+01, 3.4246452e+01, 3.3483982e+01, 3.3542747e+01, 3.3847820e+01, 3.5174644e+01, 3.5427490e+01, 3.4421764e+01, 3.4347435e+01, 3.4626266e+01, 3.4442036e+01, 3.4479492e+01, 3.4569195e+01, 3.5695679e+01, 3.5770096e+01, 3.3925434e+01, 3.3344734e+01, 3.4328148e+01, 3.3246941e+01, 3.6386784e+01, 3.5068905e+01, 3.6650124e+01, 3.8195084e+01, 3.8098965e+01, 3.8483913e+01, 3.8413033e+01, 3.8648891e+01, 3.8181110e+01, 3.9143925e+01, 3.7722179e+01, 3.6982872e+01, 3.6395641e+01, 3.5374462e+01, 3.5677132e+01, 3.6783710e+01, 3.7833107e+01, 3.6720718e+01, 3.8577248e+01, 3.8477531e+01, 3.8318790e+01, 3.8823742e+01, 3.8284389e+01, 3.9372498e+01, 3.9304340e+01, 3.9745270e+01, 4.0708485e+01, 4.2130310e+01, 4.2701813e+01, 4.2850170e+01, 4.4041939e+01, 4.4493820e+01, 4.4333946e+01, 4.4279903e+01, 4.4928108e+01, 4.6226978e+01, 4.7380543e+01, 4.4302620e+01, 4.4585323e+01, 4.4197090e+01, 4.4005562e+01, 4.5030525e+01, 4.6177292e+01, 4.5057133e+01, 4.4796387e+01, 4.5758343e+01, 4.6264778e+01, 4.5833244e+01, 4.5269356e+01, 4.4239578e+01, 4.4179420e+01, 4.3310829e+01, 4.2324654e+01, 4.2570541e+01, 4.2294926e+01, 4.2662281e+01, 4.1898159e+01, 4.3488464e+01, 4.2308685e+01, 4.0471664e+01, 4.1282433e+01, 4.1386139e+01, 4.1974152e+01, 4.2820770e+01, 4.3028973e+01, 4.2441658e+01, 4.1195194e+01, 4.2222683e+01, 4.2629620e+01, 4.2836639e+01, 4.0610840e+01, 4.0618824e+01, 4.0622406e+01, 4.2302402e+01, 4.1653931e+01, 4.1998516e+01, 4.2532280e+01, 4.3407074e+01, 4.4703434e+01, 4.5959232e+01, 4.4900124e+01, 4.3607147e+01, 4.4185852e+01, 4.3930374e+01, 4.2671288e+01, 4.2647823e+01, 4.3074402e+01, 4.0779964e+01, 3.9422771e+01, 3.9817394e+01, 4.0903217e+01, 4.0945713e+01, 4.0179401e+01, 3.9954971e+01, 3.8774868e+01, 3.9389870e+01, 3.8990650e+01, 4.0301315e+01, 4.0336689e+01, 3.9112911e+01, 3.8991299e+01, 3.9274616e+01, 3.8024837e+01, 3.8186394e+01, 3.6334290e+01, 3.5846634e+01, 3.4716499e+01, 3.5293205e+01, 3.6128590e+01, 3.5766850e+01, 3.5954765e+01, 3.6176323e+01, 3.5974739e+01, 3.5190342e+01, 3.3506847e+01, 3.2950245e+01, 3.5488556e+01, 3.5793999e+01, 3.5961895e+01, 3.5924114e+01, 3.6066338e+01, 3.5062145e+01, 3.4197170e+01, 3.5094486e+01, 3.4696815e+01, 3.5347645e+01, 3.5017605e+01, 3.5452106e+01, 3.4469566e+01, 3.3246662e+01, 3.2555874e+01, 3.1336807e+01, 3.0030834e+01, 2.9280752e+01, 3.0166458e+01, 3.0552605e+01, 3.2284317e+01, 3.1299629e+01, 3.1654791e+01, 3.2651154e+01, 3.1412001e+01, 3.1104824e+01, 2.9514833e+01, 2.8710270e+01, 2.7595047e+01, 2.8285002e+01, 2.7775763e+01, 2.7963326e+01, 2.6885824e+01, 2.5964947e+01, 2.8124084e+01, 3.0352627e+01, 3.1669094e+01, 2.9583199e+01, 2.9716843e+01, 2.9729292e+01, 3.1786596e+01, 3.1789055e+01, 3.2763531e+01, 3.3053158e+01, 3.1871616e+01, 3.2073021e+01, 3.0871887e+01, 2.9844360e+01, 3.0924242e+01, 3.1848522e+01, 3.3157806e+01, 3.0890354e+01, 3.1631508e+01, 3.0394520e+01, 3.0855812e+01, 2.8997591e+01, 3.0913607e+01, 3.1538607e+01, 3.2221794e+01, 3.1800053e+01, 3.2862667e+01, 3.3099739e+01, 3.3546024e+01, 3.2815430e+01, 3.2776962e+01, 3.2717190e+01, 3.4095726e+01, 3.4460094e+01, 3.4477234e+01, 3.2751495e+01, 3.1662571e+01, 3.2135162e+01, 3.1407804e+01, 3.0974413e+01, 3.1359869e+01, 2.9897268e+01, 2.8611977e+01, 2.7841516e+01, 2.8652159e+01, 2.7959852e+01, 2.7184647e+01, 2.5211527e+01, 2.3363140e+01, 2.4084505e+01, 2.2724066e+01, 2.2931398e+01, 2.2261568e+01, 2.1220295e+01, 2.2457872e+01, 2.3890150e+01, 2.3751272e+01, 2.3846703e+01, 2.2932783e+01, 2.3326431e+01, 2.3571524e+01, 2.4509272e+01, 2.2720484e+01, 2.1190624e+01, 2.2890141e+01, 2.2580996e+01, 2.1119761e+01, 2.1426302e+01, 2.1359793e+01, 2.0951298e+01, 2.0533409e+01, 1.9864664e+01, 1.9137741e+01, 2.0104788e+01, 2.0991695e+01, 2.0703228e+01, 2.0457403e+01, 2.0432844e+01, 1.9983459e+01, 2.0347317e+01, 2.1548651e+01, 2.1746424e+01, 2.3398228e+01, 2.4359446e+01, 2.3417252e+01, 2.4232868e+01, 2.5758299e+01, 2.6161476e+01, 2.5474260e+01, 2.6110472e+01, 2.5719536e+01, 2.6882660e+01, 2.6393394e+01, 2.5338243e+01, 2.5338694e+01, 2.4715746e+01, 2.5425732e+01, 2.5330101e+01, 2.4126286e+01, 2.4346094e+01, 2.5075274e+01, 2.6083353e+01, 2.5672998e+01, 2.6789873e+01, 2.7441189e+01, 2.7152185e+01, 2.8773258e+01, 2.8286983e+01, 2.7623968e+01, 2.7085136e+01, 2.7892982e+01, 2.8239511e+01, 3.0850424e+01, 3.0474251e+01, 2.9058525e+01, 2.9551945e+01, 3.1022072e+01, 3.2846928e+01, 3.2378555e+01, 3.1974035e+01, 3.1732973e+01, 3.2823322e+01, 3.1534229e+01, 3.2405807e+01, 3.1176874e+01, 3.1158363e+01, 2.9453104e+01, 2.9252224e+01, 2.9300753e+01, 2.8109919e+01, 2.7284943e+01, 2.6386570e+01, 2.6772263e+01, 2.6448494e+01, 2.7085169e+01, 2.7579880e+01, 2.8534266e+01, 2.6776207e+01, 2.6352282e+01, 2.7356855e+01, 2.5677477e+01, 2.7271610e+01, 2.5735304e+01, 2.4935097e+01, 2.4828808e+01, 2.3865839e+01, 2.5935564e+01, 2.5382603e+01, 2.6312302e+01, 2.6516724e+01, 2.7956665e+01, 2.8309921e+01, 2.8551727e+01, 2.8265148e+01, 2.8631514e+01, 2.8650980e+01, 2.9362616e+01, 2.9412687e+01, 3.0437040e+01, 2.9622648e+01, 2.9777632e+01, 2.8744820e+01, 2.7734375e+01, 2.6796537e+01, 2.6956509e+01, 2.8200478e+01, 2.8237026e+01, 3.0127491e+01, 3.1191160e+01, 2.9725546e+01, 2.8540089e+01, 2.8666386e+01, 2.7104782e+01, 2.9712494e+01, 3.0739010e+01, 3.2321617e+01, 3.2645321e+01, 3.1430027e+01, 3.1431028e+01, 3.3137547e+01, 3.2631096e+01, 3.2854687e+01, 3.2398182e+01, 3.3396172e+01, 3.2681847e+01, 3.4913078e+01, 3.6094872e+01, 3.6287483e+01, 3.6486488e+01, 3.6492893e+01, 3.7116772e+01, 3.8242214e+01, 4.0165955e+01, 3.7979900e+01, 3.9059998e+01, 3.9028103e+01, 4.0075943e+01, 3.8624214e+01, 3.8290436e+01, 3.7825928e+01, 3.6552616e+01, 3.7390743e+01, 3.9012035e+01, 3.9298553e+01, 3.9827915e+01, 4.1841003e+01, 4.0262543e+01, 4.0187325e+01, 4.0574299e+01, 4.1681656e+01, 4.2406036e+01, 4.4020794e+01, 4.5037449e+01, 4.3865047e+01, 4.2779461e+01, 4.1440739e+01, 4.1341339e+01, 3.9818481e+01, 4.0123234e+01, 3.9883667e+01, 4.0748554e+01, 4.2278969e+01, 4.2600727e+01, 4.2560783e+01, 4.3364643e+01, 4.3650795e+01, 4.2917637e+01, 4.2937355e+01, 4.2296829e+01, 4.2014278e+01, 4.2042435e+01, 4.3259327e+01, 4.3390633e+01, 4.4372753e+01, 4.4892471e+01, 4.5385017e+01, 4.7565838e+01, 4.7109280e+01, 4.6290051e+01, 4.5906693e+01, 4.4330467e+01, 4.4608757e+01, 4.3309235e+01, 4.2039761e+01, 4.3436153e+01, 4.3031368e+01, 4.3179710e+01, 4.1960804e+01, 4.2161831e+01, 4.2877914e+01, 4.2087650e+01, 4.2310814e+01, 4.3533005e+01, 4.4523136e+01, 4.4735619e+01, 4.4101280e+01, 4.4951130e+01, 4.5912594e+01, 4.7106377e+01, 4.6318672e+01, 4.5454987e+01, 4.4767693e+01, 4.4964012e+01, 4.4671551e+01, 4.5825607e+01, 4.6738506e+01, 4.4972668e+01, 4.4546360e+01, 4.3239277e+01, 4.4169300e+01, 4.4169174e+01, 4.3997723e+01, 4.3268627e+01, 4.1956364e+01, 4.1147232e+01, 4.1304501e+01, 4.1476833e+01, 4.1862213e+01, 4.2731663e+01, 4.3843063e+01, 4.5468189e+01, 4.6342617e+01, 4.7410614e+01, 4.7741390e+01, 4.7063366e+01, 4.7882603e+01, 4.8219383e+01, 4.9347443e+01, 4.6957180e+01, 4.7834709e+01, 4.7918716e+01, 4.7106808e+01, 4.6122921e+01, 4.5530235e+01, 4.5815086e+01, 4.5350376e+01, 4.5075825e+01], dtype=float32) import matplotlib.pyplot as plt plt . plot ( result ) [<matplotlib.lines.Line2D at 0x7fcb18564510>] Looks like we did it! Definitely looks like a proper Gaussian random walk to me. Let's encapsulate this inside a funciton generator, because the next thing we're going to do is to generate multiple realizations of the Gaussian random walk. from dl_workshop.jax_idioms import make_gaussian_random_walk_func make_gaussian_random_walk_func ?? Now, what if we wanted to generate multiple realizations of the Gaussian random walk? Does this sound familiar? If so... yeah, it's a vanilla for-loop, which directly brings us to vmap ! from jax import vmap num_realizations = 200 keys = random . split ( key , num_realizations ) grw_1000_steps = make_gaussian_random_walk_func ( 1000 ) final , trajectories = vmap ( grw_1000_steps )( keys ) trajectories . shape (200, 1000) We did it! We have 200 trajectories of a 1000-step Gaussian random walk. Notice also how the program is structured very nicely: Each layer of abstraction in the program corresponds to a new axis dimension along which we are working. The onion layering of the program has very natural structure for the problem at hand. Enough prosyletizing from me, let's visualize the Gaussian random walk to make sure it genuinely is a GRW. import seaborn as sns fig , ax = plt . subplots () for trajectory in trajectories [ 0 : 20 ]: ax . plot ( trajectory ) sns . despine () Now, note how if you were to re-run the entire program from top-to-bottom again, you would get exactly the same plot . This is what we mean by \"reproducible\". Traditional array programs are not fully reproducible, they are only \"kind of\" reproducible in the limit of many runs of the same program. With JAX's random number generation paradigm, any random number generation program is 100% reproducible, down to the level of the exact sequence of random number draws, as long as the seed(s) controlling the program are 100% identical. When an error shows up in a program, as long as its stochastic components are controlled by hand-set seeds, that error is 100% reproducible. For those who have tried working with stochastic programs before, this is an extremely desirable property, as it means we gain the ability to reliably debug our program -- absolutely crucial especially when it comes to working with probabilistic models. Also notice how we finally wrote our first productive for-loop -- but it was only to plot something, not for some form of calculations :).","title":"Example: Simulating a Gaussian random walk"},{"location":"02-jax-idioms/05-composition-example/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' What do we gain doing with compositionality? To help us get a handle over what kind of gains we get, I'm going to do a comparison between composed lax.scan and vmaps against a program that we might write in pure Python versus our compiled version. Writing a Gaussian random walk in pure Python Let's start with a pure Python implementation of a Gaussian random walk, leveraging vanilla NumPy's random module for API convenience only (and not for performance). import numpy as onp def gaussian_random_walk_python ( num_realizations , num_timesteps ): rws = [] for i in range ( num_realizations ): rw = [] prev_draw = 0 for t in range ( num_timesteps ): prev_draw = onp . random . normal ( loc = prev_draw ) rw . append ( prev_draw ) rws . append ( rw ) return rws from time import time N_REALIZATIONS = 500 N_TIMESTEPS = 10_000 start = time () trajectories_python = gaussian_random_walk_python ( N_REALIZATIONS , N_TIMESTEPS ) end = time () print ( f \" { end - start : .2f } seconds\" ) 12.44 seconds import matplotlib.pyplot as plt import seaborn as sns for trajectory in trajectories_python [: 20 ]: plt . plot ( trajectory ) sns . despine () Comparison against our JAX program Let's now compare the program against the version we wrote above. from dl_workshop.jax_idioms import make_gaussian_random_walk_func from jax import vmap def gaussian_random_walk_jax ( num_realizations , num_timesteps ): keys = random . split ( key , num_realizations ) grw_k_steps = make_gaussian_random_walk_func ( num_timesteps ) final , trajectories = vmap ( grw_k_steps )( keys ) return final , trajectories from jax import random key = random . PRNGKey ( 42 ) start = time () final_jax , trajectories_jax = gaussian_random_walk_jax ( N_REALIZATIONS , N_TIMESTEPS ) trajectories_jax . block_until_ready () end = time () print ( f \" { end - start : .2f } seconds\" ) WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) 1.45 seconds for trajectory in trajectories_jax [: 20 ]: plt . plot ( trajectory ) sns . despine () Compare against a JIT-compiled version of our JAX program Now we're going to JIT-compile our Gaussian Random Walk function and see how long it takes for the program to run. from jax import jit def gaussian_random_walk_jit ( num_realizations , num_timesteps ): keys = random . split ( key , num_realizations ) grw_k_steps = make_gaussian_random_walk_func ( num_timesteps ) grw_k_steps = jit ( grw_k_steps ) final , trajectories = vmap ( grw_k_steps )( keys ) return final , trajectories start = time () final_jit , trajectories_jit = gaussian_random_walk_jit ( N_REALIZATIONS , N_TIMESTEPS ) trajectories_jit . block_until_ready () end = time () print ( f \" { end - start : .2f } seconds\" ) 1.14 seconds for trajectory in trajectories_jit [: 20 ]: plt . plot ( trajectory ) sns . despine () JIT-compilation gave us about a 1-2X speedup over non-JIT compiled code, and was about 20X faster than the pure Python version. That shouldn't surprise you one bit :). A few pointers on syntax Firstly, if we subscribe to the Zen of Python's notion that \"flat is better than nested\", then following the idioms listed here -- closures/partials, vmap and lax.scan , then we'll likely only ever go one closure deep into our programs. Notice how we basically never wrote any for-loops in our array code; they were handled elegantly by the looping constructs vmap and lax.scan . Secondly, using jit , we get further optimizations on our code for free. A pre-requisite of jit is that the every function call made in the program function being jit -ed is required to be written in a \"pure functional\" style, i.e. there are no side effects, no mutation of global state. If you write a program using the idioms used here (closures to wrap state, vmap / lax.scan in lieu of loops, explicit random number generation using PRNGKeys), then you will be able to JIT compile the program with ease.","title":"Compositionality"},{"location":"02-jax-idioms/05-composition-example/#what-do-we-gain-doing-with-compositionality","text":"To help us get a handle over what kind of gains we get, I'm going to do a comparison between composed lax.scan and vmaps against a program that we might write in pure Python versus our compiled version.","title":"What do we gain doing with compositionality?"},{"location":"02-jax-idioms/05-composition-example/#writing-a-gaussian-random-walk-in-pure-python","text":"Let's start with a pure Python implementation of a Gaussian random walk, leveraging vanilla NumPy's random module for API convenience only (and not for performance). import numpy as onp def gaussian_random_walk_python ( num_realizations , num_timesteps ): rws = [] for i in range ( num_realizations ): rw = [] prev_draw = 0 for t in range ( num_timesteps ): prev_draw = onp . random . normal ( loc = prev_draw ) rw . append ( prev_draw ) rws . append ( rw ) return rws from time import time N_REALIZATIONS = 500 N_TIMESTEPS = 10_000 start = time () trajectories_python = gaussian_random_walk_python ( N_REALIZATIONS , N_TIMESTEPS ) end = time () print ( f \" { end - start : .2f } seconds\" ) 12.44 seconds import matplotlib.pyplot as plt import seaborn as sns for trajectory in trajectories_python [: 20 ]: plt . plot ( trajectory ) sns . despine ()","title":"Writing a Gaussian random walk in pure Python"},{"location":"02-jax-idioms/05-composition-example/#comparison-against-our-jax-program","text":"Let's now compare the program against the version we wrote above. from dl_workshop.jax_idioms import make_gaussian_random_walk_func from jax import vmap def gaussian_random_walk_jax ( num_realizations , num_timesteps ): keys = random . split ( key , num_realizations ) grw_k_steps = make_gaussian_random_walk_func ( num_timesteps ) final , trajectories = vmap ( grw_k_steps )( keys ) return final , trajectories from jax import random key = random . PRNGKey ( 42 ) start = time () final_jax , trajectories_jax = gaussian_random_walk_jax ( N_REALIZATIONS , N_TIMESTEPS ) trajectories_jax . block_until_ready () end = time () print ( f \" { end - start : .2f } seconds\" ) WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) 1.45 seconds for trajectory in trajectories_jax [: 20 ]: plt . plot ( trajectory ) sns . despine ()","title":"Comparison against our JAX program"},{"location":"02-jax-idioms/05-composition-example/#compare-against-a-jit-compiled-version-of-our-jax-program","text":"Now we're going to JIT-compile our Gaussian Random Walk function and see how long it takes for the program to run. from jax import jit def gaussian_random_walk_jit ( num_realizations , num_timesteps ): keys = random . split ( key , num_realizations ) grw_k_steps = make_gaussian_random_walk_func ( num_timesteps ) grw_k_steps = jit ( grw_k_steps ) final , trajectories = vmap ( grw_k_steps )( keys ) return final , trajectories start = time () final_jit , trajectories_jit = gaussian_random_walk_jit ( N_REALIZATIONS , N_TIMESTEPS ) trajectories_jit . block_until_ready () end = time () print ( f \" { end - start : .2f } seconds\" ) 1.14 seconds for trajectory in trajectories_jit [: 20 ]: plt . plot ( trajectory ) sns . despine () JIT-compilation gave us about a 1-2X speedup over non-JIT compiled code, and was about 20X faster than the pure Python version. That shouldn't surprise you one bit :).","title":"Compare against a JIT-compiled version of our JAX program"},{"location":"02-jax-idioms/05-composition-example/#a-few-pointers-on-syntax","text":"Firstly, if we subscribe to the Zen of Python's notion that \"flat is better than nested\", then following the idioms listed here -- closures/partials, vmap and lax.scan , then we'll likely only ever go one closure deep into our programs. Notice how we basically never wrote any for-loops in our array code; they were handled elegantly by the looping constructs vmap and lax.scan . Secondly, using jit , we get further optimizations on our code for free. A pre-requisite of jit is that the every function call made in the program function being jit -ed is required to be written in a \"pure functional\" style, i.e. there are no side effects, no mutation of global state. If you write a program using the idioms used here (closures to wrap state, vmap / lax.scan in lieu of loops, explicit random number generation using PRNGKeys), then you will be able to JIT compile the program with ease.","title":"A few pointers on syntax"},{"location":"03-stax/01-linear/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Writing linear models with stax In this notebook, I'll show the code for how to use JAX's stax submodule to write arbitrary models. Prerequisites I'm assuming you have read through the jax-programming.ipynb notebook, as well as the tutorial.ipynb notebook. The main tutorial.ipynb notebook gives you a general introduction to differential programming using grad , while the jax-programming.ipynb notebook gives you a flavour of the other four main JAX idioms: vmap , lax.scan , random.PRNGKey , and jit . What is stax ? Most deep learning libraries use objects as the data structure for a neural network layer. As such, the tunable parameters of the layer, for example w and b for a linear (\"dense\") layer are class attributes associated with the forward function. In some sense, because a neural network layer is nothing more than a math function, specifying the layer in terms of a function might also make sense. stax , then, is a new take on writing neural network models using pure functions rather than objects. How does stax work? The way that stax layers work is as follows. Every neural network layer is nothing more than a math function with a \"forward\" pass. Neural network models typically have their parameters initialized into the right shapes using random number generators. Put these two together, and we have a pair of functions that specify a layer: An init_fun function, that initializes parameters into the correct shapes, and An apply_fun function, that applies the specified math transformations onto incoming data, using parameters of the correct shape. Example: Linear layer Let's see an example of this in action, by studying the implementation of the linear (\"dense\") layer in stax from jax.experimental import stax stax . Dense ?? As you can see, the apply_fun specifies the linear transformation. It accepts a parameter called params , which gets tuple-unpacked into the appropriate W and b . Notice how the params argument matches up with the second output of init_fun ! The init_fun always accepts an rng parameter, which is returned from JAX's jax.random.PRNGKey() . It also accepts an input_shape parameter, which specifies what the elementary shape of one sample of data is. So if your entire dataset is of shape (n_samples, n_columns) , then you would put in (n_columns,) inside there, as you would want to ignore the sample dimension, thus allowing us to take advantage of vmap to map our model function over each and every i.i.d. sample in our dataset. The init_fun also returns the output_shape , which is used later when we chain layers together. Let's see how we can use the Dense layer to specify a linear regression model. Create the initialization and application function pairs Firstly, we create the init_fun and apply_fun pair: init_fun , apply_fun = stax . Dense ( 1 ) Initialize the parameters Now, let's initialize parameters using the init_fun . Let's assume that we have data that is of 4 columns only. from jax import random , numpy as np key = random . PRNGKey ( 42 ) output_shape , params_initial = init_fun ( key , input_shape = ( 4 ,)) WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) params_initial (DeviceArray([[ 0.43186307], [ 0.9563157 ], [ 0.9000483 ], [-0.32341173]], dtype=float32), DeviceArray([0.01369469], dtype=float32)) Apply parameters and data through function We'll create some randomly generated data. X = random . normal ( key , shape = ( 200 , 4 )) X [ 0 : 5 ], X . shape (DeviceArray([[ 0.03751167, 0.8777058 , -1.2008178 , -1.3824965 ], [-0.37519178, 1.5957882 , -1.5086783 , -0.75612265], [-0.51650995, -1.056697 , 0.99382603, -0.3520223 ], [-1.4446373 , -0.51537496, 0.30910558, -0.31770143], [ 0.9590066 , -0.6170032 , -0.37160665, -0.7339001 ]], dtype=float32), (200, 4)) Here's some y_true values that I've snuck in. y_true = np . dot ( X , np . array ([ 1 , 2 , 3 , 4 ])) + 5 y_true = y_true . reshape ( - 1 , 1 ) y_true [ 0 : 5 ], y_true . shape (DeviceArray([[-2.3395162 ], [ 0.26585913], [ 3.9434848 ], [ 2.1811237 ], [ 0.6745796 ]], dtype=float32), (200, 1)) Now, we'll pass data through the linear model! apply_fun ?? from jax import vmap from functools import partial y_pred = vmap ( partial ( apply_fun , params_initial ))( X ) y_pred [ 0 : 5 ], y_pred . shape (DeviceArray([[ 0.23557997], [ 0.26439613], [-0.21156326], [-0.72209364], [-0.2593076 ]], dtype=float32), (200, 1)) Voil\u00e0! We have a simple linear model implemented just like that. Optimization Next question: how do we optimize the parameters using JAX? Instead of writing a training loop on our own, we can take advantage of JAX's optimizers, which are also written in a functional paradigm! JAX's optimizers are constructed as a \"triplet\" set of functions: init : Takes params and initializes them in as a state , which is structured in a fashion that update can operate on. update : Takes in i , g , and state , which respectively are: i : The current loop iteration g : Gradients calculated from grad ! state : The current state of the parameters. get_params : Takes in the state at a given point, and returns the parameters structured correctly. from jax import jit , grad from jax.experimental.optimizers import adam init , update , get_params = adam ( step_size = 1e-1 ) update = jit ( update ) get_params = jit ( get_params ) Loss Function We're still missing a piece here, that is the loss function. For illustration purposes, let's use the mean squared error. def mseloss ( params , model , x , y_true ): y_preds = vmap ( partial ( model , params ))( x ) return np . mean ( np . power ( y_preds - y_true , 2 )) dmseloss = grad ( mseloss ) \"Step\" portion of update loop Now, we're going to define the \"step\" portion of the update loop. from dl_workshop.stax_models import step step ?? JIT compilation Because it takes so many parameters (in order to remain pure, and not rely on notebook state), we're going to bind some of them using functools.partial . I'm also going to show you what happens when we JIT-compile vs. don't JIT-compile the function. step_partial = partial ( step , get_params = get_params , dlossfunc = dmseloss , update = update , model = apply_fun , x = X , y_true = y_true ) step_partial_jit = jit ( step_partial ) Explicit loops Firstly, let's see what kind of code we'd write if we did write the loop explicitly. from time import time start = time () state = init ( params_initial ) for i in range ( 1000 ): params = get_params ( state ) g = dmseloss ( params , apply_fun , X , y_true ) state = update ( i , g , state ) end = time () print ( end - start ) 5.72387170791626 Partialled out loop step Now, let's run the loop with the partialled out function. start = time () state = init ( params_initial ) for i in range ( 1000 ): state = step_partial ( i , state ) end = time () print ( end - start ) 5.486717700958252 JIT-compiled loop! This is much cleaner of a loop, but we did have to do some work up-front. What happens if we now use the JIT-ed function? start = time () state = init ( params_initial ) for i in range ( 1000 ): state = step_partial_jit ( i , state ) end = time () print ( end - start ) 0.45990562438964844 Whoa, holy smokes, that's fast! At least 10X faster using JIT-compilation. lax.scan loop Now we'll use some JAX trickery ot write a training loop without ever writing a for-loop. from dl_workshop.stax_models import make_scannable_step make_scannable_step ?? from jax import lax scannable_step = make_scannable_step ( step_partial_jit ) start = time () initial_state = init ( params_initial ) final_state , states_history = lax . scan ( scannable_step , initial_state , np . arange ( 1000 )) end = time () print ( end - start ) 0.35522913932800293 get_params ( final_state ) (DeviceArray([[1. ], [2.0000002], [3. ], [3.9999995]], dtype=float32), DeviceArray([5.0000005], dtype=float32)) vmap -ed training loop over multiple starting points Now, we're going to do the ultimate: we'll create at least 100 different parameter initializations and run our training loop over each of them. from dl_workshop.stax_models import make_training_start make_training_start ?? from jax import lax train_linear = make_training_start ( partial ( init_fun , input_shape = ( - 1 , 4 )), init , scannable_step , 1000 ) start = time () N_INITIALIZATIONS = 100 initialization_keys = random . split ( key , N_INITIALIZATIONS ) final_states , states_histories = vmap ( train_linear )( initialization_keys ) end = time () print ( end - start ) 1.7242414951324463 w_final , b_final = vmap ( get_params )( final_states ) w_final . squeeze ()[ 0 : 5 ] DeviceArray([[1.0000002, 2. , 2.9999998, 4.0000005], [1.0000002, 2. , 3. , 4.0000005], [1.0000002, 2. , 3. , 4.000001 ], [1.0000001, 2. , 3. , 3.9999998], [1.0000001, 2. , 3.0000002, 4. ]], dtype=float32) b_final . squeeze ()[ 0 : 5 ] DeviceArray([5.000001, 5.000001, 5.000001, 5.000001, 5.000001], dtype=float32) Looks like we were also able to run the whole optimization pretty fast, and recover the correct parameters over multiple training starts. JIT-compiled training loop What happens if we JIT-compile the vmapped initialization? start = time () N_INITIALIZATIONS = 100 initialization_keys = random . split ( key , N_INITIALIZATIONS ) train_linear_jit = jit ( train_linear ) final_states , states_histories = vmap ( train_linear_jit )( initialization_keys ) vmap ( get_params )( final_states ) # this line exists to just block the computation until it completes. end = time () print ( end - start ) 1.3208539485931396 HOOOOOLY SMOKES! Did you see that? With JIT-compilation, we essentially took the training time down to be identical to training on one starting point. Naturally, I don't expect this result to hold 100% of the time, but it's pretty darn rad to see that live. The craziest piece here is that we could vmap our training loop over multiple starting points and get massive speedups there.","title":"Linear Models with stax"},{"location":"03-stax/01-linear/#writing-linear-models-with-stax","text":"In this notebook, I'll show the code for how to use JAX's stax submodule to write arbitrary models.","title":"Writing linear models with stax"},{"location":"03-stax/01-linear/#prerequisites","text":"I'm assuming you have read through the jax-programming.ipynb notebook, as well as the tutorial.ipynb notebook. The main tutorial.ipynb notebook gives you a general introduction to differential programming using grad , while the jax-programming.ipynb notebook gives you a flavour of the other four main JAX idioms: vmap , lax.scan , random.PRNGKey , and jit .","title":"Prerequisites"},{"location":"03-stax/01-linear/#what-is-stax","text":"Most deep learning libraries use objects as the data structure for a neural network layer. As such, the tunable parameters of the layer, for example w and b for a linear (\"dense\") layer are class attributes associated with the forward function. In some sense, because a neural network layer is nothing more than a math function, specifying the layer in terms of a function might also make sense. stax , then, is a new take on writing neural network models using pure functions rather than objects.","title":"What is stax?"},{"location":"03-stax/01-linear/#how-does-stax-work","text":"The way that stax layers work is as follows. Every neural network layer is nothing more than a math function with a \"forward\" pass. Neural network models typically have their parameters initialized into the right shapes using random number generators. Put these two together, and we have a pair of functions that specify a layer: An init_fun function, that initializes parameters into the correct shapes, and An apply_fun function, that applies the specified math transformations onto incoming data, using parameters of the correct shape.","title":"How does stax work?"},{"location":"03-stax/01-linear/#example-linear-layer","text":"Let's see an example of this in action, by studying the implementation of the linear (\"dense\") layer in stax from jax.experimental import stax stax . Dense ?? As you can see, the apply_fun specifies the linear transformation. It accepts a parameter called params , which gets tuple-unpacked into the appropriate W and b . Notice how the params argument matches up with the second output of init_fun ! The init_fun always accepts an rng parameter, which is returned from JAX's jax.random.PRNGKey() . It also accepts an input_shape parameter, which specifies what the elementary shape of one sample of data is. So if your entire dataset is of shape (n_samples, n_columns) , then you would put in (n_columns,) inside there, as you would want to ignore the sample dimension, thus allowing us to take advantage of vmap to map our model function over each and every i.i.d. sample in our dataset. The init_fun also returns the output_shape , which is used later when we chain layers together. Let's see how we can use the Dense layer to specify a linear regression model.","title":"Example: Linear layer"},{"location":"03-stax/01-linear/#create-the-initialization-and-application-function-pairs","text":"Firstly, we create the init_fun and apply_fun pair: init_fun , apply_fun = stax . Dense ( 1 )","title":"Create the initialization and application function pairs"},{"location":"03-stax/01-linear/#initialize-the-parameters","text":"Now, let's initialize parameters using the init_fun . Let's assume that we have data that is of 4 columns only. from jax import random , numpy as np key = random . PRNGKey ( 42 ) output_shape , params_initial = init_fun ( key , input_shape = ( 4 ,)) WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) params_initial (DeviceArray([[ 0.43186307], [ 0.9563157 ], [ 0.9000483 ], [-0.32341173]], dtype=float32), DeviceArray([0.01369469], dtype=float32))","title":"Initialize the parameters"},{"location":"03-stax/01-linear/#apply-parameters-and-data-through-function","text":"We'll create some randomly generated data. X = random . normal ( key , shape = ( 200 , 4 )) X [ 0 : 5 ], X . shape (DeviceArray([[ 0.03751167, 0.8777058 , -1.2008178 , -1.3824965 ], [-0.37519178, 1.5957882 , -1.5086783 , -0.75612265], [-0.51650995, -1.056697 , 0.99382603, -0.3520223 ], [-1.4446373 , -0.51537496, 0.30910558, -0.31770143], [ 0.9590066 , -0.6170032 , -0.37160665, -0.7339001 ]], dtype=float32), (200, 4)) Here's some y_true values that I've snuck in. y_true = np . dot ( X , np . array ([ 1 , 2 , 3 , 4 ])) + 5 y_true = y_true . reshape ( - 1 , 1 ) y_true [ 0 : 5 ], y_true . shape (DeviceArray([[-2.3395162 ], [ 0.26585913], [ 3.9434848 ], [ 2.1811237 ], [ 0.6745796 ]], dtype=float32), (200, 1)) Now, we'll pass data through the linear model! apply_fun ?? from jax import vmap from functools import partial y_pred = vmap ( partial ( apply_fun , params_initial ))( X ) y_pred [ 0 : 5 ], y_pred . shape (DeviceArray([[ 0.23557997], [ 0.26439613], [-0.21156326], [-0.72209364], [-0.2593076 ]], dtype=float32), (200, 1)) Voil\u00e0! We have a simple linear model implemented just like that.","title":"Apply parameters and data through function"},{"location":"03-stax/01-linear/#optimization","text":"Next question: how do we optimize the parameters using JAX? Instead of writing a training loop on our own, we can take advantage of JAX's optimizers, which are also written in a functional paradigm! JAX's optimizers are constructed as a \"triplet\" set of functions: init : Takes params and initializes them in as a state , which is structured in a fashion that update can operate on. update : Takes in i , g , and state , which respectively are: i : The current loop iteration g : Gradients calculated from grad ! state : The current state of the parameters. get_params : Takes in the state at a given point, and returns the parameters structured correctly. from jax import jit , grad from jax.experimental.optimizers import adam init , update , get_params = adam ( step_size = 1e-1 ) update = jit ( update ) get_params = jit ( get_params )","title":"Optimization"},{"location":"03-stax/01-linear/#loss-function","text":"We're still missing a piece here, that is the loss function. For illustration purposes, let's use the mean squared error. def mseloss ( params , model , x , y_true ): y_preds = vmap ( partial ( model , params ))( x ) return np . mean ( np . power ( y_preds - y_true , 2 )) dmseloss = grad ( mseloss )","title":"Loss Function"},{"location":"03-stax/01-linear/#step-portion-of-update-loop","text":"Now, we're going to define the \"step\" portion of the update loop. from dl_workshop.stax_models import step step ??","title":"\"Step\" portion of update loop"},{"location":"03-stax/01-linear/#jit-compilation","text":"Because it takes so many parameters (in order to remain pure, and not rely on notebook state), we're going to bind some of them using functools.partial . I'm also going to show you what happens when we JIT-compile vs. don't JIT-compile the function. step_partial = partial ( step , get_params = get_params , dlossfunc = dmseloss , update = update , model = apply_fun , x = X , y_true = y_true ) step_partial_jit = jit ( step_partial )","title":"JIT compilation"},{"location":"03-stax/01-linear/#explicit-loops","text":"Firstly, let's see what kind of code we'd write if we did write the loop explicitly. from time import time start = time () state = init ( params_initial ) for i in range ( 1000 ): params = get_params ( state ) g = dmseloss ( params , apply_fun , X , y_true ) state = update ( i , g , state ) end = time () print ( end - start ) 5.72387170791626","title":"Explicit loops"},{"location":"03-stax/01-linear/#partialled-out-loop-step","text":"Now, let's run the loop with the partialled out function. start = time () state = init ( params_initial ) for i in range ( 1000 ): state = step_partial ( i , state ) end = time () print ( end - start ) 5.486717700958252","title":"Partialled out loop step"},{"location":"03-stax/01-linear/#jit-compiled-loop","text":"This is much cleaner of a loop, but we did have to do some work up-front. What happens if we now use the JIT-ed function? start = time () state = init ( params_initial ) for i in range ( 1000 ): state = step_partial_jit ( i , state ) end = time () print ( end - start ) 0.45990562438964844 Whoa, holy smokes, that's fast! At least 10X faster using JIT-compilation.","title":"JIT-compiled loop!"},{"location":"03-stax/01-linear/#laxscan-loop","text":"Now we'll use some JAX trickery ot write a training loop without ever writing a for-loop. from dl_workshop.stax_models import make_scannable_step make_scannable_step ?? from jax import lax scannable_step = make_scannable_step ( step_partial_jit ) start = time () initial_state = init ( params_initial ) final_state , states_history = lax . scan ( scannable_step , initial_state , np . arange ( 1000 )) end = time () print ( end - start ) 0.35522913932800293 get_params ( final_state ) (DeviceArray([[1. ], [2.0000002], [3. ], [3.9999995]], dtype=float32), DeviceArray([5.0000005], dtype=float32))","title":"lax.scan loop"},{"location":"03-stax/01-linear/#vmap-ed-training-loop-over-multiple-starting-points","text":"Now, we're going to do the ultimate: we'll create at least 100 different parameter initializations and run our training loop over each of them. from dl_workshop.stax_models import make_training_start make_training_start ?? from jax import lax train_linear = make_training_start ( partial ( init_fun , input_shape = ( - 1 , 4 )), init , scannable_step , 1000 ) start = time () N_INITIALIZATIONS = 100 initialization_keys = random . split ( key , N_INITIALIZATIONS ) final_states , states_histories = vmap ( train_linear )( initialization_keys ) end = time () print ( end - start ) 1.7242414951324463 w_final , b_final = vmap ( get_params )( final_states ) w_final . squeeze ()[ 0 : 5 ] DeviceArray([[1.0000002, 2. , 2.9999998, 4.0000005], [1.0000002, 2. , 3. , 4.0000005], [1.0000002, 2. , 3. , 4.000001 ], [1.0000001, 2. , 3. , 3.9999998], [1.0000001, 2. , 3.0000002, 4. ]], dtype=float32) b_final . squeeze ()[ 0 : 5 ] DeviceArray([5.000001, 5.000001, 5.000001, 5.000001, 5.000001], dtype=float32) Looks like we were also able to run the whole optimization pretty fast, and recover the correct parameters over multiple training starts.","title":"vmap-ed training loop over multiple starting points"},{"location":"03-stax/01-linear/#jit-compiled-training-loop","text":"What happens if we JIT-compile the vmapped initialization? start = time () N_INITIALIZATIONS = 100 initialization_keys = random . split ( key , N_INITIALIZATIONS ) train_linear_jit = jit ( train_linear ) final_states , states_histories = vmap ( train_linear_jit )( initialization_keys ) vmap ( get_params )( final_states ) # this line exists to just block the computation until it completes. end = time () print ( end - start ) 1.3208539485931396 HOOOOOLY SMOKES! Did you see that? With JIT-compilation, we essentially took the training time down to be identical to training on one starting point. Naturally, I don't expect this result to hold 100% of the time, but it's pretty darn rad to see that live. The craziest piece here is that we could vmap our training loop over multiple starting points and get massive speedups there.","title":"JIT-compiled training loop"},{"location":"03-stax/02-neural/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Writing neural network models using stax We're now going to try rewriting the neural network model that we had earlier on, now using stax syntax, and traing it using the syntax that we have learned above. Using stax.serial Firstly, let's replicate the model using stax.serial . It's a serial composition of a Dense+Tanh layer, followed by a Dense+Sigmoid layer. from jax.experimental import stax nn_init , nn_apply = stax . serial ( stax . Dense ( 20 ), stax . Tanh , stax . Dense ( 1 ), stax . Sigmoid ) def nn_init_wrapper ( input_shape ): def inner ( key ): return nn_init ( key , input_shape ) return inner nn_initializer = nn_init_wrapper ( input_shape = ( - 1 , 41 )) nn_initializer <function __main__.nn_init_wrapper.<locals>.inner(key)> Now, we initialize one instance of the parameters. from jax import random key = random . PRNGKey ( 42 ) output_shape , params_init = nn_initializer ( key ) WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) We'll need a loss funciton to optimize as well. from jax import grad , numpy as np , vmap from functools import partial def binary_cross_entropy ( y_true , y_pred , tol = 1e-6 ): return y_true * np . log ( y_pred + tol ) + ( 1 - y_true ) * np . log ( 1 - y_pred + tol ) def logistic_loss ( params , model , x , y ): preds = vmap ( partial ( model , params ))( x ) bces = vmap ( binary_cross_entropy )( y , preds ) return - np . sum ( bces ) dlogistic_loss = grad ( logistic_loss ) Load in data Now, we load in the data. import pandas as pd from pyprojroot import here X = pd . read_csv ( here () / 'data/biodeg_X.csv' , index_col = 0 ) y = pd . read_csv ( here () / 'data/biodeg_y.csv' , index_col = 0 ) Test-drive functions to make sure they work Always important. It'll reveal whether there's anything wrong with our code. logistic_loss ( params_init , nn_apply , X . values , y . values ) DeviceArray(910.0804, dtype=float32) Progressively construct our training functions Firstly, we make sure the step function works with our logistic loss, model func, and actual data. from jax.experimental.optimizers import adam adam_init , update , get_params = adam ( 0.0005 ) from dl_workshop.stax_models import step , make_scannable_step , make_training_start from time import time stepfunc_nn = partial ( step , dlossfunc = dlogistic_loss , get_params = get_params , update = update , model = nn_apply , x = X . values , y_true = y . values ) scannable_step = make_scannable_step ( stepfunc_nn ) train_nn = make_training_start ( nn_initializer , adam_init , scannable_step , n_steps = 3000 ) start = time () final_state , states_history = train_nn ( key ) end = time () print ( end - start ) 0.944758415222168 Friends, if you remember where we started in the tutorial.ipynb notebook, the original neural network took approximately a minute to train on a GPU (and longer if on a CPU). Let's now start by ploting the loss over training iterations. We start first with a function that returns the loss from a given state object. import matplotlib.pyplot as plt def calculate_loss ( state , get_params , model , lossfunc , x , y ): params = get_params ( state ) return lossfunc ( params , model , x , y ) calculate_loss ( final_state , get_params , nn_apply , logistic_loss , X . values , y . values ) DeviceArray(101.23043, dtype=float32) Now, we need to vmap it over all states in the states history, to get back the loss score. calc_loss_vmap = partial ( calculate_loss , get_params = get_params , model = nn_apply , lossfunc = logistic_loss , x = X . values , y = y . values ) start = time () losses = vmap ( calc_loss_vmap )( states_history ) end = time () print ( end - start ) plt . plot ( losses ) 1.5747764110565186 [<matplotlib.lines.Line2D at 0x7f5d800e06d0>] Training with multiple starting points Just as above, we can also train the neural network with multiple starting points, again by vmap -ing our training function across split PRNGKeys. keys = random . split ( key , 5 ) start = time () final_states , state_histories = vmap ( train_nn )( keys ) end = time () print ( end - start ) 3.3660812377929688 get_params ( final_states )[ 0 ][ 0 ] . shape (5, 41, 20) Let's plot the losses over each of the state histories. Our last function calc_loss_vmap calculates loss score for one time point, which we then vmap over a single states_history , so we need another function that encapsulates this behaviour and vmap s over all state histories. def state_history_loss ( state_history ): losses = vmap ( calc_loss_vmap )( state_history ) return losses losses = vmap ( state_history_loss )( state_histories ) losses . shape (5, 3000) losses DeviceArray([[ 892.20435 , 885.81555 , 879.50555 , ..., 106.13542 , 106.08061 , 106.02581 ], [ 747.18805 , 740.2636 , 733.4714 , ..., 87.10681 , 87.06522 , 87.02366 ], [ 808.54504 , 802.43036 , 796.3972 , ..., 87.570694, 87.52497 , 87.47929 ], [1273.8547 , 1264.2145 , 1254.6217 , ..., 140.18053 , 140.12332 , 140.06607 ], [ 720.6325 , 716.0028 , 711.449 , ..., 76.929665, 76.8839 , 76.83814 ]], dtype=float32) Correctly-shaped! And now plotting it... plt . plot ( losses . T ) [<matplotlib.lines.Line2D at 0x7f5d706d8bd0>, <matplotlib.lines.Line2D at 0x7f5d70617a50>, <matplotlib.lines.Line2D at 0x7f5d7063cc10>, <matplotlib.lines.Line2D at 0x7f5d7063c190>, <matplotlib.lines.Line2D at 0x7f5d7063ca10>] Now that's pretty cool! We were able to see the loss from three independent runs. With sufficient memory, one would be able to do more runs; when I was writing this notebook early on, I saw that it was getting difficult to do on the order of tens of runs due to memory allocation issues. Summary In this notebook, we saw a few things in action. Firstly, we saw how to use the stax module on a linear model. Anytime we have a new framework for doing differential programming, it's super important to be able to explore it in the context of a linear model, which is basically the foundation of all deep learning. Secondly, we also explored how to leverage the JAX idioms to create fast parallelized training loops. We mixed-and-matched together jit , vmap , lax.scan , and grad into a performant training loop that was minimally nested. A corollary of this programming style is that every piece of the code can, in principle, be properly tested , because they are properly isolated. Have you written training loops where you modify a little piece here and a little piece there, until you lost what your original working one looked like? With training functions that are minimally nested, we can control the behaviour explicitly using closures/partials easily. Even when doing experimenation, our code can run reliably and fast. Thirdly, we saw how to apply the same lessons to training a neural network really fast with multiple starting points. The essence of the solution was to properly structure our program in progressively higher level layers of abstraction. We carefully wrote the program to go from the inner most layer out until we hit our goal of allowing for a set of multiple starts. The key here is that each level of abstraction is very natural, and corresponds to a \"unit computation\" being applied consistently across an \"array\" of things. Once we identify that \"unit computation\", writing the vmap -able or lax.scan -able function becomes very easy.","title":"Neural Networks with stax"},{"location":"03-stax/02-neural/#writing-neural-network-models-using-stax","text":"We're now going to try rewriting the neural network model that we had earlier on, now using stax syntax, and traing it using the syntax that we have learned above.","title":"Writing neural network models using stax"},{"location":"03-stax/02-neural/#using-staxserial","text":"Firstly, let's replicate the model using stax.serial . It's a serial composition of a Dense+Tanh layer, followed by a Dense+Sigmoid layer. from jax.experimental import stax nn_init , nn_apply = stax . serial ( stax . Dense ( 20 ), stax . Tanh , stax . Dense ( 1 ), stax . Sigmoid ) def nn_init_wrapper ( input_shape ): def inner ( key ): return nn_init ( key , input_shape ) return inner nn_initializer = nn_init_wrapper ( input_shape = ( - 1 , 41 )) nn_initializer <function __main__.nn_init_wrapper.<locals>.inner(key)> Now, we initialize one instance of the parameters. from jax import random key = random . PRNGKey ( 42 ) output_shape , params_init = nn_initializer ( key ) WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) We'll need a loss funciton to optimize as well. from jax import grad , numpy as np , vmap from functools import partial def binary_cross_entropy ( y_true , y_pred , tol = 1e-6 ): return y_true * np . log ( y_pred + tol ) + ( 1 - y_true ) * np . log ( 1 - y_pred + tol ) def logistic_loss ( params , model , x , y ): preds = vmap ( partial ( model , params ))( x ) bces = vmap ( binary_cross_entropy )( y , preds ) return - np . sum ( bces ) dlogistic_loss = grad ( logistic_loss )","title":"Using stax.serial"},{"location":"03-stax/02-neural/#load-in-data","text":"Now, we load in the data. import pandas as pd from pyprojroot import here X = pd . read_csv ( here () / 'data/biodeg_X.csv' , index_col = 0 ) y = pd . read_csv ( here () / 'data/biodeg_y.csv' , index_col = 0 )","title":"Load in data"},{"location":"03-stax/02-neural/#test-drive-functions-to-make-sure-they-work","text":"Always important. It'll reveal whether there's anything wrong with our code. logistic_loss ( params_init , nn_apply , X . values , y . values ) DeviceArray(910.0804, dtype=float32)","title":"Test-drive functions to make sure they work"},{"location":"03-stax/02-neural/#progressively-construct-our-training-functions","text":"Firstly, we make sure the step function works with our logistic loss, model func, and actual data. from jax.experimental.optimizers import adam adam_init , update , get_params = adam ( 0.0005 ) from dl_workshop.stax_models import step , make_scannable_step , make_training_start from time import time stepfunc_nn = partial ( step , dlossfunc = dlogistic_loss , get_params = get_params , update = update , model = nn_apply , x = X . values , y_true = y . values ) scannable_step = make_scannable_step ( stepfunc_nn ) train_nn = make_training_start ( nn_initializer , adam_init , scannable_step , n_steps = 3000 ) start = time () final_state , states_history = train_nn ( key ) end = time () print ( end - start ) 0.944758415222168 Friends, if you remember where we started in the tutorial.ipynb notebook, the original neural network took approximately a minute to train on a GPU (and longer if on a CPU). Let's now start by ploting the loss over training iterations. We start first with a function that returns the loss from a given state object. import matplotlib.pyplot as plt def calculate_loss ( state , get_params , model , lossfunc , x , y ): params = get_params ( state ) return lossfunc ( params , model , x , y ) calculate_loss ( final_state , get_params , nn_apply , logistic_loss , X . values , y . values ) DeviceArray(101.23043, dtype=float32) Now, we need to vmap it over all states in the states history, to get back the loss score. calc_loss_vmap = partial ( calculate_loss , get_params = get_params , model = nn_apply , lossfunc = logistic_loss , x = X . values , y = y . values ) start = time () losses = vmap ( calc_loss_vmap )( states_history ) end = time () print ( end - start ) plt . plot ( losses ) 1.5747764110565186 [<matplotlib.lines.Line2D at 0x7f5d800e06d0>]","title":"Progressively construct our training functions"},{"location":"03-stax/02-neural/#training-with-multiple-starting-points","text":"Just as above, we can also train the neural network with multiple starting points, again by vmap -ing our training function across split PRNGKeys. keys = random . split ( key , 5 ) start = time () final_states , state_histories = vmap ( train_nn )( keys ) end = time () print ( end - start ) 3.3660812377929688 get_params ( final_states )[ 0 ][ 0 ] . shape (5, 41, 20) Let's plot the losses over each of the state histories. Our last function calc_loss_vmap calculates loss score for one time point, which we then vmap over a single states_history , so we need another function that encapsulates this behaviour and vmap s over all state histories. def state_history_loss ( state_history ): losses = vmap ( calc_loss_vmap )( state_history ) return losses losses = vmap ( state_history_loss )( state_histories ) losses . shape (5, 3000) losses DeviceArray([[ 892.20435 , 885.81555 , 879.50555 , ..., 106.13542 , 106.08061 , 106.02581 ], [ 747.18805 , 740.2636 , 733.4714 , ..., 87.10681 , 87.06522 , 87.02366 ], [ 808.54504 , 802.43036 , 796.3972 , ..., 87.570694, 87.52497 , 87.47929 ], [1273.8547 , 1264.2145 , 1254.6217 , ..., 140.18053 , 140.12332 , 140.06607 ], [ 720.6325 , 716.0028 , 711.449 , ..., 76.929665, 76.8839 , 76.83814 ]], dtype=float32) Correctly-shaped! And now plotting it... plt . plot ( losses . T ) [<matplotlib.lines.Line2D at 0x7f5d706d8bd0>, <matplotlib.lines.Line2D at 0x7f5d70617a50>, <matplotlib.lines.Line2D at 0x7f5d7063cc10>, <matplotlib.lines.Line2D at 0x7f5d7063c190>, <matplotlib.lines.Line2D at 0x7f5d7063ca10>] Now that's pretty cool! We were able to see the loss from three independent runs. With sufficient memory, one would be able to do more runs; when I was writing this notebook early on, I saw that it was getting difficult to do on the order of tens of runs due to memory allocation issues.","title":"Training with multiple starting points"},{"location":"03-stax/02-neural/#summary","text":"In this notebook, we saw a few things in action. Firstly, we saw how to use the stax module on a linear model. Anytime we have a new framework for doing differential programming, it's super important to be able to explore it in the context of a linear model, which is basically the foundation of all deep learning. Secondly, we also explored how to leverage the JAX idioms to create fast parallelized training loops. We mixed-and-matched together jit , vmap , lax.scan , and grad into a performant training loop that was minimally nested. A corollary of this programming style is that every piece of the code can, in principle, be properly tested , because they are properly isolated. Have you written training loops where you modify a little piece here and a little piece there, until you lost what your original working one looked like? With training functions that are minimally nested, we can control the behaviour explicitly using closures/partials easily. Even when doing experimenation, our code can run reliably and fast. Thirdly, we saw how to apply the same lessons to training a neural network really fast with multiple starting points. The essence of the solution was to properly structure our program in progressively higher level layers of abstraction. We carefully wrote the program to go from the inner most layer out until we hit our goal of allowing for a set of multiple starts. The key here is that each level of abstraction is very natural, and corresponds to a \"unit computation\" being applied consistently across an \"array\" of things. Once we identify that \"unit computation\", writing the vmap -able or lax.scan -able function becomes very easy.","title":"Summary"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Gaussian mixture model-based clustering In this notebook, we are going to take a look at how to cluster Gaussian-distributed data. Imagine you have data that are multi-modal, something that looks like the following: import jax.numpy as np from jax import random import matplotlib.pyplot as plt weights_true = np . array ([ 1 , 5 ]) # 1:5 ratio locs_true = np . array ([ - 2. , 5. ]) # different means scale_true = np . array ([ 1.1 , 2 ]) # different variances base_n_draws = 1000 key = random . PRNGKey ( 100 ) k1 , k2 = random . split ( key ) draws_1 = scale_true [ 0 ] * random . normal ( k1 , shape = ( base_n_draws * weights_true [ 0 ],)) + locs_true [ 0 ] draws_2 = scale_true [ 1 ] * random . normal ( k2 , shape = ( base_n_draws * weights_true [ 1 ],)) + locs_true [ 1 ] data_mixture = np . concatenate ([ draws_1 , draws_2 ]) plt . hist ( data_mixture , bins = 40 ); WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) (array([ 2., 9., 32., 43., 76., 136., 132., 172., 146., 113., 81., 57., 46., 47., 71., 121., 132., 182., 264., 349., 349., 418., 440., 409., 416., 393., 348., 282., 232., 192., 117., 70., 47., 32., 23., 12., 4., 2., 0., 3.]), array([-5.2814174 , -4.8386564 , -4.395896 , -3.953135 , -3.5103743 , -3.0676136 , -2.6248527 , -2.182092 , -1.7393312 , -1.2965705 , -0.8538097 , -0.41104895, 0.03171182, 0.47447258, 0.91723335, 1.3599942 , 1.8027549 , 2.2455156 , 2.6882763 , 3.1310372 , 3.573798 , 4.0165586 , 4.4593196 , 4.90208 , 5.344841 , 5.787602 , 6.2303624 , 6.6731234 , 7.115884 , 7.558645 , 8.001406 , 8.444166 , 8.886927 , 9.329688 , 9.772449 , 10.215209 , 10.65797 , 11.100731 , 11.543491 , 11.986253 , 12.429013 ], dtype=float32), <BarContainer object of 40 artists>) Likelihoods of Mixture Data We might look at this data and say, \"I think there's two clusters of data here.\" One that belongs to the left mode, and one that belongs to the right mode. By visual inspection, the relative weighting might be about 1:3 to 1:6, or somewhere in between. What might be the \"data generating process\" here? Well, we could claim that when a data point is drawn from the mixture distribution, it could have come from either of the modes. By basic probability logic, the joint likelihood of observing the data point is: The likelihood that the datum came from the left Gaussian, times the probability of drawing a number from the left Gaussian, plus... The likelihood that the datum came from the right Gaussian, times the probability of drawing a number from the right Gaussian. Phrased more generally: The sum over \"components j j of the likelihood that the datum x_i x_i came from Gaussian j j with parameters \\mu_j, \\sigma_j \\mu_j, \\sigma_j times the likelihood of observing a draw from component j j .\" In math, we would need to calculate: \\sum_j P(x_i|\\mu_j, \\sigma_j) P(\\mu_j, \\sigma_j|w_j) P(w_j) \\sum_j P(x_i|\\mu_j, \\sigma_j) P(\\mu_j, \\sigma_j|w_j) P(w_j) Now, we can make the middle term P(\\mu_j, \\sigma_j|w_j) P(\\mu_j, \\sigma_j|w_j) is always 1, by assuming that the \\mu_j \\mu_j and \\sigma_j \\sigma_j chosen are always fixed given the component weight chosen. The expression then simplifies to: \\sum_j P(x_i|\\mu_j, \\sigma_j) P(w_j) \\sum_j P(x_i|\\mu_j, \\sigma_j) P(w_j) Log Likelihood of One Datum under One Component Because this is a summation, let's work out the elementary steps first. from dl_workshop.gaussian_mixture import loglike_one_component loglike_one_component ?? The summation here is because we are operating in logarithmic space. You might ask, why do we use \"log\" of the component scale? This is a math trick that helps us whenever we are doing computations in an unbounded space. When doing gradient descent, we can never guarantee that a gradient update on a parameter that ought to be positive-only will give us a positive number. Thus, for positive numbers, we operate in logarithmic space. We can quickly write a test here. If the component probability is 1.0, the component \\mu \\mu is 0, and the observed datum is also 0, it should equal to the log-likelihood of 0 under a unit Gaussian. from jax.scipy import stats our_test = loglike_one_component ( component_weight = 1.0 , component_mu = 0. , log_component_scale = np . log ( 1. ), datum = 0. ) ground_truth = ( stats . norm . logpdf ( x = 0 , loc = 0 , scale = 1 ) ) our_test , ground_truth (DeviceArray(-0.9189385, dtype=float32), DeviceArray(-0.9189385, dtype=float32)) Log Likelihood of One Datum under All Components Now that we are done with the elementary computation of one datum under one component, we can vmap the log-likelihood calculation over all components, thereby giving us the loglikelihood of a datum under any of the possible given components. Firstly, we need a function that normalizes component weights to sum to 1. This is enforced just in case during the gradient descent procedure, we end up with weights that do not sum to 1. from dl_workshop.gaussian_mixture import normalize_weights , loglike_across_components normalize_weights ?? Next, we leverage the normalize_weights function inside a loglike_across_components function, which vmap s the log likelihood calculation across components: loglike_across_components ?? Inside that function, we first calculated elementwise the log-likelihood of observing that data under each component. That only gives us per-component log-likelihoods though. Because our data could have been drawn from any of those components, the total likelihood is a sum of the per-component likelihoods. Thus, we have to elementwise exponentiate the log-likelihoods first. Because we have sum up each of those probability components together, a shortcut function we have access to is the logsumexp function, which first exponentiates each of the probabilities, sums them up, and then takes their log again, thereby accomplishing what we need. We could have written our own version of the function, but I think it makes a ton of sense to trust the numerically-stable, professionally-implemented version provided in SciPy! The choice to pass in log_component_weights rather than weights is because the normalize_weights function assumes that all numbers in the vector are positive, but in gradient descent, we operate in an unbounded space, which may bring us into negative numbers. To make things safe, we assume the numbers come to us from an unbounded space, and then use an exponential transform first before normalizing. Let us now test-drive our loglike_across_components function, which should give us a scalar value at the end. loglike_across_components ( log_component_weights = np . log ( weights_true ), component_mus = locs_true , log_component_scales = np . log ( scale_true ), datum = data_mixture [ 1 ], ) DeviceArray(-2.8022985, dtype=float32) Great, that worked! Log Likelihood of All Data under All Components Now that we've got the log-likelihood of each datum under each component, we can now vmap the function across all data given to us. Mathematically, this would be: \\prod_i \\sum_j P(x_i|\\mu_j, \\sigma_j) P(w_j) \\prod_i \\sum_j P(x_i|\\mu_j, \\sigma_j) P(w_j) Or in prose: The total likelihood of all datum x_i x_i together under all components j j is given by first summing the likelihoods of each datum x_i x_i under each component j j , and then taking the product of likelihoods for each data point x_i x_i , assuming data are i.i.d. from the mixture distribution. from dl_workshop.gaussian_mixture import mixture_loglike mixture_loglike ?? Notice how we vmap -ed the loglike_across_components function over all data points provided in the function above. This helped us eliminate a for-loop, basically! If we execute the function, we should get a scalar value. mixture_loglike ( log_component_weights = np . log ( weights_true ), component_mus = locs_true , log_component_scales = np . log ( scale_true ), data = data_mixture , ) DeviceArray(-14590.395, dtype=float32) Log Likelihood of Weighting The final thing we are missing is a generative story for the weights. In other words, we are asking the question, \"How did the weights come about?\" We might say that the weights were drawn from a Dirichlet distribution (the generalization of a Beta distribution to multiple dimensions), and as a na\u00efve first pass, were drawn with equal probability. from dl_workshop.gaussian_mixture import weights_loglike weights_loglike ?? alpha_prior = 2 * np . ones_like ( weights_true ) weights_loglike ( np . log ( weights_true ), alpha_prior = alpha_prior ) DeviceArray(-0.18232119, dtype=float32) Review thus far Now that we have composed together our generative story for the data, let's pause for a moment and break down our model a bit. This will serve as a review of what we've done. Firstly, we have our \"model\", i.e. the log-likelihood of our data conditioned on some parameter set and their values. Secondly, our parameters of the model are: Component weights. Component central tendencies/means Component scales/variances. What we're going to attempt next is to use gradient based optimization to learn what those parameters are, conditioned on data, leveraging the JAX idioms that we've learned before. Gradient descent to find maximum likelihood values Given a mixture Gaussian dataset, one natural task we might want to do is estimate the weights, central tendencies/means and scales/variances from data. This corresponds naturally to a maximum likelihood estimation task. Now, one thing we know is that JAX's optimizers assume we are minimizing a function, so to use JAX's optimizers with a maximum likelihood function, we simply take the negative of the log likelihood and minimize that. Loss function Let's first take a look at the loss function. from dl_workshop.gaussian_mixture import loss_mixture_weights loss_mixture_weights ?? As you can see, our function is designed to be compatible with JAX's grad . We are taking derivatives w.r.t. the first argument, the parameters, which we unpack into our likelihood function parameters. The two likelihood functions are used inside there too: mixture_loglike weights_loglike The alpha_prior is hard-coded; it's not the most ideal. For convenience, I have just hard-coded it, but the principled way to handle this is to add it as a keyword argument that gets passed in. Gradient of loss function As usual, we now define the gradient function of loss_mixture_weights by calling grad on it: from jax import grad dloss_mixture_weights = grad ( loss_mixture_weights ) Parameter Initialization Next up, we initialize our parameters randomly. For convenience, we'll use Gaussian draws. N_MIXTURE_COMPONENTS = 2 k1 , k2 , k3 , k4 = random . split ( key , 4 ) log_component_weights_init = random . normal ( k1 , shape = ( N_MIXTURE_COMPONENTS ,)) component_mus_init = random . normal ( k2 , shape = ( N_MIXTURE_COMPONENTS ,)) log_component_scales_init = random . normal ( k3 , shape = ( N_MIXTURE_COMPONENTS ,)) params_init = log_component_weights_init , component_mus_init , log_component_scales_init params_true = np . log ( weights_true ), locs_true , np . log ( scale_true ) Here, you see JAX's controllable handling of random numbers. Our parameters are always going to be initialized in exactly the same way on each notebook cell re-run, since we have explicit keys passed in. Test-drive functions Let's test-drive the functions to make sure that they work correctly. For the loss function, we should expect to get back a scalar. If we pass in initialized parameters, it should also have a higher value (corresponding to more lower log likelihood) than if we pass in true parameters. loss_mixture_weights ( params_true , data_mixture ) DeviceArray(14590.577, dtype=float32) loss_mixture_weights ( params_init , data_mixture ) DeviceArray(36641.8, dtype=float32) Indeed, both criteria are satisfied. Test-driving the gradient function should give us a tuple of gradients evaluated. dloss_mixture_weights ( params_init , data_mixture ) (DeviceArray([-1778.4056, 1778.4058], dtype=float32), DeviceArray([-7817.163 , -210.50105], dtype=float32), DeviceArray([-45475.797 , -944.6539], dtype=float32)) Defining performant training loops Now, we are going to use JAX's optimizers inside a lax.scan -ed training loop to get fast training going. We begin with the elementary \"step\" function. from dl_workshop.gaussian_mixture import step step ?? This should look familiar to you. At each step of the loop, we unpack params from a JAX optimizer state, obtain gradients, and then update the state using the gradients. We then make the elementary step function a scannable one using lax.scan . This will allow us to \"scan\" the function across an array that represents the number of optimization steps we will be using. from dl_workshop.gaussian_mixture import make_step_scannable make_step_scannable ?? Recall that the inner function that gets returned here has the API that we require for using lax.scan : previous_state corresponds to the carry , and iteration corresponds to the x . Now we actually instantiate the scannable step. from jax.experimental.optimizers import adam adam_init , adam_update , adam_get_params = adam ( 0.5 ) step_scannable = make_step_scannable ( get_params_func = adam_get_params , dloss_func = dloss_mixture_weights , update_func = adam_update , data = data_mixture , ) Then, we lax.scan step_scannable over 1000 iterations (constructed as an np.arange() array). from jax import lax initial_state = adam_init ( params_init ) final_state , state_history = lax . scan ( step_scannable , initial_state , np . arange ( 1000 )) Sanity-checking whether learning has happened We can sanity check whether learning has happened. The loss function value for optimized parameters should be pretty close to the loss function when we put in true params. (Do keep in mind that because we have data that are an imperfect sample of the ground truth distribution, it is possible that our optimized params' negative log likelihood will be different than that of the true params.) Firstly, we unpack the parameters of the final state: params_opt = adam_get_params ( final_state ) log_component_weights_opt , component_mus_opt , log_component_scales_opt = params_opt Then, we look at the loss for the optimized params: loss_mixture_weights ( params_opt , data_mixture ) DeviceArray(14590.362, dtype=float32) It should be lower than the loss for the initialized params loss_mixture_weights ( params_init , data_mixture ) DeviceArray(36641.8, dtype=float32) Indeed that is so! And if we inspect the component weights: np . exp ( log_component_weights_opt ), weights_true (DeviceArray([0.4388248, 2.2122066], dtype=float32), DeviceArray([1, 5], dtype=int32)) Indeed, we have optimized our parameters such that they are close to the original 1:5 ratio! And for our component means? component_mus_opt , locs_true (DeviceArray([-2.024643 , 4.9976335], dtype=float32), DeviceArray([-2., 5.], dtype=float32)) Really close too! Finally, for the component scales: np . exp ( log_component_scales_opt ), scale_true (DeviceArray([1.0888491, 1.998768 ], dtype=float32), DeviceArray([1.1, 2. ], dtype=float32)) Very nice, really close to the ground truth too. Visualizing training dynamics Let's now visualize how training went. I have created a function called animate_training , which will provide for us a visual representation. animate_training ?? Object `animate_training` not found. animate_training leverages celluloid to make easy matplotlib animations. You can check out the package here . We can now call on animate_training to give us an animation of the mixture Gaussian PDFs as we trained the model. %% capture from dl_workshop.gaussian_mixture import animate_training params_history = adam_get_params ( state_history ) animation = animate_training ( params_history , 10 , data_mixture ) from IPython.display import HTML HTML ( animation . to_html5_video ()) Your browser does not support the video tag. There's some comments to be said on the dynamics here: At first, one Gaussian is used to approximate over the entire distribution. It's not a good fit, but approximates it fine enough. However, our optimization routine continues to push forward, eventually finding the bimodal pattern. Once this happens, the PDFs fit very nicely to the data samples. This phenomena is also reflected in the loss: from dl_workshop.gaussian_mixture import get_loss get_loss ?? Because states_history is the result of lax.scan -ing, we can vmap our get_loss function over the states_history object to get back an array of losses that can then be plotted: from jax import vmap from functools import partial losses = vmap ( partial ( get_loss , get_params_func = adam_get_params , loss_func = loss_mixture_weights , data = data_mixture ))( state_history ) plt . plot ( losses ) plt . yscale ( \"log\" ); You should notice the first plateau, followed by the second plateau. This corresponds to the two phases of learning. Now, thus far, we have set up the problem in a fashion that is essentially \"trivial\". What if, however, we wanted to try fitting a mixture Gaussian where we didn't know exactly how many mixture components there ought to be? To check that out, head over to the next section in this chapter.","title":"Introduction"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/#gaussian-mixture-model-based-clustering","text":"In this notebook, we are going to take a look at how to cluster Gaussian-distributed data. Imagine you have data that are multi-modal, something that looks like the following: import jax.numpy as np from jax import random import matplotlib.pyplot as plt weights_true = np . array ([ 1 , 5 ]) # 1:5 ratio locs_true = np . array ([ - 2. , 5. ]) # different means scale_true = np . array ([ 1.1 , 2 ]) # different variances base_n_draws = 1000 key = random . PRNGKey ( 100 ) k1 , k2 = random . split ( key ) draws_1 = scale_true [ 0 ] * random . normal ( k1 , shape = ( base_n_draws * weights_true [ 0 ],)) + locs_true [ 0 ] draws_2 = scale_true [ 1 ] * random . normal ( k2 , shape = ( base_n_draws * weights_true [ 1 ],)) + locs_true [ 1 ] data_mixture = np . concatenate ([ draws_1 , draws_2 ]) plt . hist ( data_mixture , bins = 40 ); WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) (array([ 2., 9., 32., 43., 76., 136., 132., 172., 146., 113., 81., 57., 46., 47., 71., 121., 132., 182., 264., 349., 349., 418., 440., 409., 416., 393., 348., 282., 232., 192., 117., 70., 47., 32., 23., 12., 4., 2., 0., 3.]), array([-5.2814174 , -4.8386564 , -4.395896 , -3.953135 , -3.5103743 , -3.0676136 , -2.6248527 , -2.182092 , -1.7393312 , -1.2965705 , -0.8538097 , -0.41104895, 0.03171182, 0.47447258, 0.91723335, 1.3599942 , 1.8027549 , 2.2455156 , 2.6882763 , 3.1310372 , 3.573798 , 4.0165586 , 4.4593196 , 4.90208 , 5.344841 , 5.787602 , 6.2303624 , 6.6731234 , 7.115884 , 7.558645 , 8.001406 , 8.444166 , 8.886927 , 9.329688 , 9.772449 , 10.215209 , 10.65797 , 11.100731 , 11.543491 , 11.986253 , 12.429013 ], dtype=float32), <BarContainer object of 40 artists>)","title":"Gaussian mixture model-based clustering"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/#likelihoods-of-mixture-data","text":"We might look at this data and say, \"I think there's two clusters of data here.\" One that belongs to the left mode, and one that belongs to the right mode. By visual inspection, the relative weighting might be about 1:3 to 1:6, or somewhere in between. What might be the \"data generating process\" here? Well, we could claim that when a data point is drawn from the mixture distribution, it could have come from either of the modes. By basic probability logic, the joint likelihood of observing the data point is: The likelihood that the datum came from the left Gaussian, times the probability of drawing a number from the left Gaussian, plus... The likelihood that the datum came from the right Gaussian, times the probability of drawing a number from the right Gaussian. Phrased more generally: The sum over \"components j j of the likelihood that the datum x_i x_i came from Gaussian j j with parameters \\mu_j, \\sigma_j \\mu_j, \\sigma_j times the likelihood of observing a draw from component j j .\" In math, we would need to calculate: \\sum_j P(x_i|\\mu_j, \\sigma_j) P(\\mu_j, \\sigma_j|w_j) P(w_j) \\sum_j P(x_i|\\mu_j, \\sigma_j) P(\\mu_j, \\sigma_j|w_j) P(w_j) Now, we can make the middle term P(\\mu_j, \\sigma_j|w_j) P(\\mu_j, \\sigma_j|w_j) is always 1, by assuming that the \\mu_j \\mu_j and \\sigma_j \\sigma_j chosen are always fixed given the component weight chosen. The expression then simplifies to: \\sum_j P(x_i|\\mu_j, \\sigma_j) P(w_j) \\sum_j P(x_i|\\mu_j, \\sigma_j) P(w_j)","title":"Likelihoods of Mixture Data"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/#log-likelihood-of-one-datum-under-one-component","text":"Because this is a summation, let's work out the elementary steps first. from dl_workshop.gaussian_mixture import loglike_one_component loglike_one_component ?? The summation here is because we are operating in logarithmic space. You might ask, why do we use \"log\" of the component scale? This is a math trick that helps us whenever we are doing computations in an unbounded space. When doing gradient descent, we can never guarantee that a gradient update on a parameter that ought to be positive-only will give us a positive number. Thus, for positive numbers, we operate in logarithmic space. We can quickly write a test here. If the component probability is 1.0, the component \\mu \\mu is 0, and the observed datum is also 0, it should equal to the log-likelihood of 0 under a unit Gaussian. from jax.scipy import stats our_test = loglike_one_component ( component_weight = 1.0 , component_mu = 0. , log_component_scale = np . log ( 1. ), datum = 0. ) ground_truth = ( stats . norm . logpdf ( x = 0 , loc = 0 , scale = 1 ) ) our_test , ground_truth (DeviceArray(-0.9189385, dtype=float32), DeviceArray(-0.9189385, dtype=float32))","title":"Log Likelihood of One Datum under One Component"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/#log-likelihood-of-one-datum-under-all-components","text":"Now that we are done with the elementary computation of one datum under one component, we can vmap the log-likelihood calculation over all components, thereby giving us the loglikelihood of a datum under any of the possible given components. Firstly, we need a function that normalizes component weights to sum to 1. This is enforced just in case during the gradient descent procedure, we end up with weights that do not sum to 1. from dl_workshop.gaussian_mixture import normalize_weights , loglike_across_components normalize_weights ?? Next, we leverage the normalize_weights function inside a loglike_across_components function, which vmap s the log likelihood calculation across components: loglike_across_components ?? Inside that function, we first calculated elementwise the log-likelihood of observing that data under each component. That only gives us per-component log-likelihoods though. Because our data could have been drawn from any of those components, the total likelihood is a sum of the per-component likelihoods. Thus, we have to elementwise exponentiate the log-likelihoods first. Because we have sum up each of those probability components together, a shortcut function we have access to is the logsumexp function, which first exponentiates each of the probabilities, sums them up, and then takes their log again, thereby accomplishing what we need. We could have written our own version of the function, but I think it makes a ton of sense to trust the numerically-stable, professionally-implemented version provided in SciPy! The choice to pass in log_component_weights rather than weights is because the normalize_weights function assumes that all numbers in the vector are positive, but in gradient descent, we operate in an unbounded space, which may bring us into negative numbers. To make things safe, we assume the numbers come to us from an unbounded space, and then use an exponential transform first before normalizing. Let us now test-drive our loglike_across_components function, which should give us a scalar value at the end. loglike_across_components ( log_component_weights = np . log ( weights_true ), component_mus = locs_true , log_component_scales = np . log ( scale_true ), datum = data_mixture [ 1 ], ) DeviceArray(-2.8022985, dtype=float32) Great, that worked!","title":"Log Likelihood of One Datum under All Components"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/#log-likelihood-of-all-data-under-all-components","text":"Now that we've got the log-likelihood of each datum under each component, we can now vmap the function across all data given to us. Mathematically, this would be: \\prod_i \\sum_j P(x_i|\\mu_j, \\sigma_j) P(w_j) \\prod_i \\sum_j P(x_i|\\mu_j, \\sigma_j) P(w_j) Or in prose: The total likelihood of all datum x_i x_i together under all components j j is given by first summing the likelihoods of each datum x_i x_i under each component j j , and then taking the product of likelihoods for each data point x_i x_i , assuming data are i.i.d. from the mixture distribution. from dl_workshop.gaussian_mixture import mixture_loglike mixture_loglike ?? Notice how we vmap -ed the loglike_across_components function over all data points provided in the function above. This helped us eliminate a for-loop, basically! If we execute the function, we should get a scalar value. mixture_loglike ( log_component_weights = np . log ( weights_true ), component_mus = locs_true , log_component_scales = np . log ( scale_true ), data = data_mixture , ) DeviceArray(-14590.395, dtype=float32)","title":"Log Likelihood of All Data under All Components"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/#log-likelihood-of-weighting","text":"The final thing we are missing is a generative story for the weights. In other words, we are asking the question, \"How did the weights come about?\" We might say that the weights were drawn from a Dirichlet distribution (the generalization of a Beta distribution to multiple dimensions), and as a na\u00efve first pass, were drawn with equal probability. from dl_workshop.gaussian_mixture import weights_loglike weights_loglike ?? alpha_prior = 2 * np . ones_like ( weights_true ) weights_loglike ( np . log ( weights_true ), alpha_prior = alpha_prior ) DeviceArray(-0.18232119, dtype=float32)","title":"Log Likelihood of Weighting"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/#review-thus-far","text":"Now that we have composed together our generative story for the data, let's pause for a moment and break down our model a bit. This will serve as a review of what we've done. Firstly, we have our \"model\", i.e. the log-likelihood of our data conditioned on some parameter set and their values. Secondly, our parameters of the model are: Component weights. Component central tendencies/means Component scales/variances. What we're going to attempt next is to use gradient based optimization to learn what those parameters are, conditioned on data, leveraging the JAX idioms that we've learned before.","title":"Review thus far"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/#gradient-descent-to-find-maximum-likelihood-values","text":"Given a mixture Gaussian dataset, one natural task we might want to do is estimate the weights, central tendencies/means and scales/variances from data. This corresponds naturally to a maximum likelihood estimation task. Now, one thing we know is that JAX's optimizers assume we are minimizing a function, so to use JAX's optimizers with a maximum likelihood function, we simply take the negative of the log likelihood and minimize that.","title":"Gradient descent to find maximum likelihood values"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/#loss-function","text":"Let's first take a look at the loss function. from dl_workshop.gaussian_mixture import loss_mixture_weights loss_mixture_weights ?? As you can see, our function is designed to be compatible with JAX's grad . We are taking derivatives w.r.t. the first argument, the parameters, which we unpack into our likelihood function parameters. The two likelihood functions are used inside there too: mixture_loglike weights_loglike The alpha_prior is hard-coded; it's not the most ideal. For convenience, I have just hard-coded it, but the principled way to handle this is to add it as a keyword argument that gets passed in.","title":"Loss function"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/#gradient-of-loss-function","text":"As usual, we now define the gradient function of loss_mixture_weights by calling grad on it: from jax import grad dloss_mixture_weights = grad ( loss_mixture_weights )","title":"Gradient of loss function"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/#parameter-initialization","text":"Next up, we initialize our parameters randomly. For convenience, we'll use Gaussian draws. N_MIXTURE_COMPONENTS = 2 k1 , k2 , k3 , k4 = random . split ( key , 4 ) log_component_weights_init = random . normal ( k1 , shape = ( N_MIXTURE_COMPONENTS ,)) component_mus_init = random . normal ( k2 , shape = ( N_MIXTURE_COMPONENTS ,)) log_component_scales_init = random . normal ( k3 , shape = ( N_MIXTURE_COMPONENTS ,)) params_init = log_component_weights_init , component_mus_init , log_component_scales_init params_true = np . log ( weights_true ), locs_true , np . log ( scale_true ) Here, you see JAX's controllable handling of random numbers. Our parameters are always going to be initialized in exactly the same way on each notebook cell re-run, since we have explicit keys passed in.","title":"Parameter Initialization"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/#test-drive-functions","text":"Let's test-drive the functions to make sure that they work correctly. For the loss function, we should expect to get back a scalar. If we pass in initialized parameters, it should also have a higher value (corresponding to more lower log likelihood) than if we pass in true parameters. loss_mixture_weights ( params_true , data_mixture ) DeviceArray(14590.577, dtype=float32) loss_mixture_weights ( params_init , data_mixture ) DeviceArray(36641.8, dtype=float32) Indeed, both criteria are satisfied. Test-driving the gradient function should give us a tuple of gradients evaluated. dloss_mixture_weights ( params_init , data_mixture ) (DeviceArray([-1778.4056, 1778.4058], dtype=float32), DeviceArray([-7817.163 , -210.50105], dtype=float32), DeviceArray([-45475.797 , -944.6539], dtype=float32))","title":"Test-drive functions"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/#defining-performant-training-loops","text":"Now, we are going to use JAX's optimizers inside a lax.scan -ed training loop to get fast training going. We begin with the elementary \"step\" function. from dl_workshop.gaussian_mixture import step step ?? This should look familiar to you. At each step of the loop, we unpack params from a JAX optimizer state, obtain gradients, and then update the state using the gradients. We then make the elementary step function a scannable one using lax.scan . This will allow us to \"scan\" the function across an array that represents the number of optimization steps we will be using. from dl_workshop.gaussian_mixture import make_step_scannable make_step_scannable ?? Recall that the inner function that gets returned here has the API that we require for using lax.scan : previous_state corresponds to the carry , and iteration corresponds to the x . Now we actually instantiate the scannable step. from jax.experimental.optimizers import adam adam_init , adam_update , adam_get_params = adam ( 0.5 ) step_scannable = make_step_scannable ( get_params_func = adam_get_params , dloss_func = dloss_mixture_weights , update_func = adam_update , data = data_mixture , ) Then, we lax.scan step_scannable over 1000 iterations (constructed as an np.arange() array). from jax import lax initial_state = adam_init ( params_init ) final_state , state_history = lax . scan ( step_scannable , initial_state , np . arange ( 1000 ))","title":"Defining performant training loops"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/#sanity-checking-whether-learning-has-happened","text":"We can sanity check whether learning has happened. The loss function value for optimized parameters should be pretty close to the loss function when we put in true params. (Do keep in mind that because we have data that are an imperfect sample of the ground truth distribution, it is possible that our optimized params' negative log likelihood will be different than that of the true params.) Firstly, we unpack the parameters of the final state: params_opt = adam_get_params ( final_state ) log_component_weights_opt , component_mus_opt , log_component_scales_opt = params_opt Then, we look at the loss for the optimized params: loss_mixture_weights ( params_opt , data_mixture ) DeviceArray(14590.362, dtype=float32) It should be lower than the loss for the initialized params loss_mixture_weights ( params_init , data_mixture ) DeviceArray(36641.8, dtype=float32) Indeed that is so! And if we inspect the component weights: np . exp ( log_component_weights_opt ), weights_true (DeviceArray([0.4388248, 2.2122066], dtype=float32), DeviceArray([1, 5], dtype=int32)) Indeed, we have optimized our parameters such that they are close to the original 1:5 ratio! And for our component means? component_mus_opt , locs_true (DeviceArray([-2.024643 , 4.9976335], dtype=float32), DeviceArray([-2., 5.], dtype=float32)) Really close too! Finally, for the component scales: np . exp ( log_component_scales_opt ), scale_true (DeviceArray([1.0888491, 1.998768 ], dtype=float32), DeviceArray([1.1, 2. ], dtype=float32)) Very nice, really close to the ground truth too.","title":"Sanity-checking whether learning has happened"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/#visualizing-training-dynamics","text":"Let's now visualize how training went. I have created a function called animate_training , which will provide for us a visual representation. animate_training ?? Object `animate_training` not found. animate_training leverages celluloid to make easy matplotlib animations. You can check out the package here . We can now call on animate_training to give us an animation of the mixture Gaussian PDFs as we trained the model. %% capture from dl_workshop.gaussian_mixture import animate_training params_history = adam_get_params ( state_history ) animation = animate_training ( params_history , 10 , data_mixture ) from IPython.display import HTML HTML ( animation . to_html5_video ()) Your browser does not support the video tag. There's some comments to be said on the dynamics here: At first, one Gaussian is used to approximate over the entire distribution. It's not a good fit, but approximates it fine enough. However, our optimization routine continues to push forward, eventually finding the bimodal pattern. Once this happens, the PDFs fit very nicely to the data samples. This phenomena is also reflected in the loss: from dl_workshop.gaussian_mixture import get_loss get_loss ?? Because states_history is the result of lax.scan -ing, we can vmap our get_loss function over the states_history object to get back an array of losses that can then be plotted: from jax import vmap from functools import partial losses = vmap ( partial ( get_loss , get_params_func = adam_get_params , loss_func = loss_mixture_weights , data = data_mixture ))( state_history ) plt . plot ( losses ) plt . yscale ( \"log\" ); You should notice the first plateau, followed by the second plateau. This corresponds to the two phases of learning. Now, thus far, we have set up the problem in a fashion that is essentially \"trivial\". What if, however, we wanted to try fitting a mixture Gaussian where we didn't know exactly how many mixture components there ought to be? To check that out, head over to the next section in this chapter.","title":"Visualizing training dynamics"},{"location":"04-gaussian-clustering/02-dirichlet-processes/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Dirichlet Processes: A simulated guide Introduction In the previous section, we saw how we could fit a two-component Gaussian mixture model to data that looked like it had just two components. In many real-world settings, though, we oftentimes do not know exactly how many components are present, so one way we can approach the problem is to assume that there are an infinite (or \"countably large\") number of components available for our model to pick from, but we \"guide\" our model to focus its attention on only a small number of components provided. Does that sound magical? It sure did for me when I first heard about this possibility. The key modelling component that we need is a process for creating infinite numbers of mixture weight components from a single controllable parameter, and that naturally gives us a Dirichlet process , which we will look at in this section. What are Dirichlet processes? To quote from Wikipedia's article on DPs : In probability theory, Dirichlet processes (after Peter Gustav Lejeune Dirichlet) are a family of stochastic processes whose realizations are probability distributions. Hmm, now that doesn't look very concrete. Is there a more concrete way to think about DPs? Turns out, the answer is yes! At its core, each realization/draw from a DP provides an infinite (or, in computing world, a \"large\") set of weights that sum to 1. Remember that: A long vector of numbers that sum to 1, which we can interpret as a probability distribution over sets of weights. Simulating a Dirichlet Process using \"stick-breaking\" We're going to look at one way to construct a probability vector, the \"stick-breaking\" process. How does it work? At its core, it looks like this, a very simple idea. We take a length 1 stick, draw a probability value from a Beta distribution, break the length 1 stick into two at the point drawn, and record the left side's value. We then take the right side, draw another probability value from a Beta distribution again, break that stick proportionally into two portions at the point drawn, and record the absolute length of the left side's value We then braek the right side again, using the same process. We repeat this until we have the countably large number of states that we desire. In code, this looks like a loop with a carryover from the previous iteration, which means it is a lax.scan -able function! from dl_workshop.gaussian_mixture import stick_breaking_weights stick_breaking_weights ?? As you can see, in the inner function weighting , we first calculate the weight associated with the \"left side\" of the stick, which we record down and accumulate as the \"history\" (second tuple element of the return). Our carry is the occupied_probability + weight , which we can use to calculate the length of the right side of the stick ( 1 - occupied_probability ). Because each beta_i is an i.i.d. draw from beta_draws , we can pre-instantiate a vector of beta_draws and then lax.scan the weighting function over the vector. Beta distribution crash-course Because on computers it's hard to deal with infinitely-long arrays, we can instead instantiate a \"countably large\" array of beta_draws . Now, the beta_draws , need to be i.i.d. from a source Beta distribution, which has two parameters, a and b , and gives us a continuous distribution over the interval (0, 1) (0, 1) . Because of the nature of a and b corresponding to success and failure weights: higher a at constant b shifts the distribution closer to 1, higher b at constant a shifts the distribution closer to 0, higher magnitudes of a and b narrow the distribution width. Visualizing stick-breaking For our purposes, we are going to hold a constant at 1.0 while varying b . We'll then see how our weight vectors are generated as a function of b . As you will see, b becomes a \"concentration\" parameter, which governs how \"concentrated\" our probability mass is allocated. Let's see how one draw from a Dirichlet process looks like. def dp_draw ( key , concentration , vector_length ): beta_draws = random . beta ( key = key , a = 1 , b = concentration , shape = ( vector_length ,)) occupied_probability , weights = stick_breaking_weights ( beta_draws ) return occupied_probability , weights from jax import random import matplotlib.pyplot as plt key = random . PRNGKey ( 42 ) occupied_probability , weights = dp_draw ( key , 3 , 50 ) plt . plot ( weights ) plt . xlabel ( \"Vector slot\" ) plt . ylabel ( \"Probability\" ); WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) Text(0, 0.5, 'Probability') Now, what if we took 20 draws from the Dirichlet process? To do so, we can vmap dp_draw over split PRNGKey s. from jax import vmap from functools import partial import seaborn as sns keys = random . split ( key , 20 ) occupied_probabilities , weights_draws = vmap ( partial ( dp_draw , concentration = 3 , vector_length = 50 ))( keys ) sns . heatmap ( weights_draws ); <AxesSubplot:> Effect of concentration on Dirichlet weights draws As is visible here, when concentration = 3 , most of our probability mass is concentrated across roughly the first 5-8 states. What happens if we varied the concentration? How does that parameter affect the distribution of weights? import jax.numpy as np concentrations = np . array ([ 0.5 , 1 , 3 , 5 , 10 , 20 ]) def dirichlet_one_concentration ( key , concentration , num_draws ): keys = random . split ( key , num_draws ) occupied_probabilities , weights_draws = vmap ( partial ( dp_draw , concentration = concentration , vector_length = 50 ))( keys ) return occupied_probabilities , weights_draws keys = random . split ( key , len ( concentrations )) occupied_probabilities , weights_draws = vmap ( partial ( dirichlet_one_concentration , num_draws = 20 ))( keys , concentrations ) weights_draws . shape (6, 20, 50) fig , axes = plt . subplots ( nrows = 2 , ncols = 3 , figsize = ( 3 * 3 , 3 * 2 ), sharex = True , sharey = True ) for ax , weights_mat , conc in zip ( axes . flatten (), weights_draws , concentrations ): sns . heatmap ( weights_mat , ax = ax ) ax . set_title ( f \"Concentration = { conc } \" ) ax . set_xlabel ( \"Component\" ) ax . set_ylabel ( \"Draw\" ) plt . tight_layout () As we increase the concentration value, the probabilities get more diffuse. This is evident from the above heatmaps in the following ways. Over each draw, as we increase the value of the concentration parameter, the probability mass allocated to the components that have significant probability mass decreases. Additionally, more components have \"significant\" amounts of probability mass allocated. Running stick-breaking backwards From this forward process of generating Dirichlet-distributed weights, instead of evaluating the log likelihood of the component weights under a \"fixed\" Dirichlet distribution prior, we can instead evaluate it under a Dirichlet process with a \"concentration\" prior. The requirement here is that we be able to recover correctly the i.i.d. Beta draws that generated the Dirichlet process weights. Let's try that out. from dl_workshop.gaussian_mixture import beta_draw_from_weights beta_draw_from_weights ?? We essentially run the process backwards, taking advantage of the fact that we know the first weight exactly. Let's try to see how well we can recover the weights. concentration = 3 beta_draws = random . beta ( key = key , a = 1 , b = concentration , shape = ( 50 ,)) occupied_probability , weights = stick_breaking_weights ( beta_draws ) final , beta_hat = beta_draw_from_weights ( weights ) plt . plot ( beta_draws , label = \"original\" ) plt . plot ( beta_hat , label = \"inferred\" ) plt . legend () plt . xlabel ( \"Component\" ) plt . ylabel ( \"Beta Draw\" ); Text(0, 0.5, 'Beta Draw') As is visible from the plot above, we were able to recover about 1/2 to 2/3 of the weights before the divergence in the two curves shows up. One of the difficulties that we have is that when we get back the observed weights in real life, we have no access to how much of the length 1 \"stick\" is leftover. This, alongside numerical underflow issues arising from small numbers, means we can only use about 1/2 of the drawn weights to recover the Beta-distributed draws from which we can evaluate our log likelihoods. Evaluating log-likelihood of recovered Beta-distributed weights So putting things all together, we can take a weights vector, run the stick-breaking process backwards (up to a certain point) to recover Beta-distributed draws that would have generated the weights vector, and then evaluate the log-likelihood of the Beta-disributed draws under a Beta distribution. Let's see that in action: from dl_workshop.gaussian_mixture import component_probs_loglike component_probs_loglike ?? And evaluating our draws should give us a scalar likelihood: component_probs_loglike ( np . log ( weights ), log_concentration = 1.0 , num_components = 25 ) DeviceArray(11.000253, dtype=float32) Log likelihood as a function of concentration Once again, let's build up our understanding by seeing how the log likelihood of our weights under an assumed Dirichlet process from a Beta distribution changes as we vary the concentration parameter. log_concentration = np . linspace ( - 3 , 3 , 1000 ) def make_vmappable_loglike ( log_component_probs , num_components ): def inner ( log_concentration ): return component_probs_loglike ( log_component_probs , log_concentration , num_components ) return inner component_probs_loglike_vmappable = make_vmappable_loglike ( log_component_probs = np . log ( weights ), num_components = 25 ) lls = vmap ( component_probs_loglike_vmappable )( log_concentration ) plt . plot ( log_concentration , lls ) plt . xlabel ( \"Concentration\" ) plt . ylabel ( \"Log likelihood\" ); Text(0, 0.5, 'Log likelihood') As you can see above, we first constructed the vmappable log-likelihood function using a closure. The shape of the curve tells us that it is an optimizable problem with one optimal point, at least within bounds of possible concentrations that we're interested in. Optimizing the log-likelihood Once again, we're going to see how we can use gradient-based optimization to see how we can identify the most likely concentration value that generated a Dirichlet process weights vector. Define loss function As always, we start with the loss function definition. Because our component_probs_loglike function operates only on a single draw, we need a function that will allow us to operate on multiple draws. We can do this by using a closure. from jax import grad def make_loss_dp ( num_components ): def loss_dp ( log_concentration , log_component_probs ): \"\"\"Log-likelihood of component_probabilities of dirichlet process. :param log_concentration: Scalar value. :param log_component_probs: One or more component probability vectors. \"\"\" vm_func = partial ( component_probs_loglike , log_concentration = log_concentration , num_components = num_components , ) ll = vmap ( vm_func , in_axes = 0 )( log_component_probs ) return - np . sum ( ll ) return loss_dp loss_dp = make_loss_dp ( num_components = 25 ) dloss_dp = grad ( loss_dp ) loss_dp ( np . log ( 3 ), log_component_probs = np . log ( weights_draws [ 3 ] + 1e-6 )) DeviceArray(-353.45496, dtype=float32) I have opted for a closure pattern here because we are going to require that the Dirichlet-process log likelihood loss function accept log_concentration (parameter to optimize) as the first argument, and log_component_probs (data) as the second. However, we need to specify the number of components we are going to allow for evaluating the Beta-distributed log likelihood, so that goes on the outside. Moreover, we are assuming i.i.d. draws of weights, therefore, we also vmap over all of the log_component_probs . Define training loop Just as with the previous sections, we are going to define the training loops. from dl_workshop.gaussian_mixture import make_step_scannable make_step_scannable ?? For our demonstration here, we are going to use draws from the weights_draws matrix defined above, specifically the one at index 3, which had a concentration value of 5. Just to remind ourselves what that heatmapt looks like: sns . heatmap ( weights_draws [ 3 ]); <AxesSubplot:> Now, we set up the scannable step function: from jax.experimental.optimizers import adam adam_init , adam_update , adam_get_params = adam ( 0.05 ) step_scannable = make_step_scannable ( get_params_func = adam_get_params , dloss_func = dloss_dp , update_func = adam_update , data = np . log ( weights_draws [ 3 ] + 1e-6 ), ) And then we initialize our parameters log_concentration_init = random . normal ( key ) params_init = log_concentration_init And finally, we run the training loop as a lax.scan function. from jax import lax initial_state = adam_init ( params_init ) final_state , state_history = lax . scan ( step_scannable , initial_state , np . arange ( 1000 )) Now, we can calculate the losses over history. from dl_workshop.gaussian_mixture import get_loss get_loss ?? from jax import vmap from functools import partial losses = vmap ( partial ( get_loss , get_params_func = adam_get_params , loss_func = loss_dp , data = np . log ( weights_draws [ 1 ] + 1e-6 ) ) )( state_history ) plt . plot ( losses ) [<matplotlib.lines.Line2D at 0x7f5bb06890d0>] What is the final value that we obtain? params_opt = adam_get_params ( final_state ) params_opt DeviceArray(1.6304003, dtype=float32) np . exp ( params_opt ) DeviceArray(5.105918, dtype=float32) This is pretty darn close to what we started with! Summary Here, we took a detour through Dirichlet processes to help you get a grounding onto how its math works. Through code, we saw how to: Use the Beta distribution, Write the stick-breaking process using Beta-distributed draws to generate large vectors of weights that correspond to categorical probabilities, Run the stick-breaking process backwards from a vector of categorical probabilities to get back Beta-distributed draws Infer the maximum likelihood concentration value given a set of draws. The primary purpose of this section was to get you primed for the next section, in which we try to simulatenously infer the number of prominent mixture components and their distribution parameters. A (ahem!) derivative outcome here was that I hopefully showed you how it is possible to use gradient-based optimization on seemingly discrete problems.","title":"Dirichlet Processes"},{"location":"04-gaussian-clustering/02-dirichlet-processes/#dirichlet-processes-a-simulated-guide","text":"","title":"Dirichlet Processes: A simulated guide"},{"location":"04-gaussian-clustering/02-dirichlet-processes/#introduction","text":"In the previous section, we saw how we could fit a two-component Gaussian mixture model to data that looked like it had just two components. In many real-world settings, though, we oftentimes do not know exactly how many components are present, so one way we can approach the problem is to assume that there are an infinite (or \"countably large\") number of components available for our model to pick from, but we \"guide\" our model to focus its attention on only a small number of components provided. Does that sound magical? It sure did for me when I first heard about this possibility. The key modelling component that we need is a process for creating infinite numbers of mixture weight components from a single controllable parameter, and that naturally gives us a Dirichlet process , which we will look at in this section.","title":"Introduction"},{"location":"04-gaussian-clustering/02-dirichlet-processes/#what-are-dirichlet-processes","text":"To quote from Wikipedia's article on DPs : In probability theory, Dirichlet processes (after Peter Gustav Lejeune Dirichlet) are a family of stochastic processes whose realizations are probability distributions. Hmm, now that doesn't look very concrete. Is there a more concrete way to think about DPs? Turns out, the answer is yes! At its core, each realization/draw from a DP provides an infinite (or, in computing world, a \"large\") set of weights that sum to 1. Remember that: A long vector of numbers that sum to 1, which we can interpret as a probability distribution over sets of weights.","title":"What are Dirichlet processes?"},{"location":"04-gaussian-clustering/02-dirichlet-processes/#simulating-a-dirichlet-process-using-stick-breaking","text":"We're going to look at one way to construct a probability vector, the \"stick-breaking\" process. How does it work? At its core, it looks like this, a very simple idea. We take a length 1 stick, draw a probability value from a Beta distribution, break the length 1 stick into two at the point drawn, and record the left side's value. We then take the right side, draw another probability value from a Beta distribution again, break that stick proportionally into two portions at the point drawn, and record the absolute length of the left side's value We then braek the right side again, using the same process. We repeat this until we have the countably large number of states that we desire. In code, this looks like a loop with a carryover from the previous iteration, which means it is a lax.scan -able function! from dl_workshop.gaussian_mixture import stick_breaking_weights stick_breaking_weights ?? As you can see, in the inner function weighting , we first calculate the weight associated with the \"left side\" of the stick, which we record down and accumulate as the \"history\" (second tuple element of the return). Our carry is the occupied_probability + weight , which we can use to calculate the length of the right side of the stick ( 1 - occupied_probability ). Because each beta_i is an i.i.d. draw from beta_draws , we can pre-instantiate a vector of beta_draws and then lax.scan the weighting function over the vector.","title":"Simulating a Dirichlet Process using \"stick-breaking\""},{"location":"04-gaussian-clustering/02-dirichlet-processes/#beta-distribution-crash-course","text":"Because on computers it's hard to deal with infinitely-long arrays, we can instead instantiate a \"countably large\" array of beta_draws . Now, the beta_draws , need to be i.i.d. from a source Beta distribution, which has two parameters, a and b , and gives us a continuous distribution over the interval (0, 1) (0, 1) . Because of the nature of a and b corresponding to success and failure weights: higher a at constant b shifts the distribution closer to 1, higher b at constant a shifts the distribution closer to 0, higher magnitudes of a and b narrow the distribution width.","title":"Beta distribution crash-course"},{"location":"04-gaussian-clustering/02-dirichlet-processes/#visualizing-stick-breaking","text":"For our purposes, we are going to hold a constant at 1.0 while varying b . We'll then see how our weight vectors are generated as a function of b . As you will see, b becomes a \"concentration\" parameter, which governs how \"concentrated\" our probability mass is allocated. Let's see how one draw from a Dirichlet process looks like. def dp_draw ( key , concentration , vector_length ): beta_draws = random . beta ( key = key , a = 1 , b = concentration , shape = ( vector_length ,)) occupied_probability , weights = stick_breaking_weights ( beta_draws ) return occupied_probability , weights from jax import random import matplotlib.pyplot as plt key = random . PRNGKey ( 42 ) occupied_probability , weights = dp_draw ( key , 3 , 50 ) plt . plot ( weights ) plt . xlabel ( \"Vector slot\" ) plt . ylabel ( \"Probability\" ); WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) Text(0, 0.5, 'Probability') Now, what if we took 20 draws from the Dirichlet process? To do so, we can vmap dp_draw over split PRNGKey s. from jax import vmap from functools import partial import seaborn as sns keys = random . split ( key , 20 ) occupied_probabilities , weights_draws = vmap ( partial ( dp_draw , concentration = 3 , vector_length = 50 ))( keys ) sns . heatmap ( weights_draws ); <AxesSubplot:>","title":"Visualizing stick-breaking"},{"location":"04-gaussian-clustering/02-dirichlet-processes/#effect-of-concentration-on-dirichlet-weights-draws","text":"As is visible here, when concentration = 3 , most of our probability mass is concentrated across roughly the first 5-8 states. What happens if we varied the concentration? How does that parameter affect the distribution of weights? import jax.numpy as np concentrations = np . array ([ 0.5 , 1 , 3 , 5 , 10 , 20 ]) def dirichlet_one_concentration ( key , concentration , num_draws ): keys = random . split ( key , num_draws ) occupied_probabilities , weights_draws = vmap ( partial ( dp_draw , concentration = concentration , vector_length = 50 ))( keys ) return occupied_probabilities , weights_draws keys = random . split ( key , len ( concentrations )) occupied_probabilities , weights_draws = vmap ( partial ( dirichlet_one_concentration , num_draws = 20 ))( keys , concentrations ) weights_draws . shape (6, 20, 50) fig , axes = plt . subplots ( nrows = 2 , ncols = 3 , figsize = ( 3 * 3 , 3 * 2 ), sharex = True , sharey = True ) for ax , weights_mat , conc in zip ( axes . flatten (), weights_draws , concentrations ): sns . heatmap ( weights_mat , ax = ax ) ax . set_title ( f \"Concentration = { conc } \" ) ax . set_xlabel ( \"Component\" ) ax . set_ylabel ( \"Draw\" ) plt . tight_layout () As we increase the concentration value, the probabilities get more diffuse. This is evident from the above heatmaps in the following ways. Over each draw, as we increase the value of the concentration parameter, the probability mass allocated to the components that have significant probability mass decreases. Additionally, more components have \"significant\" amounts of probability mass allocated.","title":"Effect of concentration on Dirichlet weights draws"},{"location":"04-gaussian-clustering/02-dirichlet-processes/#running-stick-breaking-backwards","text":"From this forward process of generating Dirichlet-distributed weights, instead of evaluating the log likelihood of the component weights under a \"fixed\" Dirichlet distribution prior, we can instead evaluate it under a Dirichlet process with a \"concentration\" prior. The requirement here is that we be able to recover correctly the i.i.d. Beta draws that generated the Dirichlet process weights. Let's try that out. from dl_workshop.gaussian_mixture import beta_draw_from_weights beta_draw_from_weights ?? We essentially run the process backwards, taking advantage of the fact that we know the first weight exactly. Let's try to see how well we can recover the weights. concentration = 3 beta_draws = random . beta ( key = key , a = 1 , b = concentration , shape = ( 50 ,)) occupied_probability , weights = stick_breaking_weights ( beta_draws ) final , beta_hat = beta_draw_from_weights ( weights ) plt . plot ( beta_draws , label = \"original\" ) plt . plot ( beta_hat , label = \"inferred\" ) plt . legend () plt . xlabel ( \"Component\" ) plt . ylabel ( \"Beta Draw\" ); Text(0, 0.5, 'Beta Draw') As is visible from the plot above, we were able to recover about 1/2 to 2/3 of the weights before the divergence in the two curves shows up. One of the difficulties that we have is that when we get back the observed weights in real life, we have no access to how much of the length 1 \"stick\" is leftover. This, alongside numerical underflow issues arising from small numbers, means we can only use about 1/2 of the drawn weights to recover the Beta-distributed draws from which we can evaluate our log likelihoods.","title":"Running stick-breaking backwards"},{"location":"04-gaussian-clustering/02-dirichlet-processes/#evaluating-log-likelihood-of-recovered-beta-distributed-weights","text":"So putting things all together, we can take a weights vector, run the stick-breaking process backwards (up to a certain point) to recover Beta-distributed draws that would have generated the weights vector, and then evaluate the log-likelihood of the Beta-disributed draws under a Beta distribution. Let's see that in action: from dl_workshop.gaussian_mixture import component_probs_loglike component_probs_loglike ?? And evaluating our draws should give us a scalar likelihood: component_probs_loglike ( np . log ( weights ), log_concentration = 1.0 , num_components = 25 ) DeviceArray(11.000253, dtype=float32)","title":"Evaluating log-likelihood of recovered Beta-distributed weights"},{"location":"04-gaussian-clustering/02-dirichlet-processes/#log-likelihood-as-a-function-of-concentration","text":"Once again, let's build up our understanding by seeing how the log likelihood of our weights under an assumed Dirichlet process from a Beta distribution changes as we vary the concentration parameter. log_concentration = np . linspace ( - 3 , 3 , 1000 ) def make_vmappable_loglike ( log_component_probs , num_components ): def inner ( log_concentration ): return component_probs_loglike ( log_component_probs , log_concentration , num_components ) return inner component_probs_loglike_vmappable = make_vmappable_loglike ( log_component_probs = np . log ( weights ), num_components = 25 ) lls = vmap ( component_probs_loglike_vmappable )( log_concentration ) plt . plot ( log_concentration , lls ) plt . xlabel ( \"Concentration\" ) plt . ylabel ( \"Log likelihood\" ); Text(0, 0.5, 'Log likelihood') As you can see above, we first constructed the vmappable log-likelihood function using a closure. The shape of the curve tells us that it is an optimizable problem with one optimal point, at least within bounds of possible concentrations that we're interested in.","title":"Log likelihood as a function of concentration"},{"location":"04-gaussian-clustering/02-dirichlet-processes/#optimizing-the-log-likelihood","text":"Once again, we're going to see how we can use gradient-based optimization to see how we can identify the most likely concentration value that generated a Dirichlet process weights vector.","title":"Optimizing the log-likelihood"},{"location":"04-gaussian-clustering/02-dirichlet-processes/#define-loss-function","text":"As always, we start with the loss function definition. Because our component_probs_loglike function operates only on a single draw, we need a function that will allow us to operate on multiple draws. We can do this by using a closure. from jax import grad def make_loss_dp ( num_components ): def loss_dp ( log_concentration , log_component_probs ): \"\"\"Log-likelihood of component_probabilities of dirichlet process. :param log_concentration: Scalar value. :param log_component_probs: One or more component probability vectors. \"\"\" vm_func = partial ( component_probs_loglike , log_concentration = log_concentration , num_components = num_components , ) ll = vmap ( vm_func , in_axes = 0 )( log_component_probs ) return - np . sum ( ll ) return loss_dp loss_dp = make_loss_dp ( num_components = 25 ) dloss_dp = grad ( loss_dp ) loss_dp ( np . log ( 3 ), log_component_probs = np . log ( weights_draws [ 3 ] + 1e-6 )) DeviceArray(-353.45496, dtype=float32) I have opted for a closure pattern here because we are going to require that the Dirichlet-process log likelihood loss function accept log_concentration (parameter to optimize) as the first argument, and log_component_probs (data) as the second. However, we need to specify the number of components we are going to allow for evaluating the Beta-distributed log likelihood, so that goes on the outside. Moreover, we are assuming i.i.d. draws of weights, therefore, we also vmap over all of the log_component_probs .","title":"Define loss function"},{"location":"04-gaussian-clustering/02-dirichlet-processes/#define-training-loop","text":"Just as with the previous sections, we are going to define the training loops. from dl_workshop.gaussian_mixture import make_step_scannable make_step_scannable ?? For our demonstration here, we are going to use draws from the weights_draws matrix defined above, specifically the one at index 3, which had a concentration value of 5. Just to remind ourselves what that heatmapt looks like: sns . heatmap ( weights_draws [ 3 ]); <AxesSubplot:> Now, we set up the scannable step function: from jax.experimental.optimizers import adam adam_init , adam_update , adam_get_params = adam ( 0.05 ) step_scannable = make_step_scannable ( get_params_func = adam_get_params , dloss_func = dloss_dp , update_func = adam_update , data = np . log ( weights_draws [ 3 ] + 1e-6 ), ) And then we initialize our parameters log_concentration_init = random . normal ( key ) params_init = log_concentration_init And finally, we run the training loop as a lax.scan function. from jax import lax initial_state = adam_init ( params_init ) final_state , state_history = lax . scan ( step_scannable , initial_state , np . arange ( 1000 )) Now, we can calculate the losses over history. from dl_workshop.gaussian_mixture import get_loss get_loss ?? from jax import vmap from functools import partial losses = vmap ( partial ( get_loss , get_params_func = adam_get_params , loss_func = loss_dp , data = np . log ( weights_draws [ 1 ] + 1e-6 ) ) )( state_history ) plt . plot ( losses ) [<matplotlib.lines.Line2D at 0x7f5bb06890d0>] What is the final value that we obtain? params_opt = adam_get_params ( final_state ) params_opt DeviceArray(1.6304003, dtype=float32) np . exp ( params_opt ) DeviceArray(5.105918, dtype=float32) This is pretty darn close to what we started with!","title":"Define training loop"},{"location":"04-gaussian-clustering/02-dirichlet-processes/#summary","text":"Here, we took a detour through Dirichlet processes to help you get a grounding onto how its math works. Through code, we saw how to: Use the Beta distribution, Write the stick-breaking process using Beta-distributed draws to generate large vectors of weights that correspond to categorical probabilities, Run the stick-breaking process backwards from a vector of categorical probabilities to get back Beta-distributed draws Infer the maximum likelihood concentration value given a set of draws. The primary purpose of this section was to get you primed for the next section, in which we try to simulatenously infer the number of prominent mixture components and their distribution parameters. A (ahem!) derivative outcome here was that I hopefully showed you how it is possible to use gradient-based optimization on seemingly discrete problems.","title":"Summary"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Applying Dirichlet-processes to mixture-model clustering Over the previous two sections, we learned about Dirichlet processes and Gaussian Mixture Model-based clustering. In this section, we're going to put the two concepts together! A data problem with unclear number of modes Let's start with a data problem that is a bit trickier to solve: one that has multiple numbers of modes, but for which the mixture distribution visually obscures the true number of modes present. from jax import numpy as np , random , vmap , jit , grad , lax import matplotlib.pyplot as plt weights_true = np . array ([ 2 , 10 , 1 , 6 ]) locs_true = np . array ([ - 2. , - 5. , 3. , 8. ]) scale_true = np . array ([ 1.1 , 2 , 1. , 1.5 ,]) base_n_draws = 1000 key = random . PRNGKey ( 42 ) keys = random . split ( key , 4 ) draws = [] for i in range ( 4 ): shape = int ( base_n_draws * weights_true [ i ]), draw = scale_true [ i ] * random . normal ( keys [ i ], shape = shape ) + locs_true [ i ] draws . append ( draw ) data_mixture = np . concatenate ( draws ) plt . hist ( data_mixture ); WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) (array([ 74., 1226., 4357., 4496., 1794., 550., 820., 2939., 2542., 202.]), array([-12.391266 , -9.818682 , -7.246098 , -4.6735144 , -2.1009305 , 0.47165346, 3.0442374 , 5.6168213 , 8.189405 , 10.761989 , 13.334573 ], dtype=float32), <BarContainer object of 10 artists>) From the histogram, it should be easy to tell that this is not going to be an easy problem to solve. Firstly, the mixture distributions in reality have 4 components. But what we get looks more like 2 components... or really? Could it be that we're lying by using a histogram? plt . hist ( data_mixture , bins = 100 ); (array([ 2., 1., 0., 2., 3., 6., 4., 13., 29., 14., 36., 47., 42., 72., 103., 115., 149., 200., 212., 250., 270., 348., 397., 388., 437., 476., 444., 520., 529., 548., 474., 495., 482., 501., 484., 451., 427., 417., 386., 379., 327., 310., 286., 230., 188., 171., 123., 80., 52., 27., 34., 23., 29., 29., 39., 63., 68., 83., 77., 105., 91., 94., 107., 73., 63., 64., 73., 71., 80., 104., 132., 163., 253., 228., 287., 340., 347., 385., 405., 399., 381., 383., 362., 329., 300., 221., 194., 163., 124., 85., 60., 51., 26., 21., 14., 13., 10., 3., 2., 2.]), array([-12.391266 , -12.134007 , -11.876749 , -11.619491 , -11.362232 , -11.104974 , -10.847715 , -10.590457 , -10.333199 , -10.07594 , -9.818682 , -9.561423 , -9.304165 , -9.046906 , -8.789648 , -8.53239 , -8.275131 , -8.017874 , -7.760615 , -7.5033565 , -7.246098 , -6.9888396 , -6.731581 , -6.474323 , -6.2170644 , -5.9598064 , -5.702548 , -5.4452896 , -5.188031 , -4.930773 , -4.6735144 , -4.416256 , -4.1589975 , -3.9017391 , -3.6444807 , -3.3872223 , -3.1299639 , -2.8727055 , -2.6154473 , -2.3581889 , -2.1009305 , -1.843672 , -1.5864136 , -1.3291552 , -1.0718969 , -0.8146385 , -0.5573801 , -0.3001217 , -0.04286331, 0.21439508, 0.47165346, 0.7289119 , 0.98617023, 1.2434286 , 1.500687 , 1.7579454 , 2.0152037 , 2.2724621 , 2.5297205 , 2.786979 , 3.0442374 , 3.3014958 , 3.5587542 , 3.8160124 , 4.073271 , 4.330529 , 4.5877876 , 4.845046 , 5.1023045 , 5.359563 , 5.6168213 , 5.8740797 , 6.131338 , 6.3885965 , 6.645855 , 6.903113 , 7.1603713 , 7.4176297 , 7.674888 , 7.9321465 , 8.189405 , 8.446664 , 8.703922 , 8.961181 , 9.218438 , 9.475697 , 9.732955 , 9.990213 , 10.247472 , 10.50473 , 10.761989 , 11.019247 , 11.276505 , 11.533764 , 11.791022 , 12.048281 , 12.305539 , 12.562798 , 12.820056 , 13.077314 , 13.334573 ], dtype=float32), <BarContainer object of 100 artists>) Aha! The case against histograms reveals itself. Turns out there's lots of problems using histograms, and I shan't go deeper into them here, but obscuring data is one of those issues. To learn more, I wrote a blog post on the matter . In any case, this situation is a clear one where the distribution shape clearly masks the number of mixture components. How can we get around this? Here, we can turn to Dirichlet processes as a tool to help us. Because DPs don't impose an exact number of significant categories on us, but instead allow us to control their number probabilistically with a single \"concentration\" parameter, we can instead write down a model to learn the: concentration parameters, optimal relative weighting of components, conditioned on concentration parameters, distribution parameters for each component, conditioned on data. This effectively forms a Dirichlet-Process Gaussian Mixture Model . Let's see this in action! Dirichlet-Process Gaussian Mixture Model (DP-GMM) The DP-GMM model presumes an infinite (or countably large) number of states, with one Gaussian available per state. The first thing we need to do is to write down the joint log-likelihood of every parameter in our model. As always, before we write down that joint log-likelihood, the first thing we must do is correctly specify what the data generating process is. Data generating process for a DP-GMM This could be our data generating process: We start with a large number of states, and for each one, their likelihood of ocurring is goverened by a concentration parameter. With each state and their corresponding probabilities, we draw a number from the corresponding mixture Gaussian. That number's likelihood is proportional to the state from which it was drawn. With this idea in hand, we can start composing together the joint log-likelihood of the model, conditioned on its parameters and data. Log-likelihood for the component weights The first piece we need to compose together is the component weights. We have that already defined! from dl_workshop.gaussian_mixture import component_probs_loglike component_probs_loglike ?? To quickly recap what this is: it's the log likelihood of a categorical probability vector under a Dirichlet process with a specified concentration parameter. Log-likelihood for the Gaussian mixture The second piece we need is the Gaussian mixture log-likelihood. from dl_workshop.gaussian_mixture import mixture_loglike mixture_loglike ?? And to recap this one really quickly: this is the log likelihood of the observed data under each of the component weights. Joint log-likelihood Put together, the joint log-likelihood of the Gaussian mixture model is: def joint_loglike ( log_component_weights , log_concentration , num_components , component_mus , log_component_scales , data ): component_probs = np . exp ( log_component_weights ) probs_ll = component_probs_loglike ( log_component_weights , log_concentration , num_components ) mix_ll = mixture_loglike ( log_component_weights , component_mus , log_component_scales , data ) return probs_ll + mix_ll Through log likelihood function, we are expressing the dependence of the mixture Gaussians on the component probs, and the dependence of the component probs on the concentration parameter. Optimization We can now begin optimizing our mixture model parameters. Loss function As always, we define the loss function. def make_joint_loss ( num_components ): def inner ( params , data ): ( log_component_weights , log_concentration , component_mus , log_component_scales ) = params ll = joint_loglike ( log_component_weights , log_concentration , num_components , component_mus , log_component_scales , data , ) return - ll return inner joint_loss = make_joint_loss ( num_components = 25 ) The closure pattern is here, so that we can set the number of components to use for Dirichlet estimation without making it part of the params to optimize. Gradient function We then define the gradient function: djoint_loss = grad ( joint_loss ) Because I know these work, I am going to skip over test-driving them. Initialization We'll now start by initializing our parameters. k1 , k2 , k3 , k4 = random . split ( key , 4 ) n_components = 50 log_component_weights_init = random . normal ( k1 , shape = ( n_components ,)) log_concentration_init = random . normal ( k2 , shape = ( 1 ,)) component_mus_init = random . normal ( k3 , shape = ( n_components ,)) log_component_scales_init = random . normal ( k4 , shape = ( n_components ,)) params_init = log_component_weights_init , log_concentration_init , component_mus_init , log_component_scales_init Training Loop Now we write the training loop, leveraging the functions we had before. from jax.experimental.optimizers import adam from dl_workshop.gaussian_mixture import make_step_scannable adam_init , adam_update , adam_get_params = adam ( 0.05 ) step_scannable = make_step_scannable ( get_params_func = adam_get_params , dloss_func = djoint_loss , update_func = adam_update , data = data_mixture , ) step_scannable = jit ( step_scannable ) Run training Finally, we train the model! from time import time start = time () initial_state = adam_init ( params_init ) N_STEPS = 10000 final_state , state_history = lax . scan ( step_scannable , initial_state , np . arange ( N_STEPS )) end = time () print ( f \"Time taken: { end - start : .2f } seconds.\" ) Time taken: 2.24 seconds. Visualize training We're going to make the money figure first. Let's visualize the evolution of the mixture Gaussians over training iteration. params_history = adam_get_params ( state_history ) log_component_weights_history , log_concentration_history , component_mus_history , log_component_scales_history = params_history from dl_workshop.gaussian_mixture import animate_training %% capture params_for_plotting = [ log_component_weights_history , component_mus_history , log_component_scales_history ] animation = animate_training ( params_for_plotting , int ( N_STEPS / 200 ), data_mixture ) from IPython.display import HTML HTML ( animation . to_html5_video ()) Your browser does not support the video tag. And for the losses: joint_loss = jit ( joint_loss ) losses = [] for w , c , m , s in zip ( log_component_weights_history , log_concentration_history , component_mus_history , log_component_scales_history ): prm = ( w , c , m , s ) l = joint_loss ( prm , data_mixture ) losses . append ( l ) plt . plot ( losses ) plt . yscale ( \"log\" ) from dl_workshop.gaussian_mixture import normalize_weights params_opt = adam_get_params ( final_state ) log_component_weights_opt = params_opt [ 0 ] component_weights_opt = np . exp ( log_component_weights_opt ) plt . plot ( normalize_weights ( component_weights_opt ), marker = \"o\" ) [<matplotlib.lines.Line2D at 0x7fb1bd245c50>] Looks like we are able to recover the major components, in the correct proportions! If you remembered what the data looked like in 1 dimension, there were basically only 3 majorly-identifiable components. Given enough training iterations (we had to go to 10,000 iterations), our trained model was able to identify all of them, while assigning insignificant probability mass to the rest. Some caveats While the main point of this chapter was to show you that it is possible to use gradient-based optimization to cluster data, the same caveats that apply to GMM-based clustering also apply here. For example, label switching is prominent: the components that are prominent may switch at any time during the gradient descent process. If you observed the video carefully, you would see that in action too. When it comes to MCMC for fully Bayesian inference, this is a problem. With maximum likelihood estimation using gradient descent, however, this is less of an issue, as we usually only end up taking the final optimized parameters. Summary The primary purpose of this notebook was to show you that gradient descent is not only for supervised machine learning, but also for unsupervised learning. More generally, gradients can be used anywhere there is an \"optimization\" problem setup. In this case, identifying clusters of data in a mixture model is a classic unsupervised machine learning problem, but because we cast it in the form of a log-likelihood optimization problem, we were able to leverage gradients to solve this problem. Aside from that, we saw the JAX idioms in action: vmap , lax.scan , grad , jit and more. Once again, vmap and lax.scan replaced many of the for-loops that we might have otherwise written, grad gave us easy access to gradients, and jit gave us the advantage of compilation.","title":"DP-GMM"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#applying-dirichlet-processes-to-mixture-model-clustering","text":"Over the previous two sections, we learned about Dirichlet processes and Gaussian Mixture Model-based clustering. In this section, we're going to put the two concepts together!","title":"Applying Dirichlet-processes to mixture-model clustering"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#a-data-problem-with-unclear-number-of-modes","text":"Let's start with a data problem that is a bit trickier to solve: one that has multiple numbers of modes, but for which the mixture distribution visually obscures the true number of modes present. from jax import numpy as np , random , vmap , jit , grad , lax import matplotlib.pyplot as plt weights_true = np . array ([ 2 , 10 , 1 , 6 ]) locs_true = np . array ([ - 2. , - 5. , 3. , 8. ]) scale_true = np . array ([ 1.1 , 2 , 1. , 1.5 ,]) base_n_draws = 1000 key = random . PRNGKey ( 42 ) keys = random . split ( key , 4 ) draws = [] for i in range ( 4 ): shape = int ( base_n_draws * weights_true [ i ]), draw = scale_true [ i ] * random . normal ( keys [ i ], shape = shape ) + locs_true [ i ] draws . append ( draw ) data_mixture = np . concatenate ( draws ) plt . hist ( data_mixture ); WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) (array([ 74., 1226., 4357., 4496., 1794., 550., 820., 2939., 2542., 202.]), array([-12.391266 , -9.818682 , -7.246098 , -4.6735144 , -2.1009305 , 0.47165346, 3.0442374 , 5.6168213 , 8.189405 , 10.761989 , 13.334573 ], dtype=float32), <BarContainer object of 10 artists>) From the histogram, it should be easy to tell that this is not going to be an easy problem to solve. Firstly, the mixture distributions in reality have 4 components. But what we get looks more like 2 components... or really? Could it be that we're lying by using a histogram? plt . hist ( data_mixture , bins = 100 ); (array([ 2., 1., 0., 2., 3., 6., 4., 13., 29., 14., 36., 47., 42., 72., 103., 115., 149., 200., 212., 250., 270., 348., 397., 388., 437., 476., 444., 520., 529., 548., 474., 495., 482., 501., 484., 451., 427., 417., 386., 379., 327., 310., 286., 230., 188., 171., 123., 80., 52., 27., 34., 23., 29., 29., 39., 63., 68., 83., 77., 105., 91., 94., 107., 73., 63., 64., 73., 71., 80., 104., 132., 163., 253., 228., 287., 340., 347., 385., 405., 399., 381., 383., 362., 329., 300., 221., 194., 163., 124., 85., 60., 51., 26., 21., 14., 13., 10., 3., 2., 2.]), array([-12.391266 , -12.134007 , -11.876749 , -11.619491 , -11.362232 , -11.104974 , -10.847715 , -10.590457 , -10.333199 , -10.07594 , -9.818682 , -9.561423 , -9.304165 , -9.046906 , -8.789648 , -8.53239 , -8.275131 , -8.017874 , -7.760615 , -7.5033565 , -7.246098 , -6.9888396 , -6.731581 , -6.474323 , -6.2170644 , -5.9598064 , -5.702548 , -5.4452896 , -5.188031 , -4.930773 , -4.6735144 , -4.416256 , -4.1589975 , -3.9017391 , -3.6444807 , -3.3872223 , -3.1299639 , -2.8727055 , -2.6154473 , -2.3581889 , -2.1009305 , -1.843672 , -1.5864136 , -1.3291552 , -1.0718969 , -0.8146385 , -0.5573801 , -0.3001217 , -0.04286331, 0.21439508, 0.47165346, 0.7289119 , 0.98617023, 1.2434286 , 1.500687 , 1.7579454 , 2.0152037 , 2.2724621 , 2.5297205 , 2.786979 , 3.0442374 , 3.3014958 , 3.5587542 , 3.8160124 , 4.073271 , 4.330529 , 4.5877876 , 4.845046 , 5.1023045 , 5.359563 , 5.6168213 , 5.8740797 , 6.131338 , 6.3885965 , 6.645855 , 6.903113 , 7.1603713 , 7.4176297 , 7.674888 , 7.9321465 , 8.189405 , 8.446664 , 8.703922 , 8.961181 , 9.218438 , 9.475697 , 9.732955 , 9.990213 , 10.247472 , 10.50473 , 10.761989 , 11.019247 , 11.276505 , 11.533764 , 11.791022 , 12.048281 , 12.305539 , 12.562798 , 12.820056 , 13.077314 , 13.334573 ], dtype=float32), <BarContainer object of 100 artists>) Aha! The case against histograms reveals itself. Turns out there's lots of problems using histograms, and I shan't go deeper into them here, but obscuring data is one of those issues. To learn more, I wrote a blog post on the matter . In any case, this situation is a clear one where the distribution shape clearly masks the number of mixture components. How can we get around this? Here, we can turn to Dirichlet processes as a tool to help us. Because DPs don't impose an exact number of significant categories on us, but instead allow us to control their number probabilistically with a single \"concentration\" parameter, we can instead write down a model to learn the: concentration parameters, optimal relative weighting of components, conditioned on concentration parameters, distribution parameters for each component, conditioned on data. This effectively forms a Dirichlet-Process Gaussian Mixture Model . Let's see this in action!","title":"A data problem with unclear number of modes"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#dirichlet-process-gaussian-mixture-model-dp-gmm","text":"The DP-GMM model presumes an infinite (or countably large) number of states, with one Gaussian available per state. The first thing we need to do is to write down the joint log-likelihood of every parameter in our model. As always, before we write down that joint log-likelihood, the first thing we must do is correctly specify what the data generating process is.","title":"Dirichlet-Process Gaussian Mixture Model (DP-GMM)"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#data-generating-process-for-a-dp-gmm","text":"This could be our data generating process: We start with a large number of states, and for each one, their likelihood of ocurring is goverened by a concentration parameter. With each state and their corresponding probabilities, we draw a number from the corresponding mixture Gaussian. That number's likelihood is proportional to the state from which it was drawn. With this idea in hand, we can start composing together the joint log-likelihood of the model, conditioned on its parameters and data.","title":"Data generating process for a DP-GMM"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#log-likelihood-for-the-component-weights","text":"The first piece we need to compose together is the component weights. We have that already defined! from dl_workshop.gaussian_mixture import component_probs_loglike component_probs_loglike ?? To quickly recap what this is: it's the log likelihood of a categorical probability vector under a Dirichlet process with a specified concentration parameter.","title":"Log-likelihood for the component weights"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#log-likelihood-for-the-gaussian-mixture","text":"The second piece we need is the Gaussian mixture log-likelihood. from dl_workshop.gaussian_mixture import mixture_loglike mixture_loglike ?? And to recap this one really quickly: this is the log likelihood of the observed data under each of the component weights.","title":"Log-likelihood for the Gaussian mixture"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#joint-log-likelihood","text":"Put together, the joint log-likelihood of the Gaussian mixture model is: def joint_loglike ( log_component_weights , log_concentration , num_components , component_mus , log_component_scales , data ): component_probs = np . exp ( log_component_weights ) probs_ll = component_probs_loglike ( log_component_weights , log_concentration , num_components ) mix_ll = mixture_loglike ( log_component_weights , component_mus , log_component_scales , data ) return probs_ll + mix_ll Through log likelihood function, we are expressing the dependence of the mixture Gaussians on the component probs, and the dependence of the component probs on the concentration parameter.","title":"Joint log-likelihood"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#optimization","text":"We can now begin optimizing our mixture model parameters.","title":"Optimization"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#loss-function","text":"As always, we define the loss function. def make_joint_loss ( num_components ): def inner ( params , data ): ( log_component_weights , log_concentration , component_mus , log_component_scales ) = params ll = joint_loglike ( log_component_weights , log_concentration , num_components , component_mus , log_component_scales , data , ) return - ll return inner joint_loss = make_joint_loss ( num_components = 25 ) The closure pattern is here, so that we can set the number of components to use for Dirichlet estimation without making it part of the params to optimize.","title":"Loss function"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#gradient-function","text":"We then define the gradient function: djoint_loss = grad ( joint_loss ) Because I know these work, I am going to skip over test-driving them.","title":"Gradient function"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#initialization","text":"We'll now start by initializing our parameters. k1 , k2 , k3 , k4 = random . split ( key , 4 ) n_components = 50 log_component_weights_init = random . normal ( k1 , shape = ( n_components ,)) log_concentration_init = random . normal ( k2 , shape = ( 1 ,)) component_mus_init = random . normal ( k3 , shape = ( n_components ,)) log_component_scales_init = random . normal ( k4 , shape = ( n_components ,)) params_init = log_component_weights_init , log_concentration_init , component_mus_init , log_component_scales_init","title":"Initialization"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#training-loop","text":"Now we write the training loop, leveraging the functions we had before. from jax.experimental.optimizers import adam from dl_workshop.gaussian_mixture import make_step_scannable adam_init , adam_update , adam_get_params = adam ( 0.05 ) step_scannable = make_step_scannable ( get_params_func = adam_get_params , dloss_func = djoint_loss , update_func = adam_update , data = data_mixture , ) step_scannable = jit ( step_scannable )","title":"Training Loop"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#run-training","text":"Finally, we train the model! from time import time start = time () initial_state = adam_init ( params_init ) N_STEPS = 10000 final_state , state_history = lax . scan ( step_scannable , initial_state , np . arange ( N_STEPS )) end = time () print ( f \"Time taken: { end - start : .2f } seconds.\" ) Time taken: 2.24 seconds.","title":"Run training"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#visualize-training","text":"We're going to make the money figure first. Let's visualize the evolution of the mixture Gaussians over training iteration. params_history = adam_get_params ( state_history ) log_component_weights_history , log_concentration_history , component_mus_history , log_component_scales_history = params_history from dl_workshop.gaussian_mixture import animate_training %% capture params_for_plotting = [ log_component_weights_history , component_mus_history , log_component_scales_history ] animation = animate_training ( params_for_plotting , int ( N_STEPS / 200 ), data_mixture ) from IPython.display import HTML HTML ( animation . to_html5_video ()) Your browser does not support the video tag. And for the losses: joint_loss = jit ( joint_loss ) losses = [] for w , c , m , s in zip ( log_component_weights_history , log_concentration_history , component_mus_history , log_component_scales_history ): prm = ( w , c , m , s ) l = joint_loss ( prm , data_mixture ) losses . append ( l ) plt . plot ( losses ) plt . yscale ( \"log\" ) from dl_workshop.gaussian_mixture import normalize_weights params_opt = adam_get_params ( final_state ) log_component_weights_opt = params_opt [ 0 ] component_weights_opt = np . exp ( log_component_weights_opt ) plt . plot ( normalize_weights ( component_weights_opt ), marker = \"o\" ) [<matplotlib.lines.Line2D at 0x7fb1bd245c50>] Looks like we are able to recover the major components, in the correct proportions! If you remembered what the data looked like in 1 dimension, there were basically only 3 majorly-identifiable components. Given enough training iterations (we had to go to 10,000 iterations), our trained model was able to identify all of them, while assigning insignificant probability mass to the rest.","title":"Visualize training"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#some-caveats","text":"While the main point of this chapter was to show you that it is possible to use gradient-based optimization to cluster data, the same caveats that apply to GMM-based clustering also apply here. For example, label switching is prominent: the components that are prominent may switch at any time during the gradient descent process. If you observed the video carefully, you would see that in action too. When it comes to MCMC for fully Bayesian inference, this is a problem. With maximum likelihood estimation using gradient descent, however, this is less of an issue, as we usually only end up taking the final optimized parameters.","title":"Some caveats"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#summary","text":"The primary purpose of this notebook was to show you that gradient descent is not only for supervised machine learning, but also for unsupervised learning. More generally, gradients can be used anywhere there is an \"optimization\" problem setup. In this case, identifying clusters of data in a mixture model is a classic unsupervised machine learning problem, but because we cast it in the form of a log-likelihood optimization problem, we were able to leverage gradients to solve this problem. Aside from that, we saw the JAX idioms in action: vmap , lax.scan , grad , jit and more. Once again, vmap and lax.scan replaced many of the for-loops that we might have otherwise written, grad gave us easy access to gradients, and jit gave us the advantage of compilation.","title":"Summary"}]}